<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="神机喵算" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">
<meta property="og:type" content="website">
<meta property="og:title" content="神机喵算">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="神机喵算">
<meta property="og:description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神机喵算">
<meta name="twitter:description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> 神机喵算 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">神机喵算</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/spark_and_redis_connector/" itemprop="url">
                  Spark借助Redis提升45倍处理效率！
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:57:36+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>摘要</em>:时常采用内存数据结构会使得程序更加高效，比如，Spark借助Redis可以提速45倍。</p>
<p>Spark代表着下一代大数据处理技术，并且，借着开源算法和计算节点集群分布式处理，Spark和Hadoop在执行的方式和速度已经远远的超过传统单节点的技术架构。但Spark利用内存进行数据处理，这让Spark的处理速度超过基于磁盘的Hadoop 100x 倍。</p>
<p>但Spark和内存数据库<a href="http://www.redislabs.com" target="_blank" rel="external">Redis</a>结合后可显著的提高Spark运行任务的性能，这源于Redis优秀的数据结构和执行过程，从而减小数据处理的复杂性和开销。Spark通过一个Redis连接器可以访问Redis的数据和API，加速Spark处理数据。</p>
<p>Spark和Redis结合使用到底有多大的性能提升呢？结合这两者来处理时序数据时可以提高46倍以上——而不是提高百分之四十五。</p>
<p>为什么这些数据处理速度的提升是很重要的呢？现在，越来越多的公司期望在交易完成的同时完成对应的数据分析。公司的决策也需要自动化，而这些需要数据分析能够实时的进行。Spark是一个用的较多的数据处理框架，但它不能做到百分之百实时，要想做到实时处理Spark还有很大一步工作需要做。<br><img src="http://img2.ph.126.net/QalVm_AuA5Vx7x2lxXEWbw==/6598067023763790131.png" alt="此处输入图片的描述"><br>图1</p>
<h3 id="Spark-RDD"><a href="#Spark-RDD" class="headerlink" title="Spark RDD"></a>Spark RDD</h3><p>Spark采用弹性分布式数据集（RDD），可将数据存在易变的内存中或持久化到磁盘上。 RDD具有不可变化性，分布式存储在Spark集群的各节点，RDD经过tansform操作后创建出一个新的RDD。RDD是Spark中数据集的一种重要抽象，具有良好的容错性、高效的迭代处理。</p>
<h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>Redis天生为高性能设计，通过良好的数据存储结构能达到亚毫秒级的延迟。Redis的数据存储结构不仅仅提高内存的利用和减小应用的复杂性，也降低了网络负载、带宽消耗和处理时间。Redis数据结构包括字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets）， bitmaps， hyperloglogs 和 地理空间（geospatial）索引半径查询。</p>
<p>下面来展示Redis的数据结构如何来简化应用的处理时间和复杂度。这里用有序集合来举例，一个以评分（score）大小排序的元素集合。<br><img src="http://img2.ph.126.net/MatkU-NaQLLkh2i_OdRhtw==/6598097810089184567.png" alt="此处输入图片的描述"><br>图2</p>
<p>Redis能存储多种数据类型，并自动的以评分（score）排序。常见的例子有，按价格排序的商品，以阅读数排序的文章名，股票价格时序数据，带时间戳的传感器读数。<br>有序集合依赖Redis优秀的内建操作可以实现范围查询、求交集，可以非常快地（O(log(N))）完成添加，删除和更新元素的操作。Redis内建函数不仅减少代码开发，在内存中执行也减小了网络延时和带宽消耗，可达到亚毫秒级的吞吐延迟。特别地，对时序数据集合来讲，有序集合数据结构比使用内存键值对或使用磁盘的数据库，能给数据分析带来数量级上的性能提升。</p>
<h3 id="Spark-Redis-connector"><a href="#Spark-Redis-connector" class="headerlink" title="Spark-Redis connector"></a>Spark-Redis connector</h3><p>为了提高Spark数据分析的能力，Redis团队开发了一个<a href="https://github.com/RedisLabs/spark-redis" target="_blank" rel="external">Spark-Redis connector</a>，它使得Spark可以直接使用Redis作为数据源，顺理成章的Spark也能使用Redis的各数据结构，进而显著的提升Spark分析数据的速度。<br><img src="http://img2.ph.126.net/EjDbOsyQrDo6nRi2gkM66w==/6598082416926397895.png" alt="此处输入图片的描述"><br>图3</p>
<p>为了展示Spark结合Redis所产生的效果，Redis团队拿时序数据集合做基准测试，测试了Spark在不同情况下执行时间范围查询：Spark使用堆外内存；Spark使用Tachyon作为堆外缓存；Spark使用HDFS存储；Spark结合Redis使用。</p>
<p>Redis团队改进了Cloudera的Spark分析时序数据的包，采用<a href="https://github.com/RedisLabs/spark-timeseries/blob/redis/src/main/scala/com/redislabs/provider/redis/rdd/RedisRDD.scala#L26" target="_blank" rel="external">Redis有序集合数据结构加速时序数据分析</a>，并且实现Spark访问Redis各类数据结构的接口。此Spark-Redis时序开发包主要做了两件事：</p>
<ol>
<li>它让Redis节点与Spark集群的节点自动匹配，确保每个Spark节点都使用本地Redis节点，这样可以明显的优化延迟时间；</li>
<li>集成Spark DataFrame和Spark读取数据源，使得Spark SQL查询可自动转化，并能借助Redis能有效的恢复数据。</li>
</ol>
<p>换句话说，使用Spark-Redis时序开发包意味着用户无需担心Spark和Redis两者如何使用。用户使用Spark SQL进行数据分析可以获得极大的查询性能提升。</p>
<h3 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h3><p>基准测试的时序数据集是跨度32年的1024个股票交易市场按天随机生成的数据。每个股票交易所都有有序数据集，以日期和元素属性（开盘价、最高价、最低价、收盘价等）排序，在Redis中以有序数据结构存储，采用Spark进行数据分析，描述如图4<br><img src="http://img2.ph.126.net/heSh9IpGiauqBoZqf9egyg==/6598076919368260259.png" alt="此处输入图片的描述"><br>图4</p>
<p>在上述列子中，就有序集合AAPL来看，有序数据集合以天为评分（score，以蓝色表示），每天相关的值为一行（Member，以灰色表示）。在Redis中，只要执行一个ZRANGEBYSCORE操作就可以获取一个指定时间范围内的所有股票数据，并且Redis执行此查询要比其他Key/Value数据库快100倍。<br>从图x可以看到，横向比较各种情况的基准测试，Spark结合Redis执行时间片的查询速度比Spark使用HDFS快135倍、比Spark使用堆内内存或Spark使用Tachyon作为堆外内存要快45倍。<br><img src="http://img1.ph.126.net/UieEUHKoGh9ex9aJiMHPZQ==/6598192368089172532.png" alt="此处输入图片的描述"><br>图5</p>
<h3 id="Spark-Redis其它应用"><a href="#Spark-Redis其它应用" class="headerlink" title="Spark-Redis其它应用"></a>Spark-Redis其它应用</h3><p>按照“<a href="https://redislabs.com/solutions/spark-and-redis" target="_blank" rel="external">Getting Started with Spark and Redis</a>”指南，你可以一步步安装Spark集群和使用Spark-Redis包。它提供一个简单的wordcount的例子展示如何使用Spark结合Redis。待你熟练使用后可以自己进一步挖掘、优化其他的Redis数据结构。<br>Redis的有序集合数据结构很适合时序数据集合，而Redis其他数据结构（比如，列表（lists）， 集合（sets）和 地理空间（geospatial）索引半径查询）也能进一步丰富Spark的数据分析。当使用Spark抽取地理空间信息来获取新产品的人群偏好和邻近中心的位置，可结合Redis的地理空间（geospatial）索引半径查询来优化。</p>
<p>Spark支持一系列的数据分析，包括SQL、机器学习、图计算和流式数据。Spark本身的内存数据处理能力有一定的限制，而借着Redis可以让Spark更快的做数据分析。其实Spark的DataFrame和Datasets已经在做类似的优化，先把数据进行结构化放在内存里进行计算，并且Datasets可以省掉序列化和反序列化的消耗。结合Spark和Redis，借助Redis的共享分布式内存数据存储机制，可以处理数百万个记录乃至上亿的记录<br>时序数据的分析仅仅是一个开始，更多的性能优化可以参见：<a href="https://github.com/RedisLabs/spark-timeseries" target="_blank" rel="external">Spark-Redis</a>。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考;"></a><em>参考</em>;</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/use_spark_sql_or_dataframe_for_query_graphgrame/" itemprop="url">
                  基于Spark DataFrame的图数据库GraphFrame：用Spark SQL查询Graph
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:52:22+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="GraphFrame发布"><a href="#GraphFrame发布" class="headerlink" title="GraphFrame发布"></a><strong>GraphFrame发布</strong></h3><p>GraphFrame基于Spark SQL的DataFrame，继承了DataFrame扩展性和高性能。并且可以提供支持Scala、Java和Python等语言的统一API。</p>
<h3 id="什么是GraphFrame"><a href="#什么是GraphFrame" class="headerlink" title="什么是GraphFrame"></a><strong>什么是GraphFrame</strong></h3><p>GraphFrame是类似于Spark的GraphX库，支持图处理。但GraphFrame建立在Spark DataFrame之上，具有以下重要的优势：<br>支持Scala ，Java 和Python API：GraphFrame提供统一的三种编程语言APIs，而GraphX的所有算法支持Python和Java<br>方便、简单的图查询：GraphFrame允许用户使用Spark SQL和DataFrame的API查询<br>支持导出和导入图：GraphFrame支持DataFrame数据源，使得可以读取和写入多种格式的图，比如，Parquet、JSON和CSV格式。</p>
<h3 id="社交网络的列子"><a href="#社交网络的列子" class="headerlink" title="社交网络的列子"></a><strong>社交网络的列子</strong></h3><p>社交网络中的人是以关系来互相连接的，我们能把这个网络看成一幅图，其中人看成顶点，人与人之间的关系看作是边，如图1所示：<br><img src="http://img0.ph.126.net/7NaiyaPFpW3oxa26wNb0SQ==/4856287773289568868.jpg" alt="此处输入图片的描述"><br>图1<br>在社交网络上，每个人可能由年龄和名字，每个人之间的关系也有不同类型。如表1和表2<br>表1<br><img src="http://img0.ph.126.net/VHcML9Lw9s0HThseBKZFIQ==/6631272274867367835.png" alt="此处输入图片的描述"></p>
<p>表2<br><img src="http://img2.ph.126.net/QX3apWm32f7AHBbYV8-kXA==/6631388823099920156.png" alt="此处输入图片的描述"></p>
<h3 id="图查询示列"><a href="#图查询示列" class="headerlink" title="图查询示列"></a><strong>图查询示列</strong></h3><p>由于GraphFrame的顶点和边存储为DataFrame，可以用DataFrame或SQL来很简单的查询图。<br>比如，查询有多少年龄大于35的人？<br>g.vertices.filtr(“age &gt; 35”)<br>比如，有多少人至少被2个人关注？<br>g.inDegrees.filter(“inDegree &gt;=2”)</p>
<p>GraphFrames支持所有GraphX的算法，包括PageRank、Shortest Paths、Connected components、Strongly Connected components、Triangle count 和Label Propagation Algorithm（LPA）</p>
<p>GraphFrame和GraphX之间可以无损的来回转换。<br>val gx: Graph[Row, Row] = g.toGraphX()<br>val g2: GraphFrame = GraphFrame.fromGraphX(gx)<br>更相信的GraphFrame API文档见<a href="http://graphframes.github.io/api/scala/index.html#org.graphframes.GraphFrames" target="_blank" rel="external">这里</a>。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考:"></a><em>参考</em>:</h3><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/tiwtter_discover_and_consume_data/" itemprop="url">
                  Twitter数据平台的架构演化：分析数据的数据发现和消费
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:42:48+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>导读</em>：本文详细讲述Twitter数据平台的架构演化：分析数据的数据发现和消费。</p>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p> Twitter数据平台维护数据系统来支持和管理各种业务的数据生产和消费，包括，公用报表指标（比如，月活跃或者天活跃），个性化推荐，A/B测试，广告营销等。 Twitter数据平台运维着一些全球最大的Hadoop集群，其中有几个集群超过1万个节点，存储着数百PB级数据集，每天有超过10万个日常job作业处理数十PB级数据量。<a href="http://github.com/twitter/scalding" target="_blank" rel="external">Scalding</a>用来在HDFS上进行数据清洗（ETL：Extract，Transform和Load），数据科学家和数据分析师使用<a href="https://github.com/prestodb/presto" target="_blank" rel="external">Presto</a>进行交互式查询。MySQL或者Vertica用来作普通的数据集聚合，然后Tableau仪表板展示。<a href="https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale" target="_blank" rel="external">Manhattan</a>是Twitter的分布式数据库，其为实时服务服务。</p>
<p>Twitter数据平台团队从刚开始的单个数据分析组，其仅仅拥有核心的数据集，到成百上千的员工（团队）产生和消费这些数据集。这意味着：数据源发现，数据源的完成链（例如，这些数据源是如何产生和消费的）的获取，不考虑数据源的格式、位置和工具的数据集消费和它们整个生命周期内的一致性管理，将成为一个比较现实的问题。</p>
<p>为了满足这些需求，数据平台团队开发出数据访问层（Data Access Layer (DAL)）：</p>
<ul>
<li>数据发现：如何发现最重要的数据集？谁拥有这些数据集？数据集的语义和其它相关的元数据是什么？</li>
<li>数据审计：数据集的创建这或者消费者是哪位？数据集是如何创建这些数据的？数据集的依赖和服务等级协议（SLAs）是什么？数据集的报警规则是什么？数据集和它们的依赖是否一致？数据集的生命周期是如何管理的？</li>
<li>数据抽象：数据的逻辑描述是什么？数据的物理描述是什么？数据存储在哪里？数据副本在哪里？数据格式是什么？</li>
<li>数据消费：各种客户端（比如，Scalding，Presto，Hive等等）是如何交互地使用数据平台的各数据集？</li>
</ul>
<p>本文中将讨论DAL更高层次的设计和使用，DAL是如何符合整个大数据平台生态，以及分析一些实践和经验教训。</p>
<h4 id="DAL架构设计"><a href="#DAL架构设计" class="headerlink" title="DAL架构设计"></a>DAL架构设计</h4><p>为了让数据抽象，DAL有一个逻辑数据集和物理数据集的概念。逻辑数据集代表着数据集要独立于存储类型、存储位置、存储格式和存储副本之外。一个逻辑数据集可以物化到多个存储位置，甚至可以存储到不同的存储系统上，比如，HDFS或者Vertica。物理数据集是和物理存储位置（比如，HDFS namenode，像Vertica或MySQL这样的数据库等）关联的，所有的分片（物理数据块）在物理存储上。根据它们的类型，分片可以是分区或者快照。消费的数据集的元数据（Metadata）存储在物理数据集层。<br>这种抽象的好处：<br>a）：跨多种物理实现，分组聚合相同的逻辑数据集，更易数据集发现；<br>b）：提供消费数据集所需要的所有信息，包括数据集存储格式，数据集存储位置，以及调用的客户端（比如，Scalding或者Presto）。DAL数据集附加元数据，使得数据发现和数据消费更容易（如下所示）。因为所有的数据访问都通过DAL层，我们使用DAL层获取所有数据集的生产和消费的完整链。（其实跟阿里内部的数据地图差不多的意义）。</p>
<p>下面的架构图显示DAL层是如何配合数据平台的架构：<br><img src="http://img1.ph.126.net/FEKRG1rW3QV61GWGt6PdKg==/6631557048373689499.jpg" alt="此处输入图片的描述"></p>
<p>在技术栈的底层，核心基础设施包括Hadoop集群和数据库（比如，Vertica，MySQL和Manhattan）。核心数据服务层包括数据访问层（DAL），checkpoit作业状态和依赖的应用状态管理服务，以及job作业延迟报警服务。建立在核心数据服务层之上的是数据生命周期管理，包括数据复制服务和数据删除服务，数据复制服务会管理跨Hadoop集群的数据复制；数据删除服务会根据数据过期策略来删除数据。数据处理工具包括前面提到的Scalding和Presto，也包括自建的ETL工具来实现不同后端（比如，HDFS，Vertica或者MySQL）间的数据转换。</p>
<p>数据展示的UI（外界称为EagleEye）通过核心数据服务层聚合元数据（Metadata），也作为Twitter数据入口的控制。EagleEye用来发现数据集和应用，以及展示它们之间依赖的关系图。</p>
<h4 id="如何发现和消费数据集？"><a href="#如何发现和消费数据集？" class="headerlink" title="如何发现和消费数据集？"></a>如何发现和消费数据集？</h4><p>像前面涉及到的，DAL数据集带有额外的元数据，可以轻松做到数据发现和消费。Twitter数据平台团队使用下面的数据资源管理来发现和消费数据集。</p>
<h5 id="发现一个数据集"><a href="#发现一个数据集" class="headerlink" title="发现一个数据集"></a>发现一个数据集</h5><p>数据平台提供的数据资源管理中 “Discover Data Sources”模块能发现使用过的数据集，或者搜索感兴趣的数据集。数据资源管理通过DAL层搜索这个数据集。<br><img src="http://img2.ph.126.net/rDzMYLctfac6cUUkbZjHQA==/6631478983048121372.jpg" alt="此处输入图片的描述"></p>
<h5 id="数据集的预览信息"><a href="#数据集的预览信息" class="headerlink" title="数据集的预览信息"></a>数据集的预览信息</h5><p>如果数据资源管理找到了我们想查询的数据集，它将展示给使用者预览信息。如下图，数据集在HDFS上被找到，数据资源管理中可以看到数据拥有者的描述，以及通过一定的启发式计算数据集的整体健康状态。我们也能预览的元数据字段有数据集的拥有者，数据集的访问频率，代表数据schema的thrift类，HDFS上的物理位置。<br><img src="http://img2.ph.126.net/JzYMozxQTLRtmOK7x8j26A==/6631502072792305004.jpg" alt="此处输入图片的描述"></p>
<p>我们也可以检验数据集的schema，包括用户对特殊字段添加的评论。类似的，schema也可以让其它系统（Vertica或者MySQL）发现。<br><img src="http://img0.ph.126.net/tPUwFyvntZgM4T4hmJr4GA==/6631754960466694542.jpg" alt="此处输入图片的描述"></p>
<p>接下来给个例子，下面给出的代码截图是使用Scalding的例子。注意到，对读者来说，数据存储格式和位置都经过抽象。当通过Scalding运行下面的代码，时间范围提供给DAL，DAL提供数据分片的位置和格式。DAL的Scalding客户端接收刚才的信息，以Hadoop的合适的split数目来构造合适的Cascading Tap。</p>
<p><img src="http://img0.ph.126.net/q0EO2PDu337DcBl_QLCtuQ==/6631602128350431009.jpg" alt="此处输入图片的描述"></p>
<h5 id="数据集的完整链和依赖"><a href="#数据集的完整链和依赖" class="headerlink" title="数据集的完整链和依赖"></a>数据集的完整链和依赖</h5><p>数据集资源管理也可以查看生产和消费数据集的作业和作业的完整链。从图中可以看出，有一个job作业产生数据集（图中红框），同时有好几个job作业在消费这个数据集。红框中的数据的生成依赖HDFS上的好几个数据集。并且，如果其中的某个job作业生成数据集延迟了，将会发出告警。<br><img src="http://img0.ph.126.net/gKCb7V9eheRIf62pZGTdHQ==/6631780249234131459.png" alt="此处输入图片的描述"></p>
<h4 id="实践-amp-经验教训"><a href="#实践-amp-经验教训" class="headerlink" title="实践 &amp; 经验教训"></a>实践 &amp; 经验教训</h4><p>在这样Twitter体量的公司，想简化数据集跨所有数据格式和存储系统进行消费是很难的。也有一些像<a href="https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore" target="_blank" rel="external">Hive Metastore</a>这样开源的工具可以解决数据抽象，但其只有一部分功能。其它功能，比如数据集的审计和依赖链，管理数据集过期和复制和数据更易消费，也是很重要。</p>
<p>在实现DAL时做出了设计上的选择：把DAL设计成一个抽象和消费层，而不是仅仅聚焦在数据发现和审计。这么做的目的是为了让DAL成为数据集真实的来源，这将帮助我们透明的转换数据格式（比如，从lzo压缩的Thrift转换到Parquet格式），帮助我们使用相同的元数据从各种工具中产生和消费数据（比如，Scalding和Presto），帮助我们进行job作业角色的迁移（因为job作业的所有者和团队角色的不断演化，发生的相当频繁），让数据集的过期管理和复制管理在同一个地方完成。</p>
<p>在DAL刚开始实现阶段，Twitter团队把DAL作为一种library，并且DAL可以直接喝后端数据库会话。这是相当脆弱的，有这么几个原因：安全很弱，因为证书不得不分发到各个客户端；每个客户端都直接连接到数据库是相当困难的；由于客户端的重新发布对所有用户来说，更新是非常缓慢的。数据平台团队移除了这个模块，构建了服务层。</p>
<p>数据平台团队开发DAL涉及到成千上万的job作业需要重新部署依赖（例如，从HDFS到DAL），这个过程中却有正在线上运行的产品。需要严密和严谨的工作而不中断这些job作业。如果仅仅在意数据依赖链和审计，那这个实现将是相当的简单和安全，因为作者可以通过异步或者离线处理。迁移是困难的，耗时的，做好的做法是增量式迁移。但作者知道元数据服务是每个数据平台都需要的，所以，强烈推荐首先要做的是构建一个数据平台基础。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/airbnb_repair_tools/" itemprop="url">
                  Airbnb开源ReAir工具，提供PB级数据仓库的迁移和备份
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:41:49+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>摘要</em>：本篇讲述Airbnb的开源ReAir工具，提供PB级数据仓库的迁移和备份。</p>
<p><a href="https://bigdata-ny.github.io/2016/08/21/airbnb-hadoop-hive" target="_blank" rel="external">Airbnb大数据平台架构</a>成为Airbnb公司提升产品决策的关键部分。其Hive数据仓库从2013年中旬的350 TB暴增到11 PB （2015年末统计的数据）。随着公司的成长，数据仓库的可靠性需求日益剧增。我们寻求迁移数据仓库，但现有的迁移工具要么在大数据仓库时有问题，要么就是有很明显的操作负荷，所以Airbnb开发了<a href="https://github.com/airbnb/reair" target="_blank" rel="external">ReAir</a>解决这种状况。这篇文章将详细介绍ReAir是如何工作的以及它是怎样轻松的实现PB级数据仓库的备份。</p>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>最开始Airbnb所有数据存放在单个HDFS／HIve数据仓库。单个命名空间简单、易管理，然而产品复杂后之后即席查询就影响其可靠性。因此，我们把数据仓库分成了两个：一个数据仓库是支持关键产品的任务；另一个是专为即席查询用的。分离这么大的数据仓库将面临两个问题：我们如何容易的迁移庞大的数据仓库？；分离完之后，我们又该怎么保持数据的同步？为了解决这些问题，Airbnb开发ReAir项目并开源给社区。</p>
<p>ReAir对基于Hive元数据的数据仓库进行备份非常有效，可以实现PB级别的集群扩展。使用ReAir极其简单，你只要连接Hive的metastore Thrift服务，通过MapReduce进行数据复制。ReAir可以兼容Hive和Hadoop的各种版本，并支持单机模式操作——只有Hadoop和Hive是必须要有，MySQL DB是用来进行增量复制。由于ReAir同时处理数据和元数据，所以，只要复制任务一完成就可以进行表和分区的查询。</p>
<p>在没有ReAir的时候，迁移Hive数据仓库最典型的用法是启动一个DistCp，并且通过数据库操作手动管理元数据。这种方法既费力又容易出错，甚至会引起脏数据导致数据不一致，有时复制不断变化的数据仓库目录时也会出现问题。另外其它的迁移方法要求指定Hive版本，当我们需要从旧版本的Hive数据仓库迁移就变得异常困难。</p>
<p>ReAir工具包含两种迁移工具：批量迁移和增量式迁移。批量式迁移工具允许你一次复制指定的一系列表，它适合对一个集群的迁移。相对应的，增量式迁移工具可以跟踪数据仓库的变化，并复制生成的对象或者修改的对象。增量式迁移工具适合保持集群间数据的同步，它可以实现秒级数据同步。两种方式更详细的对比请见下面部分。</p>
<h4 id="批量迁移"><a href="#批量迁移" class="headerlink" title="批量迁移"></a>批量迁移</h4><p>批量式迁移一般用来备份整个数据仓库。复制的速度和吞吐量取决于reducer的数量和吞吐量，在Airbnb公司，使用500个reducer复制2.2 PB数据仅仅用了24小时。</p>
<p>启动批量迁移的过程非常简单：用户允许shell命令启动一系列MR 任务。其执行协议是从源数据仓库复制Hive表和分区等实体到目的数据仓库。批量式复制过程是占有带宽的，它会探测源数据仓库和目的数据仓库，并只复制两者之间的不同文件。同时元数据也仅仅更新变化过的。这些策略可以保证ReAir工作的高效率。</p>
<p>批量式迁移也有一些挑战。最明显的一个是，产品数据仓库中实体（Hive表和分区）的大小不同，但是迁移的延迟并不能依赖于最大的一个实体。例如，一般的Hive表不到100个分区，然后最大的表有超过10万个分区。为了保证正常的延迟，需要并行运行迁移工作。</p>
<p>为了解决负载均衡的问题，批量式迁移运行一系列MR任务。批量迁移中两个最昂贵的操作是文件复制和元数据更新，所以这些步骤都是在shuffle阶段分布式进行。每个任务都会打印运行日志数据存储在HDFS，通过日志数据能清晰的看到任务的完成情况。</p>
<p><img src="http://img1.ph.126.net/-svfsAdr08aw8WdKBzFDVg==/6598244045136349852.jpg" alt="此处输入图片的描述"></p>
<p>第一个MR任务从HDFS读取实体标识符并shuffle后均匀的发到reducer。reducer先验证各实体，再对要复制的HDFS目录和实体进行映射；第二个MR任务扫描第一个MR产生的文件目录，并在对应的目录下创建文件。对文件名称进行hash shuffle后发到reducer，一旦shuffle之后reducer执行复制操作；第三个MR任务处理Hive元数据的提交逻辑。</p>
<p>三阶段MR任务计划的扩展性和负载均衡都很好：复制1百万实体的2.2 PB数据消耗大改24小时；同步20 TB 数据量的更新仅花了一个小时。阶段1和阶段3的瓶颈在于Hive元数据MySQL数据库；而阶段2的瓶颈在于网络带宽。</p>
<p>对于我们的迁移，需要开发定制化的文件复制MR来处理HDFS上的数据。然而，对比DistCp这样通用化的工具，在测试ReAir过程中也发现了一些问题：<br>在复制百万个文件或者整个数据仓库时，MR任务初始化比较慢；<br>错误率较高，并且需要定制错误处理；<br>却少易用的日志分析功能。</p>
<p>为了解决这些问题，我们开发了两个MR任务来处理通用HDFS数据复制。第一个任务采用一个启发式的、文件夹遍历的多线程进行一系列分隔。一旦有足够的文件夹，mapper遍历这些文件夹生成文件列表来复制。文件名经过shuffle发送到reducer来决定是否有必要复制。第二个MR任务读取文件列表来复制，并通过一个shuffle进行分布式复制工作。</p>
<h4 id="增量式迁移"><a href="#增量式迁移" class="headerlink" title="增量式迁移"></a>增量式迁移</h4><p>在两个集群的情况下，我们需要在两个集群之间共享数据。例如，日志数据需要每天在生产集群聚会，但同时即席查询的用户也需要这些数据。批量式复制任务对于这种需求显得太重，这时需要两个数据仓库间可以按小时进行更新。即席查询集群的用户需要尽快同步生产集群的数据，因此有必要找到一个尽可能快的方法来更新新的内容。虽然<a href="https://cwiki.apache.org/confluence/display/Hive/Replication" target="_blank" rel="external">有一些开源项目</a>能解决这个问题，但由于Hive版本依赖等问题并不是很理想，后来我们开发增量式复制工具来保证即席查询集群和生产集群的数据同步。</p>
<p>增量式复制工具设计到实体（Hive表和分区）的记录变化，一旦发生变化尽快复制变化量。为了记录源集群上的变化，我们使用了Hive的钩子机制（hook函数）把查询成功的实体写入MySQL数据库。采用这种方法，我们可以跟踪生产集群的所有变化。HDFS上的变化或者元数据的更新都能触发复制。</p>
<p>一旦有了数据库里的变化的记录，我们需要一种方法来复制这些变化到即席查询集群。这个执行机制是通过一个Java服务实现，读取变化的日志中实体，并转化成一系列的行为集合。比如，在源集群成功创建一个表，会在目标集群上翻译成“复制表”的行为。Java进程将调用元数据，并启动MR任务执行。</p>
<p>由于源集群上的变化是序列化到日志里，它将在目的集群中以相同的顺序进行执行。但是，实际情况是，单独的复制一个表或者分区花费几秒钟或者几分钟实在太慢，后面改成多线程并行复制。为了处理并发，所有的行为基于并发限制形成DAG。通常限制都是对于同一个表，比如，在复制分区之前要建立好表。通过并发执行，复制的延迟降低到最小。</p>
<p>用增量式复制可以实现快速、可靠的复制。两个数据仓库之间的同步复制可以实现容灾恢复——如果一个集群宕机，另外一个集群还可以正常提供服务。对比批量式复制，增量式复制对数据仓库体量巨大但改变的数据量比较小的情况更有效率。当前在Airbnb，每天的数据量增长不到2 TB，所以增量式复制比较有意义。<br>使用批量式迁移和增量式迁移，可以快速的迁移两个集群。我们希望这些工具对社区也一样有用。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考:"></a><em>参考</em>:</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/airbnb-hadoop-hive/" itemprop="url">
                  Airbnb的大数据平台架构
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:40:30+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Airbnb成立于2008年8月，拥有世界一流的客户服务和日益增长的用户社区。随着Airbnb的业务日益复杂，其大数据平台数据量也迎来了爆炸式增长。<br>本文为Airbnb公司工程师James Mayfield分析的Airbnb大数据平台构架，提供了详尽的思想和实施。</p>
<h4 id="Part-1：大数据架构背后的哲理"><a href="#Part-1：大数据架构背后的哲理" class="headerlink" title="Part 1：大数据架构背后的哲理"></a>Part 1：大数据架构背后的哲理</h4><p>Airbnb公司提倡数据信息化，凡事以数据说话。收集指标，通过实验验证假设、构建机器学习模型和挖掘商业机会使得Airbnb公司高速、灵活的成长。</p>
<p>经过多版本迭代之后，大数据架构栈基本稳定、可靠和可扩展的。本文分享了Airbnb公司大数据架构经验给社区。后续会给出一系列的文章来讲述分布式架构和使用的相应的组件。James Mayfield说，“我们每天使用着开源社区提供的优秀的项目，这些项目让大家更好的工作。我们在使用这些有用的项目得到好处之后也得反馈社区。”</p>
<p>下面基于在Airbnb公司大数据平台架构构建过程的经验，给出一些有效的观点。</p>
<ul>
<li>多关注开源社区：在开源社区有很多大数据架构方面优秀的资源，需要去采用这些系统。同样，当我们自己开发了有用的项目也最好回馈给社区，这样会良性循环。</li>
<li>多采用标准组件和方法：有时候自己造轮子并不如使用已有的更好资源。当凭直觉去开发出一种“与众不同”的方法时，你得考虑维护和修复这些程序的隐性成本。</li>
<li>确保大数据平台的可扩展性：当前业务数据已不仅仅是随着业务线性增长了，而是爆发性增长。我们得确保产品能满足这种业务的增长。</li>
<li>多倾听同事的反馈来解决问题：倾听公司数据的使用者反馈意见是架构路线图中非常重要的一步。</li>
<li>预留多余资源：集群资源的超负荷使用让我们培养了一种探索无限可能的文化。对于架构团队来说，经常沉浸在早期资源充足的兴奋中，但Airbnb大数据团队总是假设数据仓库的新业务规模比现有机器资源大。</li>
</ul>
<h4 id="Part-2：大数据架构预览"><a href="#Part-2：大数据架构预览" class="headerlink" title="Part 2：大数据架构预览"></a>Part 2：大数据架构预览</h4><p>这里是大数据平台架构一览图。<br><img src="http://img0.ph.126.net/4vmHHpYNkP4mBEcqBi6atw==/6598179173949160803.jpg" alt="此处输入图片的描述"><br>Airbnb数据源主要来自两方面：数据埋点发送事件日志到Kafka；MySQL数据库dumps存储在AWS的RDS，通过数据传输组件Sqoop传输到Hive“金”集群（其实就是Hive集群，只是Airbnb内部有两个Hive集群，分别为“金”集群和“银”集群，具体分开两个集群的原因会在文章末尾给出。）。</p>
<p>包含用户行为以及纬度快照的数据发送到Hive“金”集群存储，并进行数据清洗。这步会做些业务逻辑计算，聚合数据表，并进行数据校验。</p>
<p>在以上架构图中，Hive集群单独区分“金”集群和“银”集群大面上的原因是为了把数据存储和计算进行分离。这样可以保证灾难性恢复。这个架构中，“金”集群运行着更重要的作业和服务，对资源占用和即席查询可以达到无感知。“银”集群只是作为一个产品环境。</p>
<p>“金”集群存储的是原始数据，然后复制“金”集群上的所有数据到“银”集群。但是在“银”集群上生成的数据不会再复制到“金”集群。你可以认为 “银”集群是所有数据的一个超集。由于Airbnb大部分数据分析和报表都出自“银”集群，所以得保证“银”集群能够无延迟的复制数据。更严格的讲，对于“金”集群上已存在的数据进行更新也得迅速的同步到“银”集群。集群间的数据同步优化在开源社区并没有很好的解决方案，Airbnb自己实现了一个工具，后续文章会详细的讲。</p>
<p>在HDFS存储和Hive表的管理方面做了不少优化。数据仓库的质量依赖于数据的不变性（Hive表的分区）。更进一步，Airbnb不提倡建立不同的数据系统，也不想单独为数据源和终端用户报表维护单独的架构。以以往的经验看，中间数据系统会造成数据的不一致性，增加ETL的负担，让回溯数据源到数据指标的演化链变得异常艰难。Airbnb采用Presto来查询Hive表，代替Oracle、 Teradata、 Vertica、 Redshift等。在未来，希望可以直接用Presto连接Tableau。</p>
<p>另外一个值得注意的几个事情，在架构图中的<a href="http://airbnb.io/airpal" target="_blank" rel="external">Airpal</a>，一个基于Presto，web查询系统，已经开源。Airpal是Airbnb公司用户基于数据仓库的即席SQL查询借口，有超过1/3的Airbnb同事在使用此工具查询。任务调度系统<a href="http://airbnb.io/projects/airflow" target="_blank" rel="external">Airflow</a> ，可以跨平台运行Hive，Presto，Spark，MySQL等Job，并提供调度和监控功能。Spark集群时工程师和数据分析师偏爱的工具，可以提供机器学习和流处理。S3作为一个独立的存储，大数据团队从HDFS上收回部分数据，这样可以减少存储的成本。并更新Hive的表指向S3文件，容易访问数据和元数据管理。</p>
<h4 id="Part-2：Hadoop集群演化"><a href="#Part-2：Hadoop集群演化" class="headerlink" title="Part 2：Hadoop集群演化"></a>Part 2：Hadoop集群演化</h4><p>Airbnb公司在今年迁移集群到“金和银”集群。为了后续的可扩展，两年前迁移Amazon EMR到 EC2实例上运行HDFS，存储有300 TB数据。现在，Airbnb公司有两个独立的HDFS集群，存储的数据量达11PB。S3上也存储了几PB数据。</p>
<p>下面是遇到的主要问题和解决方案：<br>A)    基于Mesos运行Hadoop<br>早期Airbnb工程师发现Mesos计算框架可以跨服务发布。在AWS c3.8xlarge机器上搭建集群，在EBS上存储3TB的数据。在Mesos上运行所有Hadoop、 Hive、Presto、 Chronos和Marathon。</p>
<p>基于Mesos的Hadoop集群遇到的问题：</p>
<ul>
<li>Job运行和产生的日志不可见</li>
<li>Hadoop集群健康状态不可见</li>
<li>Mesos只支持MR1</li>
<li>task tracker连接导致性能问题</li>
<li>系统的高负载，并很难定位</li>
<li>不兼容Hadoop安全认证Kerberos</li>
</ul>
<p>解决方法：不自己造轮子，直接采用其它大公司的解决方案。</p>
<p>B)    远程读数据和写数据<br>所有的HDFS数据都存储在持久性数据块级存储卷（EBS），当查询时都是通过网络访问Amazon EC2。Hadoop设计在节点本地读写速度会更快，而现在的部署跟这相悖。</p>
<p>Hadoop集群数据分成三部分存储在AWS一个分区三个节点上，每个节点都在不同的机架上。所以三个不同的副本就存储在不同的机架上，导致一直在远程的读数据和写入数据。这个问题导致在数据移动或者远程复制的过程出现丢失或者崩溃。</p>
<p>解决方法：使用本地存储的实例，并运行在单个节点上。</p>
<p>C)    在同构机器上混布任务<br>纵观所有的任务，发现整体的架构中有两种完全不同的需求配置。Hive/Hadoop/HDFS是存储密集型，基本不耗内存和CPU。而Presto和Spark是耗内存和CPU型，并不怎么需要存储。在AWS c3.8xlarge机器上持久性数据块级存储卷（EBS）里存储3 TB是非常昂贵的。</p>
<p>解决方法：迁移到Mesos计算框架后，可以选择不同类型的机器运行不同的集群。比如，选择AWS c3.8xlarge实例运行Spark。AWS后来发布了“D系列”实例。从AWS c3.8xlarge实例每节点远程的3 TB存储迁移数据到AWS d2.8xlarge 4 TB本地存储，这给Airbnb公司未来三年节约了上亿美元。</p>
<p>D)    HDFS Federation<br>早期Airbnb公司使用Pinky和Brain两个集群联合，数据存储共享，但mappers和reducers是在每个集群上逻辑独立的。这导致用户访问数据需要在Pinky和Brain两个集群都查询一遍。并且这种集群联合不能广泛被支持，运行也不稳定。</p>
<p>解决方法：迁移数据到各HDFS节点，达到机器水平的隔离性，这样更容易容灾。</p>
<p>E)    繁重的系统监控<br>个性化系统架构的严重问题之一是需要自己开发独立的监控和报警系统。Hadoop、Hive和HDFS都是复杂的系统，经常出现各种bug。试图跟踪所有失败的状态，并能设置合适的阈值是一项非常具有挑战性的工作。</p>
<p>解决方法：通过和大数据公司Cloudera签订协议获得专家在架构和运维这些大系统的支持。减少公司维护的负担。Cloudera提供的Manager工具减少了监控和报警的工作。</p>
<h4 id="最后陈述"><a href="#最后陈述" class="headerlink" title="最后陈述"></a>最后陈述</h4><p>在评估老系统的问题和低效率后进行了系统的修复。无感知的迁移PB级数据和成百上千的Jobs是一个长期的过程。作者提出后面会单独写相关的文章，并开源对于的工具给开源社区。</p>
<p>大数据平台的演化给公司减少大量成本，并且优化集群的性能，下面是一些统计数据：</p>
<ul>
<li>磁盘读写数据的速度从70 – 150 MB / sec到400 + MB / sec。</li>
<li>Hive任务提高了两倍的CPU时间</li>
<li>读吞吐量提高了三倍</li>
<li>写吞吐量提高了两倍</li>
<li>成本减少百分之七十</li>
</ul>
<p>本文为Airbnb公司工程师James Mayfield分析的<a href="https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c" target="_blank" rel="external">Airbnb大数据平台构架</a>。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考:"></a><em>参考</em>:</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/spark-two-series-part-2/" itemprop="url">
                  Spark 2.0系列】：Catalog和自定义Optimizer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:18:21+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Catalog和自定义Optimizer"><a href="#Catalog和自定义Optimizer" class="headerlink" title="Catalog和自定义Optimizer"></a>Catalog和自定义Optimizer</h4><p>Spark 2.0系列第一篇见<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483729&amp;idx=1&amp;sn=22c40fd932ba7ffc3684a45e1861dd27#wechat_redirect" target="_blank" rel="external">Spark 2.0系列】：Spark Session API和Dataset API</a>，本文将讲解Spark 2.0 的Catalog 和Custom Optimizer。</p>
<p>首先，先了解下RDD 和Dataset 在开发中使用对比。</p>
<h5 id="RDD-和Dataset-使用对比"><a href="#RDD-和Dataset-使用对比" class="headerlink" title="RDD 和Dataset 使用对比"></a>RDD 和Dataset 使用对比</h5><p>Dataset API 是RDD 和DataFrame API 的统一，但大部分Dataset API 与RDD API使用方法看起来是相似的（其实实现方法是不同的）。所以RDD代码很容易转换成Dataset API。下面直接上代码：</p>
<h6 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h6><ul>
<li>RDD</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> rdd = sparkContext.textFile(<span class="string">"src/main/resources/data.txt"</span>)</div><div class="line">  </div><div class="line"><span class="keyword">val</span> wordsRDD = rdd.flatMap(value =&gt; value.split(<span class="string">"\\s+"</span>))</div><div class="line"><span class="keyword">val</span> wordsPair = wordsRDD.map(word =&gt; (word,<span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCount = wordsPair.reduceByKey(_+_)</div></pre></td></tr></table></figure>
<ul>
<li>Dataset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ds = sparkSession.read.text(<span class="string">"src/main/resources/data.txt"</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> sparkSession.implicits._</div><div class="line"><span class="keyword">val</span> wordsDs = ds.flatMap(value =&gt; value.split(<span class="string">"\\s+"</span>))</div><div class="line"><span class="keyword">val</span> wordsPairDs = wordsDs.groupByKey(value =&gt; value)</div><div class="line"><span class="keyword">val</span> wordCountDs = wordsPairDs.count()</div></pre></td></tr></table></figure>
<h6 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h6><table>
<thead>
<tr>
<th></th>
<th>RDD</th>
<th>Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Caching</td>
<td>rdd.cache()</td>
<td>ds.cache()</td>
</tr>
<tr>
<td>Filter</td>
<td>val filteredRDD = wordsRDD.filter(value =&gt; value ==”hello”)</td>
<td>val filteredDS = wordsDs.filter(value =&gt; value ==”hello”)</td>
</tr>
<tr>
<td>Map Partition</td>
<td>val mapPartitionsRDD = rdd.mapPartitions(iterator =&gt; List(iterator.count(value =&gt; true)).iterator)</td>
<td>val mapPartitionsDs = ds.mapPartitions(iterator =&gt; List(iterator.count(value =&gt; true)).iterator)</td>
</tr>
<tr>
<td>reduceByKey</td>
<td>val reduceCountByRDD = wordsPair.reduceByKey(<em>+</em>)</td>
<td>val reduceCountByDs = wordsPairDs.mapGroups((key,values) =&gt;(key,values.length))</td>
</tr>
</tbody>
</table>
<h6 id="Dataset-和RDD-相互转换"><a href="#Dataset-和RDD-相互转换" class="headerlink" title="Dataset 和RDD 相互转换"></a>Dataset 和RDD 相互转换</h6><ul>
<li>RDD</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> dsToRDD = ds.rdd</div></pre></td></tr></table></figure>
<ul>
<li>Dataset</li>
</ul>
<p>RDD 转换成Dataframe稍麻烦，需要指定schema。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> rddStringToRowRDD = rdd.map(value =&gt; <span class="type">Row</span>(value))</div><div class="line"><span class="keyword">val</span> dfschema = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"value"</span>,<span class="type">StringType</span>)))</div><div class="line"><span class="keyword">val</span> rddToDF = sparkSession.createDataFrame(rddStringToRowRDD,dfschema)</div><div class="line"><span class="keyword">val</span> rDDToDataSet = rddToDF.as[<span class="type">String</span>]</div></pre></td></tr></table></figure>
<h5 id="Catalog-API"><a href="#Catalog-API" class="headerlink" title="Catalog API"></a>Catalog API</h5><p>DataSet 和Dataframe API 支持结构化数据分析，而结构化数据重要的是管理metadata。这里的metadata包括temporary metadata（临时表）；registered udfs；permanent metadata（Hive metadata或HCatalog）。</p>
<p>早期Spark版本并未提供标准的API访问metadata，开发者需要使用类似<em>show tables</em>的查询来查询metadata；而Spark 2.0 在Spark SQL中提供标准API 调用catalog来访问metadata。</p>
<h6 id="访问Catalog"><a href="#访问Catalog" class="headerlink" title="访问Catalog"></a>访问Catalog</h6><p>建立SparkSession，然后调用Catalog：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> catalog = sparkSession.catalog</div></pre></td></tr></table></figure>
<h6 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.listDatabases().select(<span class="string">"name"</span>).show()</div></pre></td></tr></table></figure>
<p><em>listDatabases</em>可查询所有数据库。在Hive中，Catalog可以访问Hive metadata中的数据库。<em>listDatabases</em>返回一个dataset，所以你可以使用适用于dataset的所有操作去处理metadata。</p>
<h6 id="用createTempView-注册Dataframe"><a href="#用createTempView-注册Dataframe" class="headerlink" title="用createTempView 注册Dataframe"></a>用createTempView 注册Dataframe</h6><p>早期版本Spark用<em>registerTempTable</em>注册dataframe，而Spark 2.0 用<em>createTempView</em>替代。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df.createTempView(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>一旦注册视图，即可使用<em>listTables</em>访问所有表。</p>
<h6 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.listTables().select(<span class="string">"name"</span>).show()</div></pre></td></tr></table></figure>
<h6 id="检查表缓存"><a href="#检查表缓存" class="headerlink" title="检查表缓存"></a>检查表缓存</h6><p>通过Catalog可检查表是否缓存。访问频繁的表缓存起来是非常有用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.isCached(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>默认表是不缓存的，所以你会得到false。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.cache()</div><div class="line">catalog.isCached(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>现在将会打印true。</p>
<h6 id="删除视图"><a href="#删除视图" class="headerlink" title="删除视图"></a>删除视图</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.dropTempView(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<h6 id="查询注册函数"><a href="#查询注册函数" class="headerlink" title="查询注册函数"></a>查询注册函数</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">catalog.listFunctions().</div><div class="line">select(<span class="string">"name"</span>,<span class="string">"description"</span>,<span class="string">"className"</span>,<span class="string">"isTemporary"</span>).show(<span class="number">100</span>)</div></pre></td></tr></table></figure>
<p>Catalog不仅能查询表，也可以访问UDF。上面代码会显示Spark Session中所有的注册函数（包括内建函数）。</p>
<h5 id="自定义-Optimizer"><a href="#自定义-Optimizer" class="headerlink" title="自定义 Optimizer"></a>自定义 Optimizer</h5><h6 id="Catalyst-optimizer"><a href="#Catalyst-optimizer" class="headerlink" title="Catalyst optimizer"></a>Catalyst optimizer</h6><p>Spark SQL使用Catalyst优化所有的查询，优化之后的查询比直接操作RDD速度要快。Catalyst是基于rule的，每个rule都有一个特定optimization，比如，<em>ConstantFolding</em> rule用来移除常数表达式，具体可直接看Spark SQL源代码。</p>
<p>在早期版本Spark中，如果想自定义optimization，需要开发者修改Spark源代码。操作起来麻烦，而且要求开发者能读懂源码。在Spark 2.0中，已提供API自定义optimization。</p>
<h6 id="访问Optimized-plan"><a href="#访问Optimized-plan" class="headerlink" title="访问Optimized plan"></a>访问Optimized plan</h6><p>在开始编写自定义optimization之前，先来看看如何访问optimized plan：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> df = sparkSession.read.option(<span class="string">"header"</span>,<span class="string">"true"</span>).csv(<span class="string">"src/main/resources/data.csv"</span>)</div><div class="line"><span class="keyword">val</span> multipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</div><div class="line">println(multipliedDF.queryExecution.optimizedPlan.numberedTreeString)</div></pre></td></tr></table></figure>
<p>上面的代码是加载一个csv文件，并对某一行所有值乘以1。<em>queryExecution</em> 可访问查询相关的所有执行信息。 <em>queryExecution</em> 的<em>optimizedPlan</em>对象可以访问dataframe的optimized plan。</p>
<p>Spark中的执行计划以tree表示，所以用<em>numberedTreeString</em>打印optimized plan。打印结果如下：</p>
<blockquote>
<p>00 Project [(cast(amountPaid#3 as double) <em> 1.0) AS (amountPaid </em> 1)#5]<br>01 +- Relation[transactionId#0,customerId#1,itemId#2,amountPaid#3] csv</p>
</blockquote>
<p>所有执行计划是由底向上读取：</p>
<ul>
<li>01 Relation - 从csv 文件建立一个dataframe</li>
<li>00 Project - 投影操作</li>
</ul>
<h6 id="编写自定义optimizer-rule"><a href="#编写自定义optimizer-rule" class="headerlink" title="编写自定义optimizer rule"></a>编写自定义optimizer rule</h6><p>从上面的执行计划可以清晰的看到：对一列的每个值乘以1 这里并没有优化。我们知道，乘以1 这个操作应该返回的是值本身，所以可以利用这个特点来增加只能点的optimizer。代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultiplyOptimizationRule</span> <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = plan transformAllExpressions &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Multiply</span>(left,right) <span class="keyword">if</span> right.isInstanceOf[<span class="type">Literal</span>] &amp;&amp;</div><div class="line">        right.asInstanceOf[<span class="type">Literal</span>].value.asInstanceOf[<span class="type">Double</span>] == <span class="number">1.0</span> =&gt;</div><div class="line">        println(<span class="string">"optimization of one applied"</span>)</div><div class="line">        left</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>这里MultiplyOptimizationRule扩展自Rule类，采用Scala的模式匹配编写。检测右操作数是否是 1，如果是1 则直接返回左节点。</p>
<p>把MultiplyOptimizationRule加入进optimizer：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sparkSession.experimental.extraOptimizations = <span class="type">Seq</span>(<span class="type">MultiplyOptimizationRule</span>)</div></pre></td></tr></table></figure>
<p>你可以使用<em>extraOptimizations</em>将定义好的Rule加入 catalyst。</p>
<p>下面实际使用看看效果：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> multipliedDFWithOptimization = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</div><div class="line">println(<span class="string">"after optimization"</span>)</div><div class="line">println(multipliedDFWithOptimization.queryExecution.</div><div class="line">optimizedPlan.numberedTreeString)</div></pre></td></tr></table></figure>
<p>我们看到打印结果：</p>
<blockquote>
<p>00 Project [cast(amountPaid#3 as double) AS (amountPaid * 1)#7]<br>01 +- Relation[transactionId#0,customerId#1,itemId#2,amountPaid#3] csv</p>
</blockquote>
<p>说明自定义Optimizer已生效。</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/15/spark-two-series-part-1/" itemprop="url">
                  【Spark 2.0系列】：Spark Session API和Dataset API
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-15T12:57:22+08:00" content="2016-08-15">
              2016-08-15
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="Dataset：Spark新的抽象层"><a href="#Dataset：Spark新的抽象层" class="headerlink" title="Dataset：Spark新的抽象层"></a>Dataset：Spark新的抽象层</h5><p>Spark最原始的抽象基础是RDD（分布式弹性数据集），但是从Spark 2.0 开始，Dataset将成为Spark新的抽象层。所有的Spark开发者将使用Dataset API和Dataframe（Dataset子集）API编写代码，同时RDD API也还是可以用的，不过已降为low-level的API。</p>
<p>Dataframe API 在Spark 1.3时被引入，Dataset是Dataframe的超集。Dataset API和Dataframe API的使用带给Spark更好的性能和灵活性。Spark Streaming也将使用Dataset代替RDD。</p>
<h5 id="Spark-Session：Spark-2-0入口"><a href="#Spark-Session：Spark-2-0入口" class="headerlink" title="Spark Session：Spark 2.0入口"></a>Spark Session：Spark 2.0入口</h5><p>在Spark早期版本，<em>spark context</em>是Spark的入口，RDD API通过<em>context</em> API创建。相应地，<em>streaming</em>由<em>StreamingContext</em>创建；<em>SQL</em>由<em>sqlContext</em>创建；<em>hive</em>由<em>HiveContext</em>创建。而到了Spark 2.0，DataSet和Dataframe API由<em>Spark Session</em>创建。</p>
<p>SparkSession包括SQLContext，HiveContext和StreamingContext的功能。Spark session实际起计算的还是spark context。</p>
<p>下面直接看代码吧。</p>
<h4 id="创建SparkSession"><a href="#创建SparkSession" class="headerlink" title="创建SparkSession"></a>创建SparkSession</h4><p>使用工厂模式创建SparkSession。下面是创建SparkSession的代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder.</div><div class="line">      master(<span class="string">"local"</span>)</div><div class="line">      .appName(<span class="string">"spark session example"</span>)</div><div class="line">      .getOrCreate()</div></pre></td></tr></table></figure>
<p>上面的代码类似于创建<em>SparkContext</em>和<em>SQLContext</em>。如果你需要创建<em>hive context</em>，你可以使用下面的代码创建SparkSession，并支持Hive。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder.</div><div class="line">      master(<span class="string">"local"</span>)</div><div class="line">      .appName(<span class="string">"spark session example"</span>)</div><div class="line">      .enableHiveSupport()</div><div class="line">      .getOrCreate()</div></pre></td></tr></table></figure>
<p><em>enableHiveSupport</em>开启Hive支持后就可以像<em>HiveContext</em>一样使用。</p>
<p>创建Spark Session后，可以来读取数据了。</p>
<h4 id="使用Spark-Session读取数据"><a href="#使用Spark-Session读取数据" class="headerlink" title="使用Spark Session读取数据"></a>使用<em>Spark Session</em>读取数据</h4><p>使用<em>Spark Session</em>读取CSV数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> df = sparkSession.read.option(<span class="string">"header"</span>,<span class="string">"true"</span>).</div><div class="line">    csv(<span class="string">"src/main/resources/sales.csv"</span>)</div></pre></td></tr></table></figure>
<p>上面的代码与SQLContext类似，你可以复用原有SQLContext的代码。</p>
<h4 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h4><p>下面来个完整的WordCount例子：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">#create <span class="type">SparkSession</span></div><div class="line"><span class="keyword">val</span> sparkSession = <span class="type">SparkSession</span>.builder.</div><div class="line">      master(<span class="string">"local"</span>)</div><div class="line">      .appName(<span class="string">"example"</span>)</div><div class="line">      .getOrCreate()</div><div class="line"></div><div class="line">#read data and convert to <span class="type">Dataset</span></div><div class="line"><span class="keyword">import</span> sparkSession.implicits._</div><div class="line"></div><div class="line"><span class="keyword">val</span> data = sparkSession.read.text(<span class="string">"src/main/resources/data.txt"</span>).as[<span class="type">String</span>]</div><div class="line"></div><div class="line">#split and group by word</div><div class="line"><span class="keyword">val</span> words = data.flatMap(value =&gt; value.split(<span class="string">"\\s+"</span>))</div><div class="line"><span class="keyword">val</span> groupedWords = words.groupByKey(_.toLowerCase)</div><div class="line"></div><div class="line">#count</div><div class="line"><span class="keyword">val</span> counts = groupedWords.count()</div><div class="line"></div><div class="line">#print results</div><div class="line">counts.show()</div></pre></td></tr></table></figure></p>
<p>代码较简单，就不做解释了。</p>
<h4 id="话外音"><a href="#话外音" class="headerlink" title="话外音"></a>话外音</h4><p>SQLContext和HiveContext在Spark 2.0中会继续使用，因为Spark是向后兼容的。但很明显，Spark官方文档建议以后使用SparkSession作为入口。</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/12/graph-of-thrones-neo4j-social-network-analysis/" itemprop="url">
                  基于社区发现算法和图分析Neo4j解读《权力的游戏》
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-12T00:36:55+08:00" content="2016-08-12">
              2016-08-12
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>导读</em>：几个月前，数学家 Andrew Beveridge和Jie Shan在数学杂志上发表<a href="https://www.macalester.edu/~abeverid/thrones.html" target="_blank" rel="external">《权力的网络》</a>，主要分析畅销小说《冰与火之歌》第三部《冰雨的风暴》中人物关系，其已经拍成电视剧《权力的游戏》系列。他们在论文中介绍了如何通过文本分析和实体提取构建人物关系的网络。紧接着，使用社交网络分析算法对人物关系网络分析找出最重要的角色；应用社区发现算法来找到人物聚类。</p>
<p>其中的分析和可视化是用Gephi做的，Gephi是非常流行的图分析工具。但作者觉得使用Neo4j来实现更有趣。</p>
<h5 id="导入原始数据到Neo4j"><a href="#导入原始数据到Neo4j" class="headerlink" title="导入原始数据到Neo4j"></a>导入原始数据到Neo4j</h5><p>原始数据可从<a href="https://www.macalester.edu/~abeverid/data/stormofswords.csv" target="_blank" rel="external">网络上下载</a>，格式如下：</p>
<pre><code>Source,Target,Weight
Aemon,Grenn,5
Aemon,Samwell,31
Aerys,Jaime,18
...
</code></pre><p>上面是人物关系的之邻接表以及关系权重。作者使用简单的数据模型：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(:Character &#123;name&#125;)-[:INTERACTS]-&gt;(:Character &#123;name&#125;)</div></pre></td></tr></table></figure></p>
<p>带有标签<em>Character</em>的节点代表小说中的角色，用单向关系类型<em>INTERACTS</em>代表小说中的角色有过接触。节点属性会存储角色的名字<em>name</em>，两角色间接触的次数作为关系的属性：权重（<em>weight</em>）。</p>
<p>首先创建节点c，并做唯一限制性约束，<em>c.name</em>唯一，保证schema的完整性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CREATE CONSTRAINT ON (c:Character) ASSERT c.name IS UNIQUE;</div></pre></td></tr></table></figure></p>
<p>一旦约束创建即相应的创建索引，这将有助于通过角色的名字查询的性能。作者使用Neo4j的Cypher（Cypher是一种声明式图查询语言，能表达高效查询和更新图数据库）<em>LOAD CSV</em>语句导入数据：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">LOAD CSV WITH HEADERS FROM "https://www.macalester.edu/~abeverid/data/stormofswords.csv" AS row</div><div class="line">MERGE (src:Character &#123;name: row.Source&#125;)</div><div class="line">MERGE (tgt:Character &#123;name: row.Target&#125;)</div><div class="line">MERGE (src)-[r:INTERACTS]-&gt;(tgt)</div><div class="line">SET r.weight = toInt(row.Weight)</div></pre></td></tr></table></figure></p>
<p>这样得到一个简单的数据模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CALL apoc.meta.graph()</div></pre></td></tr></table></figure></p>
<p><img src="http://img0.ph.126.net/iKaRxZgqA_gA2_mWqX8HIg==/6631569143001537963.png" alt="此处输入图片的描述"></p>
<p>图1 ：《权力的游戏》模型的图。Character角色节点由INTERACTS关系联结</p>
<p>我们能可视化整个图形，但是这并不能给我们很多信息，比如哪些是最重要的人物，以及他们相互接触的信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MATCH p=(:Character)-[:INTERACTS]-(:Character)</div><div class="line">RETURN p</div></pre></td></tr></table></figure></p>
<p><img src="http://img1.ph.126.net/Enc8q7NSeSO6Z4z2iTvuDw==/6631651606373623600.jpg" alt="此处输入图片的描述"></p>
<p>图2</p>
<h4 id="人物网络分析"><a href="#人物网络分析" class="headerlink" title="人物网络分析"></a>人物网络分析</h4><p>作者使用Neo4j的图查询语言Cypher来做《权力的游戏》图分析，应用到了网络分析的一些工具，具体见<a href="https://www.cs.cornell.edu/home/kleinber/networks-book/" target="_blank" rel="external">《网络，人群和市场：关于高度连接的世界》</a>。</p>
<h5 id="人物数量"><a href="#人物数量" class="headerlink" title="人物数量"></a>人物数量</h5><p>万事以简单开始。先看看上图上由有多少人物：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character) RETURN count(c)</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th>count(c)</th>
</tr>
</thead>
<tbody>
<tr>
<td>107</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="概要统计"><a href="#概要统计" class="headerlink" title="概要统计"></a>概要统计</h5><p>统计每个角色接触的其它角色的数目：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)-[:INTERACTS]-&gt;()</div><div class="line">WITH c, count(*) AS num</div><div class="line">RETURN min(num) AS min, max(num) AS max, avg(num) AS avg_characters, stdev(num) AS stdev</div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th>min</th>
<th>max</th>
<th>avg_characters</th>
<th>stdev</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>24</td>
<td>4.957746478873241</td>
<td>6.227672391875085</td>
</tr>
</tbody>
</table>
<h5 id="图（网络）的直径"><a href="#图（网络）的直径" class="headerlink" title="图（网络）的直径"></a>图（网络）的直径</h5><p>网络的直径或者测底线或者最长最短路径：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">// Find maximum diameter of network</div><div class="line">// maximum shortest path between two nodes</div><div class="line">MATCH (a:Character), (b:Character) WHERE id(a) &gt; id(b)</div><div class="line">MATCH p=shortestPath((a)-[:INTERACTS*]-(b))</div><div class="line">RETURN length(p) AS len, extract(x IN nodes(p) | x.name) AS path</div><div class="line">ORDER BY len DESC LIMIT <span class="number">4</span></div></pre></td></tr></table></figure></p>
<table>
<thead>
<tr>
<th>len</th>
<th>path   </th>
</tr>
</thead>
<tbody>
<tr>
<td>6</td>
<td>[Illyrio, Belwas, Daenerys, Robert, Tywin, Oberyn, Amory] </td>
</tr>
<tr>
<td>6</td>
<td>[Illyrio, Belwas, Daenerys, Robert, Sansa, Bran, Jojen]</td>
</tr>
<tr>
<td>6</td>
<td>[Illyrio, Belwas, Daenerys, Robert, Stannis, Davos, Shireen]</td>
</tr>
<tr>
<td>6</td>
<td>[Illyrio, Belwas, Daenerys, Robert, Sansa, Bran, Luwin]</td>
</tr>
</tbody>
</table>
<p>我们能看到网络中有许多长度为6的路径。</p>
<h5 id="最短路径"><a href="#最短路径" class="headerlink" title="最短路径"></a>最短路径</h5><p>作者使用Cypher 的shortestPath函数找到图中任意两个角色之间的最短路径。让我们找出凯特琳·史塔克（Catelyn Stark ）和卓戈·卡奥（Kahl Drogo）之间的最短路径：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// Shortest path <span class="keyword">from</span> Catelyn Stark to Khal Drogo</div><div class="line">MATCH (catelyn:Character &#123;name: <span class="string">"Catelyn"</span>&#125;), (drogo:Character &#123;name: <span class="string">"Drogo"</span>&#125;)</div><div class="line">MATCH p=shortestPath((catelyn)-[INTERACTS*]-(drogo))</div><div class="line">RETURN p</div></pre></td></tr></table></figure></p>
<p><img src="http://img1.ph.126.net/hM5i7QenCvEwrv6FxkjpRA==/6631649407350368299.jpg" alt="此处输入图片的描述"></p>
<p>图3</p>
<h5 id="所有最短路径"><a href="#所有最短路径" class="headerlink" title="所有最短路径"></a>所有最短路径</h5><p>联结凯特琳·史塔克（Catelyn Stark ）和卓戈·卡奥（Kahl Drogo）之间的最短路径可能还有其它路径，我们可以使用Cypher的<em>allShortestPaths</em>函数来查找：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// All shortest paths <span class="keyword">from</span> Catelyn Stark to Khal Drogo</div><div class="line">MATCH (catelyn:Character &#123;name: <span class="string">"Catelyn"</span>&#125;), (drogo:Character &#123;name: <span class="string">"Drogo"</span>&#125;)</div><div class="line">MATCH p=allShortestPaths((catelyn)-[INTERACTS*]-(drogo))</div><div class="line">RETURN p</div></pre></td></tr></table></figure>
<p><img src="http://img1.ph.126.net/P_R0YArgypcQJ9qK34-FlA==/6631481182071320497.jpg" alt="此处输入图片的描述"></p>
<p>图4</p>
<h5 id="关键节点"><a href="#关键节点" class="headerlink" title="关键节点"></a>关键节点</h5><p>在网络中，如果一个节点位于其它两个节点所有的最短路径上，即称为关键节点。下面我们找出网络中所有的关键节点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">// Find all pivotal nodes <span class="keyword">in</span> network</div><div class="line">MATCH (a:Character), (b:Character)</div><div class="line">MATCH p=allShortestPaths((a)-[:INTERACTS*]-(b)) WITH collect(p) AS paths, a, b</div><div class="line">MATCH (c:Character) WHERE all(x IN paths WHERE c IN nodes(x)) AND NOT c IN [a,b]</div><div class="line">RETURN a.name, b.name, c.name AS PivotalNode SKIP <span class="number">490</span> LIMIT <span class="number">10</span></div></pre></td></tr></table></figure>
<pre><code>|a.name |b.name |PivotalNode|
</code></pre><p>—|—|—|<br>                          |Aegon  |Thoros |Daenerys   |<br>                          |Aegon  |Thoros |Robert     |<br>                          |Drogo  |Ramsay |Robb       |<br>                          |Styr   |Daario |Daenerys   |<br>                          |Styr   |Daario |Jon        |<br>                          |Styr   |Daario |Robert     |<br>                          |Qhorin |Podrick|Jon        |<br>                          |Qhorin |Podrick|Sansa      |<br>                          |Orell  |Theon  |Jon        |<br>                          |Illyrio|Bronn  |Belwas     |</p>
<p>从结果表格中我们可以看出有趣的结果：罗柏·史塔克（Robb）是卓戈·卡奥（Drogo）和拉姆塞·波顿（Ramsay）的关键节点。这意味着，所有联结卓戈·卡奥（Drogo）和拉姆塞·波顿（Ramsay）的最短路径都要经过罗柏·史塔克（Robb）。我们可以通过可视化卓戈·卡奥（Drogo）和拉姆塞·波顿（Ramsay）之间的所有最短路径来验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MATCH (a:Character &#123;name: <span class="string">"Drogo"</span>&#125;), (b:Character &#123;name: <span class="string">"Ramsay"</span>&#125;)</div><div class="line">MATCH p=allShortestPaths((a)-[:INTERACTS*]-(b))</div><div class="line">RETURN p</div></pre></td></tr></table></figure></p>
<p><img src="http://img0.ph.126.net/CV_gvGLBdVUxJttOCdRgdQ==/6631809936048025407.jpg" alt="此处输入图片的描述"></p>
<p>图5</p>
<h4 id="节点中心度"><a href="#节点中心度" class="headerlink" title="节点中心度"></a>节点中心度</h4><p><a href="https://en.wikipedia.org/wiki/Centrality" target="_blank" rel="external">节点中心度</a>给出网络中节点的重要性的相对度量。有许多不同的方式来度量中心度，每种方式都代表不同类型的“重要性”。</p>
<h5 id="度中心性-Degree-Centrality"><a href="#度中心性-Degree-Centrality" class="headerlink" title="度中心性(Degree Centrality)"></a>度中心性(Degree Centrality)</h5><p>度中心性是最简单度量，即为某个节点在网络中的联结数。在《权力的游戏》的图中，某个角色的度中心性是指该角色接触的其他角色数。作者使用Cypher计算度中心性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)-[:INTERACTS]-()</div><div class="line">RETURN c.name AS character, count(*) AS degree ORDER BY degree DESC</div></pre></td></tr></table></figure></p>
<pre><code>|character|degree|
</code></pre><p>—|—|<br>                              |Tyrion   |36    |<br>                              |Jon      |26    |<br>                              |Sansa    |26    |<br>                              |Robb     |25    |<br>                              |Jaime    |24    |<br>                              |Tywin    |22    |<br>                              |Cersei   |20    |<br>                              |Arya     |19    |<br>                              |Joffrey  |18    |<br>                              |Robert   |18    |<br>从上面可以发现，在《权力的游戏》网络中提利昂·兰尼斯特（Tyrion）和最多的角色有接触。鉴于他的心计，我们觉得这是有道理的。</p>
<h5 id="加权度中心性（Weighted-Degree-Centrality）"><a href="#加权度中心性（Weighted-Degree-Centrality）" class="headerlink" title="加权度中心性（Weighted Degree Centrality）"></a>加权度中心性（Weighted Degree Centrality）</h5><p>作者存储一对角色接触的次数作为<em>INTERACTS</em>关系的<em>weight</em>属性。对该角色的<em>INTERACTS</em>关系的所有<em>weight</em>相加得到加权度中心性。作者使用Cypher计算所有角色的这个度量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)-[r:INTERACTS]-()</div><div class="line">RETURN c.name AS character, sum(r.weight) AS weightedDegree ORDER BY weightedDegree DESC</div></pre></td></tr></table></figure></p>
<pre><code>|character|weightedDegree|
</code></pre><p>—|—|<br>                        |Tyrion   |551           |<br>                        |Jon      |442           |<br>                        |Sansa    |383           |<br>                        |Jaime    |372           |<br>                        |Bran     |344           |<br>                        |Robb     |342           |<br>                        |Samwell  |282           |<br>                        |Arya     |269           |<br>                        |Joffrey  |255           |<br>                        |Daenerys |232           |</p>
<h5 id="介数中心性（Betweenness-Centrality）"><a href="#介数中心性（Betweenness-Centrality）" class="headerlink" title="介数中心性（Betweenness Centrality）"></a>介数中心性（Betweenness Centrality）</h5><p><a href="https://en.wikipedia.org/wiki/Betweenness_centrality" target="_blank" rel="external">介数中心性</a>：在网络中，一个节点的介数中心性是指其它两个节点的所有最短路径都经过这个节点，则这些所有最短路径数即为此节点的介数中心性。介数中心性是一种重要的度量，因为它可以鉴别出网络中的“信息中间人”或者网络聚类后的联结点。<br><img src="http://img0.ph.126.net/NVe2tFIgH-cRJVLeAn3cGQ==/6631645009303857231.png" alt="此处输入图片的描述"></p>
<p>图6中红色节点是具有高的介数中心性，网络聚类的联结点。</p>
<p>为了计算介数中心性，作者使用<a href="https://github.com/neo4j-contrib/neo4j-apoc-procedures" target="_blank" rel="external">Neo4j 3.x或者apoc库</a>。安装apoc后能用Cypher调用其170+的程序：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)</div><div class="line">WITH collect(c) AS characters</div><div class="line">CALL apoc.algo.betweenness([<span class="string">'INTERACTS'</span>], characters, <span class="string">'BOTH'</span>) YIELD node, score</div><div class="line">SET node.betweenness = score</div><div class="line">RETURN node.name AS name, score ORDER BY score DESC</div></pre></td></tr></table></figure></p>
<pre><code>|name    |score             |
</code></pre><p>—|—|<br>                        |Jon     |1279.7533534055322|<br>                        |Robert  |1165.6025171231624|<br>                        |Tyrion  |1101.3849724234349|<br>                        |Daenerys|874.8372110508583 |<br>                        |Robb    |706.5572832464792 |<br>                        |Sansa   |705.1985623519137 |<br>                        |Stannis |571.5247305125714 |<br>                        |Jaime   |556.1852522889822 |<br>                        |Arya    |443.01358430043337|<br>                        |Tywin   |364.7212195528086 |</p>
<h5 id="紧度中心性（Closeness-centrality）"><a href="#紧度中心性（Closeness-centrality）" class="headerlink" title="紧度中心性（Closeness centrality）"></a>紧度中心性（Closeness centrality）</h5><p><a href="https://en.wikipedia.org/wiki/Centrality#Closeness_centrality" target="_blank" rel="external">紧度中心性</a>是指到网络中所有其他角色的平均距离的倒数。在图中，具有高紧度中心性的节点在聚类社区之间被高度联结，但在社区之外不一定是高度联结的。<br><img src="http://img1.ph.126.net/vJy1k9fWFZRryZLBHkG1MA==/6631732970234066134.png" alt="此处输入图片的描述"></p>
<p>图7 ：网络中具有高紧度中心性的节点被其它节点高度联结<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)</div><div class="line">WITH collect(c) AS characters</div><div class="line">CALL apoc.algo.closeness([<span class="string">'INTERACTS'</span>], characters, <span class="string">'BOTH'</span>) YIELD node, score</div><div class="line">RETURN node.name AS name, score ORDER BY score DESC</div></pre></td></tr></table></figure></p>
<pre><code>|name   |score                |
</code></pre><p>—|—|<br>                        |Tyrion |0.004830917874396135 |<br>                        |Sansa  |0.004807692307692308 |<br>                        |Robert |0.0047169811320754715|<br>                        |Robb   |0.004608294930875576 |<br>                        |Arya   |0.0045871559633027525|<br>                        |Jaime  |0.004524886877828055 |<br>                        |Stannis|0.004524886877828055 |<br>                        |Jon    |0.004524886877828055 |<br>                        |Tywin  |0.004424778761061947 |<br>                        |Eddard |0.004347826086956522 |</p>
<h4 id="使用python-igraph"><a href="#使用python-igraph" class="headerlink" title="使用python-igraph"></a>使用python-igraph</h4><p>Neo4j与其它工具（比如，R和Python数据科学工具）完美结合。我们继续使用apoc运行 <a href="https://neo4j-contrib.github.io/neo4j-apoc-procedures/#_pagerank_algorithm" target="_blank" rel="external">PageRank</a>和<a href="https://neo4j-contrib.github.io/neo4j-apoc-procedures/#_graph_algorithms" target="_blank" rel="external">社区发现</a>（community detection）算法。这里接着使用<a href="http://igraph.org/python/" target="_blank" rel="external">python-igraph</a>计算分析。Python-igraph移植自R的igraph图形分析库。 使用<em>pip install python-igraph</em>安装它。</p>
<h5 id="从Neo4j构建一个igraph实例"><a href="#从Neo4j构建一个igraph实例" class="headerlink" title="从Neo4j构建一个igraph实例"></a>从Neo4j构建一个igraph实例</h5><p>为了在《权力的游戏》的数据的图分析中使用igraph，首先需要从Neo4j拉取数据，用Python建立igraph实例。作者使用 Neo4j 的Python驱动库<a href="http://py2neo.org/" target="_blank" rel="external">py2neo</a>。我们能直接传入Py2neo查询结果对象到igraph的<em>TupleList</em>构造器，创建igraph实例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> py2neo <span class="keyword">import</span> Graph</div><div class="line"><span class="keyword">from</span> igraph <span class="keyword">import</span> Graph <span class="keyword">as</span> IGraph</div><div class="line">graph = Graph()</div><div class="line"></div><div class="line">query = <span class="string">'''</span></div><div class="line">MATCH (c1:Character)-[r:INTERACTS]-&gt;(c2:Character)</div><div class="line">RETURN c1.name, c2.name, r.weight AS weight</div><div class="line">'''</div><div class="line"></div><div class="line">ig = IGraph.TupleList(graph.run(query), weights=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p>现在有了igraph对象，可以运行igraph实现的各种图算法来。</p>
<h5 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h5><p>作者使用igraph运行的第一个算法是PageRank。PageRank算法源自Google的网页排名。它是一种特征向量中心性(<a href="https://en.wikipedia.org/wiki/Centrality#Eigenvector_centrality" target="_blank" rel="external">eigenvector centrality</a>)算法。</p>
<p>在igraph实例中运行PageRank算法，然后把结果写回Neo4j，在角色节点创建一个pagerank属性存储igraph计算的值：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">pg = ig.pagerank()</div><div class="line">pgvs = []</div><div class="line"><span class="keyword">for</span> p <span class="keyword">in</span> zip(ig.vs, pg):</div><div class="line">    print(p)</div><div class="line">    pgvs.append(&#123;<span class="string">"name"</span>: p[<span class="number">0</span>][<span class="string">"name"</span>], <span class="string">"pg"</span>: p[<span class="number">1</span>]&#125;)</div><div class="line">pgvs</div><div class="line"></div><div class="line">write_clusters_query = <span class="string">'''</span></div><div class="line">UNWIND &#123;nodes&#125; AS n</div><div class="line">MATCH (c:Character) WHERE c.name = n.name</div><div class="line">SET c.pagerank = n.pg</div><div class="line">'''</div><div class="line"></div><div class="line">graph.run(write_clusters_query, nodes=pgvs)</div></pre></td></tr></table></figure></p>
<p>现在可以在Neo4j的图中查询最高PageRank值的节点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MATCH (n:Character)</div><div class="line">RETURN n.name AS name, n.pagerank AS pagerank ORDER BY pagerank DESC LIMIT <span class="number">10</span></div></pre></td></tr></table></figure>
<pre><code>|name    |pagerank            |
</code></pre><p>—|—|<br>                        |Tyrion  |0.042884981999963316|<br>                        |Jon     |0.03582869669163558 |<br>                        |Robb    |0.03017114665594764 |<br>                        |Sansa   |0.030009716660108578|<br>                        |Daenerys|0.02881425425830273 |<br>                        |Jaime   |0.028727587587471206|<br>                        |Tywin   |0.02570016262642541 |<br>                        |Robert  |0.022292016521362864|<br>                        |Cersei  |0.022287327589773507|<br>                        |Arya    |0.022050209663844467|</p>
<h5 id="社区发现（Community-detection）"><a href="#社区发现（Community-detection）" class="headerlink" title="社区发现（Community detection）"></a>社区发现（Community detection）</h5><p><img src="http://img0.ph.126.net/pNb2DRMcw-bj4O7C61rC_w==/6631491077675968998.png" alt="此处输入图片的描述"></p>
<p>图8</p>
<p>社区发现算法用来找出图中的社区聚类。作者使用igraph实现的<a href="http://arxiv.org/abs/physics/0512106" target="_blank" rel="external">随机游走算法</a>（ walktrap）来找到在社区中频繁有接触的角色社区，在社区之外角色不怎么接触。</p>
<p>在igraph中运行随机游走的社区发现算法，然后把社区发现的结果导入Neo4j，其中每个角色所属的社区用一个整数来表示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">clusters = IGraph.community_walktrap(ig, weights=<span class="string">"weight"</span>).as_clustering()</div><div class="line"></div><div class="line">nodes = [&#123;<span class="string">"name"</span>: node[<span class="string">"name"</span>]&#125; <span class="keyword">for</span> node <span class="keyword">in</span> ig.vs]</div><div class="line"><span class="keyword">for</span> node <span class="keyword">in</span> nodes:</div><div class="line">    idx = ig.vs.find(name=node[<span class="string">"name"</span>]).index</div><div class="line">    node[<span class="string">"community"</span>] = clusters.membership[idx]</div><div class="line"></div><div class="line">write_clusters_query = <span class="string">'''</span></div><div class="line">UNWIND &#123;nodes&#125; AS n</div><div class="line">MATCH (c:Character) WHERE c.name = n.name</div><div class="line">SET c.community = toInt(n.community)</div><div class="line">'''</div><div class="line"></div><div class="line">graph.run(write_clusters_query, nodes=nodes)</div></pre></td></tr></table></figure></p>
<p>我们能在Neo4j中查询有多少个社区以及每个社区的成员数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">MATCH (c:Character)</div><div class="line">WITH c.community AS cluster, collect(c.name) AS  members</div><div class="line">RETURN cluster, members ORDER BY cluster ASC</div></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th></th>
<th>cluster</th>
<th>members</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>0</td>
<td>[Aemon, Alliser, Craster, Eddison, Gilly, Janos, Jon, Mance, Rattleshirt, Samwell, Val, Ygritte, Grenn, Karl, Bowen, Dalla, Orell, Qhorin, Styr]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>1</td>
<td>[Aerys, Amory, Balon, Brienne, Bronn, Cersei, Gregor, Jaime, Joffrey, Jon Arryn, Kevan, Loras, Lysa, Meryn, Myrcella, Oberyn, Podrick, Renly, Robert, Robert Arryn, Sansa, Shae, Tommen, Tyrion, Tywin, Varys, Walton, Petyr, Elia, Ilyn, Pycelle, Qyburn, Margaery, Olenna, Marillion, Ellaria, Mace, Chataya, Doran]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>2</td>
<td>[Arya, Beric, Eddard, Gendry, Sandor, Anguy, Thoros]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>3</td>
<td>[Brynden, Catelyn, Edmure, Hoster, Lothar, Rickard, Robb, Roose, Walder, Jeyne, Roslin, Ramsay]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>4</td>
<td>[Bran, Hodor, Jojen, Luwin, Meera, Rickon, Nan, Theon]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>5</td>
<td>[Belwas, Daario, Daenerys, Irri, Jorah, Missandei, Rhaegar, Viserys, Barristan, Illyrio, Drogo, Aegon, Kraznys, Rakharo, Worm]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>6</td>
<td>[Davos, Melisandre, Shireen, Stannis, Cressen, Salladhor]</td>
<td></td>
</tr>
<tr>
<td></td>
<td>7</td>
<td>[Lancel]</td>
<td></td>
</tr>
</tbody>
</table>
<h5 id="角色“大合影”"><a href="#角色“大合影”" class="headerlink" title="角色“大合影”"></a>角色“大合影”</h5><p>《权力的游戏》的权力图。节点的大小正比于介数中心性，颜色表示社区（由随机游走算法获得），边的厚度正比于两节点接触的次数。<br>现在已经计算好这些图的分析数据，让我们对其进行可视化，让数据看起来更有意义。</p>
<p>Neo4j自带浏览器可以对Cypher查询的结果进行很好的可视化，但如果我们想把可视化好的图嵌入到其它应用中，可以使用Javascript可视化库<a href="http://visjs.org/" target="_blank" rel="external">Vis.js</a>。从Neo4j拉取数据，用Vis.js的<a href="https://github.com/johnymontana/neovis.js" target="_blank" rel="external">neovis.js</a>构建可视化图。Neovis.js提供简单的API配置，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">var config = &#123;</div><div class="line">  container_id: <span class="string">"viz"</span>,</div><div class="line">  server_url: <span class="string">"localhost"</span>,</div><div class="line">  labels: &#123;</div><div class="line">    <span class="string">"Character"</span>: <span class="string">"name"</span></div><div class="line">  &#125;,</div><div class="line">  label_size: &#123;</div><div class="line">    <span class="string">"Character"</span>: <span class="string">"betweenness"</span></div><div class="line">  &#125;,</div><div class="line">  relationships: &#123;</div><div class="line">    <span class="string">"INTERACTS"</span>: null</div><div class="line">  &#125;,</div><div class="line">  relationship_thickness: &#123;</div><div class="line">    <span class="string">"INTERACTS"</span>: <span class="string">"weight"</span></div><div class="line">  &#125;,</div><div class="line">  cluster_labels: &#123;</div><div class="line">    <span class="string">"Character"</span>: <span class="string">"community"</span></div><div class="line">  &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line">var viz = new NeoVis(config);</div><div class="line">viz.render();</div></pre></td></tr></table></figure></p>
<p>其中：</p>
<ul>
<li>节点带有标签Character，属性name；</li>
<li>节点的大小正比于betweenness属性；</li>
<li>可视化中包括INTERACTS关系；</li>
<li>关系的厚度正比于weight属性；</li>
<li>节点的颜色是根据网络中社区community属性决定；</li>
<li>从本地服务器localhost拉取Neo4j的数据；</li>
<li>在一个id为viz的DOM元素中展示可视化。</li>
</ul>
<p>译者介绍：侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。<br>英文原文：<a href="http://www.lyonwj.com/2016/06/26/graph-of-thrones-neo4j-social-network-analysis/" target="_blank" rel="external">Analyzing the Graph of Thrones</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/12/first-contact-with-tensorflow-1/" itemprop="url">
                  深度学习快速实践：TensorFlow
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-12T00:14:45+08:00" content="2016-08-12">
              2016-08-12
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>写在之前</em>：本书是翻译自&lt;<a href="http://www.jorditorres.org/first-contact-with-tensorflow/" target="_blank" rel="external">first contact with tensorflow</a>&gt;，已获原作者授权。</p>
<p>作者写这本书的目的是为了快速实践深度学习。因此，读者应该有基本的机器学习知识和必要的数据知识。本书将采用主流的机器学习算法来进行tensorflow训练。</p>
<p>第一章将简短介绍TensorFlow代码和编程模型。</p>
<h4 id="Tensorflow基本概念"><a href="#Tensorflow基本概念" class="headerlink" title="Tensorflow基本概念"></a>Tensorflow基本概念</h4><p>本章将简短介绍TensorFlow代码和编程模型。学完本章后，期待读者会TensorFlow的package安装方法。</p>
<h5 id="Tensorflow开源包"><a href="#Tensorflow开源包" class="headerlink" title="Tensorflow开源包"></a>Tensorflow开源包</h5><p>TensorFlow源于Google大脑团队的机器学习和深度神经网络研究。这个系统能够解决更广泛的机器学习问题。</p>
<p>TensorFlow使用计算图来表示一个计算任务。图中的节点代表数学运算，也可以表示数据的输入、输出和读写等操作；图中的边表示多维数组(Tensors)，节点之间的某种联系。</p>
<p>TensorFlow使用计算图来构建计算过程，用符号来表示计算操作。这使得TensorFlow可以同时运用Linux 64位操作系统的CPU 和GPU 性能，TensorFlow也可以在移动端Android 或者iOS 上执行。</p>
<p>TensorFlow的可视化模块TensorBoard可监控算法的运行状态并显示。</p>
<h5 id="TensorFlow-Serving"><a href="#TensorFlow-Serving" class="headerlink" title="TensorFlow Serving"></a>TensorFlow Serving</h5><p>Google最近开源了<a href="https://tensorflow.github.io/serving" target="_blank" rel="external">TensorFlow Serving</a>，TensorFlow Serving可以帮助机器学习开发者将他们的TensorFlow机器学习模型（可以扩展到其它各类型的机器学习模型）加载到产品中。TensorFlow Serving采用C++编写，并已开源到github。</p>
<h4 id="TensorFlow和TensorFlow-Serving到底有啥区别呢？"><a href="#TensorFlow和TensorFlow-Serving到底有啥区别呢？" class="headerlink" title="TensorFlow和TensorFlow Serving到底有啥区别呢？"></a>TensorFlow和TensorFlow Serving到底有啥区别呢？</h4><p>TensorFlow项目主要是基于各种机器学习算法构建模型，并为某些特定类型的数据输入做适应学习，而TensorFlow Serving则专注于让这些模型能够加入到产品环境中。开发者使用TensorFlow构建模型，然后TensorFlow Serving基于客户端输入的数据使用前面TensorFlow训练好的模型进行预测。 </p>
<p>个人认为TensorFlow Serving是将tensorflow训练出来的模型更好的应用于生产环境中，通过它的API等支持的方式来方便对外提供稳定可靠的服务。TensorFlow Serving的意义就在于能够很方便的将深度学习生产化，解决了模型无法提供服务的弊端，并且用的是C++语言，性能上应该不错。这样以后深度学习方向的创业公司都能很方便的将产品商业化，保证7*24小时的可靠服务。</p>
<p>典型的pipeline：输入待训练的数据到学习者(Learner)中，输出训练模型。稍后模型验证之后发布到TensorFlow Serving系统。<br><img src="http://img2.ph.126.net/T1ezwNp0SFMKVJgWCZYEsg==/6631737368281209959.png" alt="image"><br>对于生产环境来说，启动模型，随着时间不断迭代模型，新的训练数据出现需要训练优化模型，这些都是常态。现在有了TensorFlow Serving就可以在不停止服务的情况下更新模型和数据，Google内部许多pipelines一直在运行。</p>
<p>客户端和服务端之间的通信采用的是RPC协议实现，其为Google开源的一个高性能RPC框架。</p>
<h4 id="安装TensorFlow"><a href="#安装TensorFlow" class="headerlink" title="安装TensorFlow"></a>安装TensorFlow</h4><p>是时候开始练手了，你需要一遍看书一边在电脑上操作。</p>
<p>TensorFlow提供Python API（也可以用C / C ++），所以你得安装Python 2.7（具体咋安装自行Google）。</p>
<p>一般来讲，使用Python工作时最好用virtualenv虚拟环境。Virtualenv可以在一台机器不同的项目间保持Python依赖隔离。使用virtualenv安装TensorFlow不会覆盖已有的Python版本，这样做也能使排查安装问题变得更容易。</p>
<p>首先安装必备软件：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ubuntu/Linux 64-bit</span></div><div class="line">$ sudo apt-get install python-pip python-dev python-virtualenv </div><div class="line"></div><div class="line"><span class="comment"># Mac OS X </span></div><div class="line">$ sudo easy_install pip</div><div class="line">$ sudo pip install --upgrade virtualenv</div></pre></td></tr></table></figure></p>
<p>建立virtualenv环境. 为了将环境建在 ~/tensorflow 目录下, 执行:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ virtualenv --system-site-packages ~/tensorflow</div></pre></td></tr></table></figure></p>
<p>接下来激活virtualenv:<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ <span class="built_in">source</span> ~/tensorflow/bin/activate <span class="comment">#  with bash </span></div><div class="line">$ <span class="built_in">source</span> ~/tensorflow/bin/activate.csh <span class="comment">#  with csh</span></div><div class="line">(tensorflow)$</div></pre></td></tr></table></figure></p>
<p>Once the virtualenv is activated, you can use pip to install TensorFlow inside it:<br>激活virtualenv后即可使用pip在virtualenv内安装TensorFlow：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Ubuntu/Linux 64-bit, CPU only:</span></div><div class="line">(tensorflow)$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl </div><div class="line"></div><div class="line"><span class="comment"># Mac OS X, CPU only:</span></div><div class="line">(tensorflow)$ sudo easy_install --upgrade six</div><div class="line">(tensorflow)$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl</div></pre></td></tr></table></figure></p>
<p>你可访问官方文档来确认所安装的版本。</p>
<p>如果你要在GPU上跑你的代码，你需要访问官方文档来看看是否满足指定的要求。运行Tensorflow GPU需要安装额外的软件。</p>
<p>当你使用完后可执行如下命令关闭虚拟环境：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ deactivate</div></pre></td></tr></table></figure></p>
<p>其它安装TensorFlow的方法请访问官方网站获取信息。</p>
<h4 id="TensorFlow的-“Hello-World”"><a href="#TensorFlow的-“Hello-World”" class="headerlink" title="TensorFlow的 “Hello World”"></a>TensorFlow的 “Hello World”</h4><p>你可以使用任何文本编辑器编写python代码，然后保存为扩展名“.py”的文件(eg test.py)。用python命令行即可执行test.py。</p>
<p>为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">  </div><div class="line"> a = tf.placeholder(<span class="string">"float"</span>)</div><div class="line"> b = tf.placeholder(<span class="string">"float"</span>)</div><div class="line">  </div><div class="line"> y = tf.mul(a, b)</div><div class="line">  </div><div class="line"> sess = tf.Session()</div><div class="line">  </div><div class="line"> <span class="keyword">print</span> sess.run(y, feed_dict=&#123;a: <span class="number">3</span>, b: <span class="number">3</span>&#125;)</div></pre></td></tr></table></figure>
<p>在上面的代码中，导入Python模块<em>tensorflow</em>。然后定义符号变量，也称为占位符。在后面程序执行中会操作这些变量。我们把这些变量作为参数，TensorFlow的乘法函数<em>tf.mul</em>会调用。数学函数<em>tf.mul</em>会操作tensor，这时的动态大小、多维数组。</p>
<p>TensorFlow的算术操作如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.add，tf.sub，tf.mul，tf.div，tf.mod，tf.abs，tf.neg，tf.sign，tf.inv，tf.square，tf.round，tf.sqrt，tf.pow，tf.exp，tf.log，tf.maximum，tf.minimum，tf.cos，tf.sin</div></pre></td></tr></table></figure></p>
<p>TensorFlow也为程序员提供一些函数来进行数学操作，列表如下：<br>操作    |描述|<br>—|—|<br>tf.diag|    给定对角线上的值，返回对角tensor|<br>tf.transpose|    转置|<br>tf.matmul|    tensor乘法，即矩阵乘法|<br>tf.matrix_determinant    |方阵的行列式|<br>tf.matrix_inverse|    方阵的逆矩阵|</p>
<p>接下来，创建一个会话。事实上，直到这步还没有执行TensorFlow代码。程序通过<em>Session()</em>创建一个会话与Tensorflow库交互；直到调用<em>run()</em>方法才会创建会话，并运行指定的代码。在本例中，<em>run()</em>方法调用变量值和<em>feed_dict</em>参数，表达式运行完成退出会显示结果<em>9</em> 。</p>
<p>本例比较简单，仅仅为了展示TensorFlow完整的过程。然而，我们更感兴趣的是灵活的结构化代码，插入操作来构建计算图。比如，类似于Python编程中的IPython。为了达到这个目的，TensorFlow提供了<em>tf.InteractiveSession()</em>类。</p>
<p>编程模型的理解已经超出了本书的范围，但为了继续接下来的章节，我们仅仅需要知道计算结构包含所有的操作信息和数据。</p>
<p>上面的计算图描述的是数学计算。节点（node）代表数学操作，但是它们也可以代表数据项的点，输出结果或者读写持久化的变量。边（edge）描述的是输入和输出的节点之间的关系。</p>
<p>TensorFlow 将图形定义转换成分布式执行的操作, 以充分利用可用的计算资源(如 CPU 或 GPU). 一般你不需要显式指定使用 CPU 还是 GPU, TensorFlow 能自动检测. 如果检测到 GPU, TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作.</p>
<p>并行计算能让代价大的算法计算加速执行，TensorFlow也在实现上对复杂操作进行了有效的改进。大部分核相关的操作都是设备相关的实现，比如GPU。下面是一些重要的操作/<a href="http://download.tensorflow.org/paper/whitepaper2015.pdf" target="_blank" rel="external">核</a>：</p>
<table>
<thead>
<tr>
<th>操作分组</th>
<th>操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maths</td>
<td>Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal</td>
<td></td>
</tr>
<tr>
<td>Array</td>
<td>Concat, Slice, Split, Constant, Rank, Shape, Shuffle</td>
<td></td>
</tr>
<tr>
<td>Matrix</td>
<td>MatMul, MatrixInverse, MatrixDeterminant</td>
<td></td>
</tr>
<tr>
<td>Neuronal Network</td>
<td>SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool</td>
<td></td>
</tr>
<tr>
<td>Checkpointing</td>
<td>Save, Restore</td>
<td></td>
</tr>
<tr>
<td>Queues and syncronizations</td>
<td>Enqueue, Dequeue, MutexAcquire, MutexRelease</td>
<td></td>
</tr>
<tr>
<td>Flow control</td>
<td>Merge, Switch, Enter, Leave, NextIteration</td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="TensorBoard-可视化学习"><a href="#TensorBoard-可视化学习" class="headerlink" title="TensorBoard:可视化学习"></a>TensorBoard:可视化学习</h4><p>为了更方便TensorFlow 程序的理解，TensorFlow 包括从函数、调试与优化等方面进行了可视化，并发布了一套叫做<em>TensorBoard</em> 的可视化工具。TensorBoard 从不同统计数据来展现图计算过程的详细信息和参数。</p>
<p>TensorBoard模块的数据展示是在TensorFlow执行和汇总数据存储的过程。在TensorFlow的<a href="https://www.tensorflow.org/versions/master/api_docs/python/train.html#summary-operations" target="_blank" rel="external">文档</a>，你可以找到更详细的Python API的解释。</p>
<p>输入下面的指令来启动TensorBoard，包括跟踪（也可以成为序列化）的路径参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(tensorflow)$ tensorboard --logdir=&amp;lt;trace file&amp;gt;</div></pre></td></tr></table></figure>
<p>TensorBoard 开始运行，你可以通过在浏览器中输入<a href="http://localhost:6006" target="_blank" rel="external">http://localhost:6006</a> 来查看。TensorBoard可视化工具的详细使用已超出本书的范围，若想了解更多，可参见<a href="https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html" target="_blank" rel="external">文档</a>。</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/11/netflix-billing-migration-to-aws-part-ii/" itemprop="url">
                  Netflix支付生态系统迁移到AWS的实践（part II）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-11T23:01:25+08:00" content="2016-08-11">
              2016-08-11
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文是Netflix支付生态系统迁移到AWS的第二部分，主要讲解具体的支付应用和数据存储的技术细节。支付系统迁移的概览见第一部分《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483691&amp;idx=1&amp;sn=b25e50ccbe2de3a0b0154bf1a3d39d5f#wechat_redirect" target="_blank" rel="external">Netflix支付生态系统迁移到AWS的实践（part I）</a>》。</p>
<p>随着Netflix全球各地的业务启动，带来了系统数据上的不断增长，进而意识到越早迁移到AWS云服务对Netflix越有利。因为现有对系统将不能继续扩展。</p>
<p>毋庸置疑，迁移如此高度复杂、敏感的应用和数据库并不能干扰线上的业务，是一项艰巨的任务。</p>
<h4 id="Billing的职责和挑战"><a href="#Billing的职责和挑战" class="headerlink" title="Billing的职责和挑战"></a>Billing的职责和挑战</h4><ul>
<li>Billing team保障Netflix公司财务数据。每天生成订阅费，礼品卡，积分，退款等数据，汇报给财务和会计部门。数据处理严格遵循SLAs，并保证处理管道无延迟；</li>
<li>Billing对数据丢失零容忍；</li>
<li>大部分情况下，支付数据是结构化存储成关系模型，并确保数据库操作是事务性的。换句话说，需要保证操作都具有ACID。但是，也有些场景支持跨区域低延迟的访问；</li>
<li>Billing集成DVD业务，但两者的架构不同，增加了集成的难度；</li>
<li>Billing team需要为Netflix客户服务中心提供数据支持，回答Netflix会员的支付问题。同时为客户支持提供数据预览。</li>
</ul>
<p>现有支付系统的架构如下：</p>
<p><img src="http://img1.ph.126.net/SD6yZeeeDWNGD3GQ57-LVQ==/6631627417118515504.jpg" alt="image"></p>
<ul>
<li>数据中心有2个Oracle数据库：一个存储用户订阅信息，另外一个存储是付费数据；</li>
<li>基于REST 的多个应用：从<em>www.netflix.com</em>到客户支持应用的服务调用；</li>
<li>3个批量应用：<ul>
<li>Subscription Renewal</li>
<li>Order &amp; Payment Processor</li>
<li>Revenue Reporting</li>
</ul>
</li>
<li>Billing Proxy应用（AWS云服务）：从Netflix应用路由调用到数据中心；</li>
<li>Weblogic队列：进程间的通信。</li>
</ul>
<p>迁移支付系统的目标是把所有这些入AWS云服务。</p>
<h4 id="迁移步骤"><a href="#迁移步骤" class="headerlink" title="迁移步骤"></a>迁移步骤</h4><p>整个过程分三步走：</p>
<ul>
<li>Act I：新国家地区的支付数据直接上云，并把数据同步回数据中心以供批量作业工作；</li>
<li>Act II：面向用户的数据持久化到Cassandra，保证数据最终一致性，不需要保障操作的ACID；</li>
<li>Act III：最后迁移SQL数据库到AWS云服务。</li>
</ul>
<h4 id="Act-I-–-转发新国家的支付数据到AWS云服务，并同步数据到数据中心（DC）"><a href="#Act-I-–-转发新国家的支付数据到AWS云服务，并同步数据到数据中心（DC）" class="headerlink" title="Act I – 转发新国家的支付数据到AWS云服务，并同步数据到数据中心（DC）"></a>Act I – 转发新国家的支付数据到AWS云服务，并同步数据到数据中心（DC）</h4><p>Netflix新增加6个国家地区，并把部分国家的支付数据放在云上。这意味着面向用户的数据和应用将在云服务中，但同时也需要同步数据回数据中心。这些新增加的国家的用户数据在云服务上服务，同时批处理仍然运行在数据中心。关于Netflix数据中心可以见Part I。</p>
<p>所有的API都是基于云服务用Spring Boot 和Spring 集成开发的应用。Spring Boot提供了框架，使得作者能够快速的开发新应用，并把注意力更多的聚焦在业务逻辑上。Spring 集成一次编写、可重用。新增6个国家的会员数据的API调用可在AWS的任意区域处理，数据存储在Cassandra数据库。这使得即使整个AWS区域宕机，这些国家的支付数据仍可使用，这就是云服务的魅力所在。</p>
<p>Netflix在AWS云服务多个区域的EC2实例上发布应用。他们为云服务代理应用增加一个转发层，把新加的国家的用户支付调用转发到云服务上新的支付API，而老国家地区的支付调用还是继续到数据中心的老支付API。他们从AWS云服务区域之一直接连接到数据中心的Oracle数据库，然后开发一个应用通过SQS来同步其它三个区域的Cassandra数据到这个区域。</p>
<p>Netflix从数据中心迁移Subscription Renewal 应用到AWS云服务，所以他们不能把负载放在数据中心。对于新增的国家区域，他们写爬虫每天去Cassandra爬取会员数据，并追上会员付费数据。行迭代方法在新增的国家使用，但对于其它国家的数据，特别是美国数据量太大，不容易迁移到云服务。不过这事还得继续，只能试试水。</p>
<p>Netflix选择Cassandra作为数据存储是因为它可以写入到AWS任意区域并快速复制写入其它区域。作者设计数据模型如图所示：</p>
<p><img src="http://img2.ph.126.net/ssutzQuxjYEwZRvOo1mk8w==/6631637312723169746.jpg" alt="image"></p>
<p>Act I步骤实施后，支付系统架构如下：</p>
<p><img src="http://img0.ph.126.net/SVhGq6SY7mxbiJRfYtUlBA==/6631715378048744964.jpg" alt="image"></p>
<h4 id="Act-II-–-迁移所有应用和已有国家的数据到AWS云服务"><a href="#Act-II-–-迁移所有应用和已有国家的数据到AWS云服务" class="headerlink" title="Act II – 迁移所有应用和已有国家的数据到AWS云服务"></a>Act II – 迁移所有应用和已有国家的数据到AWS云服务</h4><p>步骤Act I成功完成后，Netflix支付团队开始迁移其它应用到AWS云服务上，此时Oracle数据库未迁移。大部分业务逻辑是批量应用，而且已经成熟的运行了好多年。此次借着迁移的机会把原有代码进行了重构。</p>
<p>Netflix开发<a href="http://techblog.netflix.com/2012/02/aegisthus-bulk-data-pipeline-out-of.html" target="_blank" rel="external">Aegisthus</a>从Cassandra SSTable来拉取数据，并转换成JSON格式的行。Pig脚本按天跑mapreduce来处理海量数据集。Sqoop作业从Cassandra 和Oracle 拉取数据写入Hive。为了验证海量数据的迁移，Netflix开发一个comparator tool来比较验证迁移前后的数据。</p>
<p>各项准备工作做好之后，Netflix支付团队首先拿会员数少的国家“开刀”，迁移的步骤大概如下：</p>
<ul>
<li>迁移时禁用non-GET的API。（这不会影响会员，但会造成支付的更新和订阅的延迟）；</li>
<li>使用Sqoop job从Oracle 获取数据，写入S3 和Hive；</li>
<li>用Pig转换数据写入Cassandra格式；</li>
<li>插入所有的会员记录到Cassandra；</li>
<li>启用non-GET的API。</li>
</ul>
<p>在验证迁移后的数据之后，开始迁移下一个国家，最后迁移的是美国，因为美国的会员量最大。</p>
<p>步骤Act II完成之后，支付系统的架构变为：<br><img src="http://img2.ph.126.net/Zc7n4KTT67ESd5qgofsnHQ==/6631669198560380669.jpg" alt="image"></p>
<h4 id="Act-III-–-和数据中心Say-“Good-bye”！"><a href="#Act-III-–-和数据中心Say-“Good-bye”！" class="headerlink" title="Act III  – 和数据中心Say “Good bye”！"></a>Act III  – 和数据中心Say “Good bye”！</h4><p>最后开始迁移剩下的Oracle数据库。考虑到Oracle高度的关系型，如果迁移到NoSQL的数据，将会比较麻烦。在支付团队忙于前两步时，云数据库工程师迁移Oracle数据库到EC2的MySQL实例。所以在迁移第三步时，MySQL数据库已准备好。接下来，对前期的代码做了部分兼容的优化。</p>
<p>现在的数据库架构包括一个MySQL Master数据库，一个容灾 DB （从MySQL Master复制，如果Master挂掉即可启用为Master），Slave数据库（用来做应用的访问）。</p>
<p>整个支付系统迁移完之后，架构图如下：</p>
<p><img src="http://img2.ph.126.net/98bWcjCGKNQpn6bzFZvrhg==/6631590033723176969.jpg" alt="image"></p>
<h4 id="在路上"><a href="#在路上" class="headerlink" title="在路上"></a>在路上</h4><p>随着支付系统迁移到AWS云服务的完成，Netflix流式架构已全部运行在云端。Netflix可以按需扩展任意Netflix服务，基于用户量来做预测性扩容，使用<a href="http://www.spinnaker.io/" target="_blank" rel="external">Spinnaker</a>单点发布和Netflix各应用的持续发布。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>[1] <a href="http://techblog.netflix.com/2016/07/netflix-billing-migration-to-aws-part-ii.html" target="_blank" rel="external">http://techblog.netflix.com/2016/07/netflix-billing-migration-to-aws-part-ii.html</a></p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://img0.ph.126.net/3vPAbMoh_6fH3-g_I0zo-w==/6631748363397501906.jpg"
               alt="侠天" />
          <p class="site-author-name" itemprop="name">侠天</p>
          <p class="site-description motion-element" itemprop="description">侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">15</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1333564335" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.infoq.com/cn/author/%E4%BE%A0%E5%A4%A9" target="_blank" title="InfoQ">
                  
                    <i class="fa fa-fw fa-infoq"></i>
                  
                  InfoQ
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">侠天</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
