<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="神机喵算" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">
<meta property="og:type" content="website">
<meta property="og:title" content="神机喵算">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="神机喵算">
<meta property="og:description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神机喵算">
<meta name="twitter:description" content="侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/"/>

  <title> 神机喵算 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">神机喵算</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/19/distribute-frame-hsf-not-dubbo/" itemprop="url">
                  分布式服务框架选型：面对Dubbo，阿里巴巴为什么选择了HSF？
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2017-05-19T18:01:29+08:00" content="2017-05-19">
              2017-05-19
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="分布式服务框架选型：面对Dubbo，阿里巴巴为什么选择了HSF？"><a href="#分布式服务框架选型：面对Dubbo，阿里巴巴为什么选择了HSF？" class="headerlink" title="分布式服务框架选型：面对Dubbo，阿里巴巴为什么选择了HSF？"></a>分布式服务框架选型：面对Dubbo，阿里巴巴为什么选择了HSF？</h2><p>阿里巴巴集团内部使用的分布式服务框架 HSF（High Speed Framework，也有人戏称“好舒服”）已经被很多技术爱好者所熟知，目前已经支撑着近 2000 多个应用的运行。</p>
<p>其对应早期的开源项目 Dubbo（因为某些原因，Dubbo 项目在 2012 年年底，阿里巴巴就停止了对此开源项目的更新），则更是在互联网领域有着非常高的知名度和广泛的使用。</p>
<p>本文通过对阿里巴巴 HSF 服务框架的介绍，让大家能对这类分布式服务框架架构设计、运行原理，以及如何实现作为一个 SOA 架构需要满足的各个特性有一个清晰的认识。</p>
<p><strong>HSF 服务框架主要组件</strong></p>
<p><strong>1、服务提供者</strong></p>
<p>在服务框架中真正提供服务功能实现的应用实例，为了保障服务提供的高可用性，一般均是集群部署。</p>
<p>每一个 HSF 的应用均是以 War 包的形式存在，运行在阿里巴巴优化定制后的 Tomcat 容器中，在 Tomcat 容器层已经集成了 HSF 服务框架对服务提供者或服务调用者进行配置服务器发现、服务注册、订阅、失效转移等相关功能，所以不管是在服务提供者还是调用者开发时，只需要进行服务相关的配置操作，应用中无需引入任何 HSF 相关的 Jar 依赖包。</p>
<p>考虑到应用故障的隔离、更方便的服务管控，<strong>目前淘宝内部大部分应用的部署方式还是一个虚拟机（对应一个操作系统）运行一个 Tomcat 容器</strong>，每个 Tomcat 运行一个服务应用，随着近几年以 Docker 容器技术的发展和流行，现在阿里巴巴内部也正在进行应用容器化部署的工作，让服务器的资源利用更加科学和高效。</p>
<p><strong>2、服务调用者</strong></p>
<p>作为服务的消费者，大多数也是以 WAR 应用包的方式运行在 Tomcat 容器中，在阿里巴巴集团内部也有一部分是基于 C/C++、PHP、Node.js 等语言开发的服务调用者。</p>
<p><strong>3、地址服务器</strong></p>
<p>在 HSF 服务框架中，地址服务器肩负着给服务提供者和服务调用者提供部署环境中所有配置服务器和 Diamond 服务器的服务器列表信息，是由 Nginx（ 是一个高性能的 HTTP 和反向代理服务器）提供该服务能力。</p>
<p>在部署 HSF 服务环境时，会将整个环境中的配置服务器集群（服务器 IP 列表）和 Diamond 服务器集群信息设置在地址服务器上，在实际生产部署中，也会部署多台地址服务器提供负载均衡和高可用性的服务，服务提供者和调用者<strong>通过统一域名的方式访问</strong>这些地址服务器（比如“xxx.tbsite.net”），通过 DNS 轮询实现地址服务器访问的高可用性。</p>
<p><strong>4、配置服务器</strong></p>
<p>配置服务器在 HSF 框架中主要负责记录环境内所有服务发布（服务提供者的 IP 地址和服务端口信息）和服务订阅（服务调用者的 IP 地址和服务端口信息）信息，并将服务相关信息推送到服务节点上。为了追求服务发布和订阅的推送效率，所有的服务发布和订阅信息均是保存在内存中。</p>
<p><strong>配置服务器与所有服务者提供者和调用者均是长连接，采用心跳的方式可监控到各服务运行节点的状况</strong>，一旦出现服务提供者服务节点出现故障时，会自动推送更新后（将出问题的服务提供者服务节点信息从列表中删除）的服务提供者列表给相关的服务调用者端。</p>
<p>在生产环境中，会部署多台配置服务器，用于服务发布、订阅、推送的负载均衡，在多台配置服务器间会进行实时的数据同步，保证服务发布和订阅信息尽快能同步到各服务节点上。</p>
<p>某种程度上，配置服务器在 HSF 框架中扮演了服务调用调度的指挥官，通过给服务调用者端推送不同的服务提供者列表就可以轻易的调整服务调用的路由，这一特性在淘宝平台实现单元化（即某一客户在访问淘宝时，访问请求一旦路由到某一个淘宝机房后，在淘宝上进行的所有业务的操作均可以在该机房完成，而无需访问其他机房的服务，也是实现异地多活的基础）、异地多活起到了至关重要的作用。</p>
<p><strong>5、Diamond 服务器</strong></p>
<p>本质上，<strong>Diamond 服务器是一个通用的统一配置管理服务</strong>，类似 ZooKeeper，给应用提供统一的配置设置和推送服务，使用场景非常广泛，在阿里巴巴内部有很多的产品在需要进行配置的保存和获取时都会使用 Diamond 服务器。</p>
<p>在 HSF 服务框架中，则主要承担了服务调用过程中对于服务调用安全管控的规则、服务路由权重、服务 QPS 阀值等配置规则的保存，<strong>所有的信息均是持久化保存到了后端的 MySQL 服务器中</strong>，在生产环境中，会有多台 Diamond 服务器提供负载均衡的服务。</p>
<p>使用 Diamond 服务器进行服务相关设置的典型场景如下：</p>
<ul>
<li>通过设置白名单（服务调用者所在服务节点 IP 地址）的方式设置某些服务或服务中的方法只能让特定 IP 地址的服务器调用；</li>
<li>通过用户认证的方式控制服务是否能够调用；</li>
<li>按照不同的服务器权重设置服务调用者对多个服务提供者服务节点的访问；</li>
<li>设置某些服务的 QPS 能力上限值，一旦该服务的 QPS 达到该阀值，则拒绝服务的继续调用，这也是实现服务限流的技术实现，在平台进行大促或秒杀场景时，保障平台的稳定的重要屏障。</li>
</ul>
<p>通过这样规则的设置，Diamond 除了将这些规则保存在自身的数据库中外，会自动将这些规则推送到相关的服务节点上（实际实现上是服务节点会定时从 Diamond 服务器上同步相关配置信息），使这些规则能立即在服务运行环境中生效。</p>
<p>如图 3-5 所示是阿里巴巴 HSF 服务框架的工作原理，按照服务注册发布、服务订阅、服务规则推送、最终服务提供者和服务调用者间的服务交互的顺序说明了 HSF 服务框架中每个组件在整个框架中所扮演的角色。</p>
<p><img src="http://img2.ph.126.net/tpQ9qJtMy1GcOt3gujpvFA==/6632258536792469066.jpg" alt="img"></p>
<p>图 3-5 HSF 服务框架工作原理示意图</p>
<p><strong>1）服务节点对配置服务器列表的获取。</strong></p>
<p>服务调用者和服务提供者在随着 Tomcat 容器启动后，会以域名（比如“xxx.tbsite.net”）的方式获取到可用的地址服务器，通过向地址服务器分别发送获取配置服务器和 Diamond 服务器服务 IP 列表请求的方式，在容器启动完成后，就已经在该服务节点上获取到了配置服务器和 Diamond 服务器的 IP 列表信息。整个过程如图 3-5 中的步骤①②。</p>
<p><strong>2）服务的注册发布。</strong></p>
<p>作为服务提供者，当获取到配置服务器的服务器列表后，则向配置服务器发送当前应用中包含的服务提供者相关信息（这些信息均是从应用的配置文件中获取到，比如服务的接口类全名、服务版本、所属服务组等信息），联同当前服务器的 IP 地址、服务端口等信息进行服务注册发布，如图 3-5 中的步骤③。这个步骤在每一个有服务提供的应用启动时都会自动执行，比如现在有 5 个提供同一服务的应用启动后，此时在配置服务器上就已经保存了提供这一服务的 5 个服务器相关信息。</p>
<p><strong>3）服务的订阅。</strong></p>
<p>当作为服务调用者的应用启动时，同样在完成配置服务器列表的获取后，就进行与配置服务器的交互，发送服务消费者相关信息（同样包含了服务的接口全名，服务版本、所属服务组）到配置服务器进行服务的订阅，此时在配置服务器上会通过服务接口全名+服务版本作为匹配条件在当前配置服务器的内存中进行搜索，一旦获取到对应的服务注册信息，则将对应的服务提供者的服务器组 IP 地址及端口返回给服务调用者所在的应用节点上，此时也就完成了服务调用者端对于它所需要调用的服务提供者服务器列表信息，用于在服务真正交互时使用。服务订阅过程如图 3-5 中的步骤④⑤。</p>
<p><strong>4）服务规则的推送（如果需要）。</strong></p>
<p>如果没有上文提到对于服务安全管控、流量控制等需求的时候，对于 Diamond 服务器的使用并不是必需的，在有这样的需求场景时，可通过 Diamond 提供的规则设置界面，可以对指定服务的服务提供者和调用者设置相关的规则，一旦保存规则后，则此规则配置将会在 5 秒内推送到与所设置服务相关的服务节点上。如图 3-5 中的步骤⑥。</p>
<p><strong>5）服务交互。</strong></p>
<p>在应用进行业务请求处理过程中，出现了服务调用者对服务提供者的调用时，服务调用者会从已经保存在该应用节点上的服务提供者服务器列表中选择（阿里巴巴内部使用随机模式）其中一台进行服务请求的发送，服务交互期间完全是服务调用者和服务提供者间两台服务器间的，无需通过中间服务器的中转，这就是相比于“中心化” ESB 模式，所有服务交互都需要“中心” ESB 进行服务路由，而当前这种架构称为“去中心化”的主要原因。如图 3-5 中的步骤⑦。</p>
<p>阿里巴巴的分布式服务框架核心是以服务化的方式构建整个应用体系的同时，要保证在高并发的情况下，服务具备高效交互、高可用性和扩展能力。接下来对于 HSF 框架如何给服务提供以上能力具体加以说明。</p>
<p><strong>1、HSF 框架采用 Netty + Hession 数据序列化协议实现服务交互</strong></p>
<p>HSF 框架中采用如今流行的网络通信框架 Netty 加上 Hession 数据序列化协议实现 HSF 服务间的交互，主要考虑点是在大并发量时，服务交互性能达到最佳。这类 RPC 协议采用多路复用的 TCP 长连接方式，在服务提供者和调用者间有多个服务请求同时调用时会共用同一个长连接，即一个连接交替传输不同请求的字节块。它既避免了反复建立连接开销，也避免了连接的等待闲置从而减少了系统连接总数，同时还避免了 TCP 顺序传输中的线头阻塞（head-of-line blocking）问题。</p>
<p>Hessian 是 HSF 框架中默认使用的数据序列化协议，在数据量较小时性能表现出众，Hessian 的优点是精简高效，同时可以跨语言使用，目前支持 Java, C++,  .net, Python, ruby 等语言。另外 Hessian 可以充分利用 Web 容器的成熟功能，在处理大量用户访问时很有优势，在资源分配、线程排队、异常处理等方面都可以由 Web 容器保证。</p>
<p>HSF 框架同时也支持切换使用 Java 序列化，Hession 相比 JDK 标准的序列化方式（即基于 Serializable 接口的标准序列化），在典型场景中，其序列化时间开销可能缩短 20 倍。虽然 Hessian 不是最快的序列化协议，但它对于复杂业务对象的序列化正确率、准确性相较于最稳定的 Java 序列化并不逊色太多。</p>
<p>业界还有一些比 Hessian 更快的序列化协议，但它们相对于 Hessian 在复杂场景下的处理能力还是会差一些，<strong>所以 Hessian 是在性能和稳定性同时考虑下最优的序列化协议。</strong></p>
<p>阿里巴巴当时在对多种通信协议和数据序列化组件等测试中，Netty + Hession 的组合在互联网高并发量的场景下,特别是在 TPS 上达到 10w 以上时，性能和效率远比 REST 或者 Web Service 高。</p>
<p><strong>2、HSF 框架的容错机制</strong></p>
<p>因为要保证服务的高可用性，所以在生产环境部署中一定会有<strong>多个应用实例作为服务提供者提供某一相同服务。</strong></p>
<p>基于之前所提到的服务框架的运行原理的说明，在进行服务调用时，服务调用者端已经保存了它所需要调用的服务提供者的服务器列表信息（如图 3-6 中为例，则保存了三台服务提供者所在服务器的列表）。</p>
<p>当采用随机方式获取其中一台进行服务交互时（如图 3-6 步骤①），不管是第一台服务器已经某种故障造成了服务请求无法响应，还是该服务器已经接收了服务请求，在进行服务请求处理过程中出现了服务器故障（比如宕机、网络问题），造成该服务器没有在规定的时间（一般服务调用会设置到期时间）返回服务处理的结果，服务调用者端则会获取到服务调用失败的反馈（如图 3-6 步骤②）。</p>
<p>在 HSF 服务调用的代码中会立即从剩下的服务提供者服务器列表中选择另外一个服务器再次进行服务请求（如图 3-6 步骤③），这一次这个服务提供者实例正常提供了此次服务的请求（如图 3-6 步骤④），从而保证了在个别服务提供者出现故障时，完全不会影响该服务正常提供服务。</p>
<p><img src="http://img1.ph.126.net/vuGysK3QsRHQJKUTHPrRGw==/6632018843260281046.jpg" alt="img"></p>
<p>图 3-6 HSF 服务框架实现服务高可用性原理示意图</p>
<p>同时，<strong>因为配置服务器是采用长连接的方式与服务节点进行网络通讯，一旦发现有服务提供者实例出现故障，配置服务器在秒级就会感知到</strong>（如图 3-6 步骤⑤），此时会将出问题这台服务提供者的信息从该服务的服务器列表中删除，并将更新后的服务器列表采用推送的方式同步给与该服务相关的所有服务调用者端（如图 3-6 步骤⑥），这样当下次服务调用者再进行此服务的调用时，就不会因为随机的方式再次对已经停止服务提供的服务器发起服务的调用。</p>
<p><strong>3、HSF 框架的线性扩展支持</strong></p>
<p>作为 HSF 框架设计之初，<strong>最为重要的一个特性就是服务能力的可扩展性。</strong>也就是真正的做到某个服务的业务处理能力能随着服务器资源的增加得到线性的增长。</p>
<p>其实在传统架构中一直也会强调平台的扩展能力，但均会程度不一的出现服务节点数量到达一定量后，出现阻碍平台服务能力扩展的问题，有的是出现网络传输的瓶颈、也有服务节点接入数量上的限制，前文所描述的 ESB 架构带来的“雪崩”效应也均是这类架构给服务能力的扩展带来影响的原因所在。</p>
<p>如图 3-7 中所描述的场景，当服务面对较大的服务调用压力或将要面临如天猫双11大促、秒杀等活动前，已有的服务提供者各服务器水位（CPU、内存、IO等）处于比较高的情况或现有服务能力满足不了业务访问量的要求时，则需要通过增加服务节点数量的方式提升该服务的服务处理能力。</p>
<p><img src="http://img0.ph.126.net/aXYvmt58vBlDyra69PSyfA==/6632181570981194421.jpg" alt="img"></p>
<p>图 3-7 HSF 服务框架对服务能力线性扩展支持1</p>
<p>此时，只需要通过增加该服务的服务提供者实例（如图 3-8 所示，增加了一个服务），基于 HSF 框架的运行机制，<strong>新增加的服务提供者实例一旦应用启动完成后，可在几秒内开始进行服务请求的处理</strong>（主要完成服务注册发布、更新后服务列表推送到服务调用者端），从而达到分担其他服务器实例压力的作用，实现服务能力整体水位恢复到正常的状态（如图 3-9）。</p>
<p><img src="http://img0.ph.126.net/g5M-yIgyLaGutr7V-khlZA==/6632145287097477015.jpg" alt="img"></p>
<p>图 3-8 HSF服务框架对服务能力线性扩展支持2</p>
<p><img src="http://img0.ph.126.net/EJIZHJ1KmkqfSwrZ3NwA2g==/6632081515423068015.jpg" alt="img"></p>
<p>图 3-9 HSF 服务框架对服务能力线性扩展支持3</p>
<p>正是基于 HSF 框架这一特性，从而真正实现了只要增加服务实例就能实现该服务能力扩展的目标，目前在阿里巴巴共享服务事业部中多个服务中心在天猫双 11 那天各自所部署的服务实例节点数量均超过 2000，即同一个服务由超过 2000 个服务实例同时提供负载均衡的服务。</p>
<p>本文由公众号《高可用架构》原创。</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/05/turning-back-time-savepoints/" itemprop="url">
                  Savepoint：Flink让时光倒流
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-05T09:19:04+08:00" content="2016-12-05">
              2016-12-05
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>现在互联网产品对数据的实时性要求极其强烈，比如，某电商产品的推荐系统，当一个用户点击页面就会在秒级内给出相应的推荐页面。进而，实时流处理技术讨论变得越加频繁《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483673&amp;idx=1&amp;sn=d70adf019c8cf797a41da6186e93f0fb&amp;scene=21#wechat_redirect" target="_blank" rel="external">各大主流流处理框架大比拼</a>》和《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483679&amp;idx=1&amp;sn=5e544ae789c8773f73b9e1d552e5f991&amp;scene=21#wechat_redirect" target="_blank" rel="external">实时流处理框架选型：就应该这样“拉出来遛遛”</a>》，比如，延迟性、吞吐量、watermark…</p>
<p>接下来，进入主题：Flink实时流处理中的“reprocess data”。</p>
<p>相信很多同行经常遇到以下几种case：</p>
<ul>
<li>开发新feature或者bug修复，程序新版本上线；</li>
<li>不同版本产品的A/B test；</li>
<li>评估和实现在新处理框架下的应用迁移，或者迁移到不同的集群</li>
</ul>
<p>以上所有情况都可以使用Flink的savepoint功能实现。</p>
<h6 id="Savepoint是什么"><a href="#Savepoint是什么" class="headerlink" title="Savepoint是什么"></a>Savepoint是什么</h6><p>简而言之，Flink的savepoint是一个全局的、一致性的快照（snapshot）。其包含两方面：</p>
<ul>
<li>数据源所有数据的位置；</li>
<li>并行操作的状态</li>
</ul>
<p>“全局一致”是指所有的输入源数据在指定的位置，所有的并行操作的状态都被完全checkpoint了。注意理解这句话，可以多读几遍回味一下。</p>
<p>如果你的应用在过去某个时间点做了savepoint，那你随时可以从前面的savepoint更新发布应用。这时，新的应用会从savepoint中的操作状态进行初始化，并从savepoint的数据源位置开始重新处理所有数据。</p>
<p>Flink的savepoint是完全不依赖的，所以你每个应用可以有N个savepoint，你可以回退到多个位置重新开始你的应用（可以是不同的应用，如下图所示）。这个功能在流处理应用是相当强大的。</p>
<p><img src="http://img0.ph.126.net/IJHz4DXlo7fLjCsEfhqsyg==/6631537257167466063.png" alt="image"></p>
<p>有读者会觉得上图似曾相识，其实你可能想到了Flink的checkpoint，这时是不是有点糊涂了，那savepoint和checkpoint到底啥关系呢？详细答案会在后续某篇文章揭晓，这里先简单说下：checkpoint是Flink实现容错的，savepoint仅仅只是checkpoint的一个扩展。如果checkpoint开启，那Flink会周期性的创建所有操作状态的checkpoint。savepoint和checkpoint最大的不同是，checkpoint会按时间间隔自动创建，而savepoint需要手动触发。</p>
<p>为了让 “reprocess data”得到更精确的结果，那我们不得不提event-time和processing-time或者ingestion-time的区别，这也是在各个流处理技术里常提到的时间语义。不过这里先不展开，后续也会有文章专门讲到。为了让 “reprocess data”得到更精确的结果需要使用event-time，因为依赖processing-time或者ingestion-time的应用会根据当前的wall-clock时间来处理。</p>
<h6 id="如何实现savepoint"><a href="#如何实现savepoint" class="headerlink" title="如何实现savepoint"></a>如何实现savepoint</h6><p>实际上，使用savepoint的前提有以下几点：</p>
<ul>
<li>开始checkpoint；</li>
<li>可重复使用的数据源（e.g., Apache Kafka，Amazon Kinesis，或者文件系统）；</li>
<li>所有保存的状态需继承Flink的管理状态接口；</li>
<li>合适的state backend配置</li>
</ul>
<p>做到了这几点，那你可以通过CLI命令行实现savepoint并重新从savepoint开始应用：</p>
<ol>
<li>创建savepoint</li>
</ol>
<p>首先，获取Flink所有正在运行的job list：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">user$ flink list</div><div class="line">------------Running/Restarting Jobs------------</div><div class="line">12.04.2016 16:20:33 : job_id : 12345678 (RUNNING)</div></pre></td></tr></table></figure>
<p>接着，使用刚才获取到的job ID创建savepoint：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">user$ flink savepoint job_id</div></pre></td></tr></table></figure>
<p>这时你可以选择取消正在运行的job（可选操作）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">user$ flink cancel job_id</div></pre></td></tr></table></figure>
<ol>
<li>从savepoint开启job</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">user$ flink run -d -s hdfs://savepoints/1 directory/your-updated-application.jar</div></pre></td></tr></table></figure>
<h6 id="如果更新应用，该咋办？"><a href="#如果更新应用，该咋办？" class="headerlink" title="如果更新应用，该咋办？"></a>如果更新应用，该咋办？</h6><p>修改的应用从一个savepoint开始需要考虑以下两种情况：</p>
<ul>
<li>用户自定义逻辑的改变，比如，MapFunction；</li>
<li>应用的拓扑的改变，比如，增加或者移除操作</li>
</ul>
<p>如果你的情况属于上面描述的第一类，那不需要做其他额外处理。但是，第二种情况，Flink要求修改前后的操作要能匹配上，这样才好使用保存的操作状态。这时你需要手动在原始和更新的应用中分配操作ID，因为没有操作ID是不可能改变应用的拓扑，所以最好要尽可能的分配操作ID，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">DataStream stream = env.</div><div class="line"> // Stateful source (e.g. Kafka) with ID</div><div class="line"> .addSource(new StatefulSource())</div><div class="line"> .uid(“source-id”)</div><div class="line"> .shuffle()</div><div class="line"> // The stateful mapper with ID</div><div class="line"> .map(new StatefulMapper())</div><div class="line"> .uid(“mapper-id”)</div><div class="line"></div><div class="line">// Stateless sink (no specific ID required)</div><div class="line">stream.print()</div></pre></td></tr></table></figure>
<h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>Savepoint是Flink与其它流处理技术的独特之处，要好好的利用起来。</p>
<p>不过Flink的savepoint使用也有诸多限制，后续有机会再讲到，但相对于Spark Streaming的checkpoint来说还是高级了不少。</p>
<p><em>PS：虽然Spark项目的star数比Flink多一个数量级，但Flink在某些feature上的开发和布局比Spark更快，感觉Flink开发者在最近代表着实时流处理和离线大数据技术的方向，看好Flink</em>。</p>
<p>参考：<br>[1] <a href="http://data-artisans.com/turning-back-time-savepoints" target="_blank" rel="external">http://data-artisans.com/turning-back-time-savepoints</a></p>
<p>Enjoy!</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/12/05/kafka-cluster-optimize/" itemprop="url">
                  Kafka Cluster优化两三事
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-12-05T09:18:52+08:00" content="2016-12-05">
              2016-12-05
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>写在之前</em>：本文将讲述Kafka Cluster配置和优化。</p>
<p>Kafka Cluster（相对于单个server）最大的优点：可扩展性和容错性。</p>
<p><img src="http://img0.ph.126.net/InezSQTEYGXiSj-q7IAZ8Q==/6631959469629171084.png" alt="image"></p>
<p>​                               Kafka集群简图</p>
<h6 id="Kafka-Broker个数"><a href="#Kafka-Broker个数" class="headerlink" title="Kafka Broker个数"></a>Kafka Broker个数</h6><p>决定Kafka集群大小的因素有以下几点：</p>
<ul>
<li>磁盘容量：首先考虑的是所需保存的消息所占用的总磁盘容量和每个broker所能提供的磁盘空间。如果Kafka集群需要保留 10 TB数据，单个broker能存储 2 TB，那么我们需要的最小Kafka集群大小 5 个broker。此外，如果启用副本参数，则对应的存储空间需至少增加一倍（取决于副本参数）。这意味着对应的Kafka集群至少需要 10 个broker。</li>
<li>请求量：另外一个要考虑的是Kafka集群处理请求的能力。这主要取决于对Kafka client请求的网络处理能力，特别是，有多个consumer或者网路流量不稳定。如果，高峰时刻，单个broker的网络流量达到80%，这时是撑不住两个consumer的，除非有两个broker。再者，如果启用了副本参数，则需要考虑副本这个额外的consumer。也可以扩展多个broker来减少磁盘的吞吐量和系统内存。</li>
</ul>
<h6 id="Kafka-Broker配置"><a href="#Kafka-Broker配置" class="headerlink" title="Kafka Broker配置"></a>Kafka Broker配置</h6><p>同一个Kafka集群的所有broker机器必须满足以下两个参数：</p>
<ul>
<li>所有broker机器需配置相同的zookeeper连接参数（.connect）。这决定了Kafka集群存储的元数据位置；</li>
<li>所有broker机器需配置唯一的broker id（ .id）。如果一个集群下的两个broker配置了相同的broker id，则第二个broker启动时会失败并报错。</li>
</ul>
<h6 id="操作系统优化"><a href="#操作系统优化" class="headerlink" title="操作系统优化"></a>操作系统优化</h6><p>大部分Linux发布版本默认的内核参数配置能让大部分应用工作的相当好。但对于实际的Kafka broker场景来说，做稍些改变会提升broker性能。主要涉及的配置：虚拟内存、网络和磁盘挂载（用来存储log segment），一般在<em>/etc/sysctl.conf</em> (CentOS系统)。</p>
<ul>
<li>Virtual Memory</li>
</ul>
<p>一般来说，Linux的虚拟内存会根据系统负载自动调整。内存页（page）swap到磁盘会显著的影响Kafka的性能，并且Kafka重度使用page cache，如果VM系统swap到磁盘，那说明没有足够的内存来分配page cache。</p>
<p>避免swap的一种方式是设置swap空间为0。但是，swap会在系统崩溃时提供安全机制，或者会在out of memory的情况下阻止操作系统 kill 掉进程。由于这个原因，推荐 <em>vm.swappiness</em>参数设置为一个非常低的值：1 。这个参数表示 VM系统中的多少百分比用来作为swap空间。</p>
<p>另外一种方式是通过内核调节“脏页”（注：“脏页”会被刷到磁盘上）。Kafka依赖磁盘I/O性能来提高producer的响应时间。这也是为什么通常优先把log segment功能放在可以快速响应的磁盘中（比如，SSD）。这样使得flush进程把“脏数据”写入磁盘前，“脏页”数目就减少了，可以设置<em>vm.dirty_background_ratio</em>（表示占用系统内存的百分比）参数的值为 10 以下。大部分应用场景下，<em>vm.dirty_background_ratio</em>设置为 5 就够用了，要注意了：这个参数值不能设置为 0 ，因为设置为 0 后会引起内核持续刷“脏页”，使得内核的buffer write功能没法施展。</p>
<p>“脏页”的总量可以通过<em>vm.dirty_ratio</em> 来改变，默认值是 20 （此处也是百分比），这个值的设置范围较大，一般建议设置 60 到 80 为合理的值。但是<em>vm.dirty_ratio</em> 参数也引来了不小的风险，会造成大量unflush的数据在硬刷到磁盘时产生较长的I/O停顿。如果<em>vm.dirty_ratio</em> 值设置的较大时，强烈建议Kafka开启备份功能，以备系统崩溃。</p>
<p>在设置了这些参数后，需要监控Kafka集群运行时“脏页”的数量，当前“脏页”数量可由如下方式查看（/proc/vmstat文件）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># cat /proc/vmstat | egrep &quot;dirty|writeback&quot; nr_dirty 3875</div><div class="line">nr_writeback 29</div><div class="line">nr_writeback_temp 0</div></pre></td></tr></table></figure>
<ul>
<li>磁盘</li>
</ul>
<p>除了考虑磁盘硬件本身和RAID配置外，磁盘的filesystem对Kafka集群的影响最大。虽然有许多filesystem，但最常用的是EXT4或者XFS。在这里XFS文件系统比EXT4稍好，具体原因Google下。</p>
<p>另外一点是，建议开启mount的<em>noatime</em> mount选项。文件系统在文件被访问、创建、修改等的时候会记录文件的一些时间戳，比如：文件创建时间（ctime）、最近一次修改时间（mtime）和最近一次访问时间（atime）。默认情况下，atime的更新会有一次读操作，这会产生大量的磁盘读写，然而atime对Kafka完全没用。</p>
<ul>
<li>网络</li>
</ul>
<p>Linux发布版本的网络参数对高网络流量不适用。对于Kafka集群，推荐更改每个socket发送和接收buffer的最大内存：<em>net.core.wmem_default</em> 和 <em>net.core.rmem_default</em> 为128 kb，<em>net.core.wmem_max</em> 和<em>net.core.rmem_max</em> 为 2 Mb。另外一个socket参数是TCP socket的发送和接收buffer： <em>net.ipv4.tcp_wmem</em> 和 <em>net.ipv4.tcp_rmem</em>。</p>
<h6 id="Kafka集群稳定"><a href="#Kafka集群稳定" class="headerlink" title="Kafka集群稳定"></a>Kafka集群稳定</h6><p>主要涉及到GC、数据中心布局和ZK使用：</p>
<ul>
<li>GC调优</li>
</ul>
<p>调GC是门手艺活，幸亏Java 7引进了G1 垃圾回收，使得GC调优变的没那么难。G1主要有两个配置选项来调优：<em>MaxGCPauseMillis</em>和<em>InitiatingHeapOccupancyPercent</em>，具体参数设置可以参考Google，这里不赘述。</p>
<p>Kafka broker能够有效的利用堆内存和对象回收，所以这些值可以调小点。对于 64Gb内存，Kafka运行堆内存5Gb，<em>MaxGCPauseMillis</em>和<em>InitiatingHeapOccupancyPercent</em> 分别设置为 20毫秒和 35。</p>
<p>Kafka的启动脚本使用的不是 G1回收，需要在环境变量中加入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># export JAVA_HOME=/usr/java/jdk1.8.0_51</div><div class="line"># export KAFKA_JVM_PERFORMANCE_OPTS=&quot;-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true&quot;</div><div class="line"># /usr/local/kafka/bin/kafka-server-start.sh -daemon /usr/local/kafka/config/server.properties</div></pre></td></tr></table></figure>
<ul>
<li>数据中心布局</li>
</ul>
<p>原则上Kafka broker不建议都在一个机架上，为了容灾，但现实情况大部分公司做不到，此处略去。</p>
<ul>
<li>Zookeeper</li>
</ul>
<p>Kafka集群利用ZK来存储broker、topic和partition的元数据信息。</p>
<p>在Kafka 0.9.0之前，consumer利用ZK来直接存储consumer group的信息，包括topic的消费情况、每个partition消费的周期性commit。在0.9.0版本，提供新的consumer接口利用Kafka broker来管理。</p>
<p>Consumer可以选择使用Zk或者Kafka来提交 offset和 提交间隔。如果consumer使用ZK管理offset，那每个consumer在每个partition的每个时间间隔写入ZK。合理的offset提交间隔是1分钟，但如果一个Kafka集群有大量的consumer消费时，这个ZK流量将是巨大的。所以如果ZK不能处理大流量，那只能把offset提交间隔设大，但同时也带来丢数据的风险。最保险的建议是使用Kafka来提交offset。</p>
<p>另外，建议Kafka集群不要和其他应用使用同一组ZK，因为Kafka对于ZK的延迟和超时是相当敏感的，ZK的不通将会导致Kafka的不可预测性。</p>
<h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>Kafka在各大互联公司应用广泛，希望上述Kafka集群调优对各位有帮助。</p>
<p>PS：最近在负责招聘，有Hadoop、Spark、Flink、Kafka、Storm等相关经验的优秀人才，请联系我或者后台留言。</p>
<p>Enjoy!</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/10/24/visual-tools-for-machine-learning-part-3/" itemprop="url">
                  机器学习可视化系统完结篇：模型评估和参数调优
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-10-24T01:30:30+08:00" content="2016-10-24">
              2016-10-24
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>机器学习可视化系统完结篇：模型评估和参数调优</p>
<p><em>写在之前：前两篇讲述了特征分析：《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483684&amp;idx=1&amp;sn=428cf35632b2408e1dc7d36dff497c53&amp;scene=0#wechat_redirect" target="_blank" rel="external">可视化图表让机器学习“biu”的一样简单：特征分析</a>》和模型选择：《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483750&amp;idx=1&amp;sn=34d9979d6e2608535d9921db54d57a8d&amp;chksm=e91f19acde6890baa7e6950b7133bcf290549c8b2ef150e5422a3c2f0df233e51cbde2b5baf3&amp;scene=0#wechat_redirect" target="_blank" rel="external">机器学习模型选择如此简单</a>》。</em></p>
<p>本篇文章详细阐述机器学习模型评估和参数调优。将主要围绕两个问题来阐述：</p>
<ol>
<li>“知其所以然”：当你选择的一个机器学习模型运行时，你要知道它是如何工作的；</li>
<li>“青出于蓝”：更进一步，你得知道如何让此机器学习模型工作的更优。</li>
</ol>
<h4 id="模型评估的方法"><a href="#模型评估的方法" class="headerlink" title="模型评估的方法"></a>模型评估的方法</h4><p>一般情况来说，F1评分或者R平方(R-Squared value)等数值评分可以告诉我们训练的机器学习模型的好坏。也有其它许多度量方式来评估拟合模型。</p>
<p>你应该猜出来，我将提出使用可视化的方法结合数值评分来更直观的评判机器学习模型。接下来的几个部分将分享一些有用的工具。</p>
<p>首先想声明的，单单一个评分或者一条线，是无法完全评估一个机器学习模型。偏离真实场景来评估机器学习模型（’good’ or ‘bad’）都是“耍流氓”。某个机器学习模型若可“驾驭”小样本数据集生成最多预测模型（即，命中更多预测数据集）。如果一个拟合模型比其它拟合过的模型形式或者你昨天的预测模型能够得到更好的结果，那即是好（’good’）。</p>
<p>下面是一些标准指标： <em>confusion_matrix</em>，<em>mean_squared_error</em>， <em>r2_score</em>，这些可以用来评判分类器或者回归的好坏。表格中给出的是<strong><em>Scikit-Learn</em></strong>中的函数以及描述：</p>
<p>评估分类模型：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>Scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>精准度</td>
<td>from sklearn.metrics import precision_score</td>
</tr>
<tr>
<td>Recall</td>
<td>召回率</td>
<td>from sklearn.metrics import recall_score</td>
</tr>
<tr>
<td>F1</td>
<td>F1值</td>
<td>from sklearn.metrics import f1_score</td>
</tr>
<tr>
<td>Confusion Matrix</td>
<td>混淆矩阵</td>
<td>from sklearn.metrics import confusion_matrix</td>
</tr>
<tr>
<td>ROC</td>
<td>ROC曲线</td>
<td>from sklearn.metrics import roc</td>
</tr>
<tr>
<td>AUC</td>
<td>ROC曲线下的面积</td>
<td>from sklearn.metrics import auc</td>
</tr>
</tbody>
</table>
<p>评估回归模型：</p>
<table>
<thead>
<tr>
<th>指标</th>
<th>描述</th>
<th>Scikit-learn函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean Square Error (MSE, RMSE)</td>
<td>平均方差</td>
<td>from sklearn.metrics import mean_squared_error</td>
</tr>
<tr>
<td>Absolute Error (MAE, RAE)</td>
<td>绝对误差</td>
<td>from sklearn.metrics import mean_absolute_error, median_absolute_error</td>
</tr>
<tr>
<td>R-Squared</td>
<td>R平方值</td>
<td>from sklearn.metrics import r2_score</td>
</tr>
</tbody>
</table>
<p>下面开始使用<strong><em>Scikit-Learn</em></strong>的可视化工具来更直观的展现模型的好坏。</p>
<h6 id="评估分类模型"><a href="#评估分类模型" class="headerlink" title="评估分类模型"></a>评估分类模型</h6><p>我们评估分类器是判断预测值时否很好的与实际标记值相匹配。正确的鉴别出正样本（True Positives）或者负样本（True Negatives）都是True。同理，错误的判断正样本（False Positive，即一类错误）或者负样本（False Negative，即二类错误）。</p>
<p>注意：True和False是对于评价预测结果而言，也就是评价预测结果是正确的(True)还是错误的(False)。而Positive和Negative则是样本分类的标记。</p>
<p>通常，我们希望通过一些参数来告知模型评估如何。为此，我们使用混淆矩阵。</p>
<h5 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h5><p><img src="https://silvrback.s3.amazonaws.com/uploads/4ab81a17-4a77-4e9e-b092-de5fac2afa07/confusionmatrix_large.png" alt="Confusion Matrix"></p>
<p>幸运的是，<strong><em>Scikit-Learn</em></strong>提供内建函数（<em>sklearn.metrics.confusion_matrix</em>）来计算混淆矩阵。输入数据集实际值和模型预测值作为参数，输出即为混淆矩阵，结果类似这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[[<span class="number">1238</span>   <span class="number">19</span>]   <span class="comment"># True Positives = 1238, False Negatives = 19</span></div><div class="line"> [   <span class="number">2</span>  <span class="number">370</span>]]  <span class="comment"># False Positives = 2, True Negatives = 370</span></div></pre></td></tr></table></figure>
<h5 id="分类报告"><a href="#分类报告" class="headerlink" title="分类报告"></a>分类报告</h5><p>分类报告除了包括混淆矩阵，也增加了其它优势，比如，混淆矩阵会标示样例是否被正确鉴别，同时也提供precision，recall和 F1 值三种评估指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</div><div class="line"></div><div class="line">print(classification_report(y_true, y_pred, target_names=target_names))</div></pre></td></tr></table></figure>
<p> 更进一步，可以对<strong><em>Scikit-Learn</em></strong>的内建函数做些加强，比如，使用带颜色区分的热力图，它将帮助我们的眼睛更容易的辨别预测成功（橘黄色）和失败（灰色）。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> colors</div><div class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</div><div class="line"></div><div class="line">ddl_heat = [<span class="string">'#DBDBDB'</span>,<span class="string">'#DCD5CC'</span>,<span class="string">'#DCCEBE'</span>,<span class="string">'#DDC8AF'</span>,<span class="string">'#DEC2A0'</span>,<span class="string">'#DEBB91'</span>,\</div><div class="line">            <span class="string">'#DFB583'</span>,<span class="string">'#DFAE74'</span>,<span class="string">'#E0A865'</span>,<span class="string">'#E1A256'</span>,<span class="string">'#E19B48'</span>,<span class="string">'#E29539'</span>]</div><div class="line">ddlheatmap = colors.ListedColormap(ddl_heat)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_classification_report</span><span class="params">(cr, title=None, cmap=ddlheatmap)</span>:</span></div><div class="line">    title = title <span class="keyword">or</span> <span class="string">'Classification report'</span></div><div class="line">    lines = cr.split(<span class="string">'\n'</span>)</div><div class="line">    classes = []</div><div class="line">    matrix = []</div><div class="line"></div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines[<span class="number">2</span>:(len(lines)<span class="number">-3</span>)]:</div><div class="line">        s = line.split()</div><div class="line">        classes.append(s[<span class="number">0</span>])</div><div class="line">        value = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> s[<span class="number">1</span>: len(s) - <span class="number">1</span>]]</div><div class="line">        matrix.append(value)</div><div class="line"></div><div class="line">    fig, ax = plt.subplots(<span class="number">1</span>)</div><div class="line"></div><div class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> range(len(matrix)+<span class="number">1</span>):</div><div class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(len(classes)):</div><div class="line">            txt = matrix[row][column]</div><div class="line">            ax.text(column,row,matrix[row][column],va=<span class="string">'center'</span>,ha=<span class="string">'center'</span>)</div><div class="line"></div><div class="line">    fig = plt.imshow(matrix, interpolation=<span class="string">'nearest'</span>, cmap=cmap)</div><div class="line">    plt.title(title)</div><div class="line">    plt.colorbar()</div><div class="line">    x_tick_marks = np.arange(len(classes)+<span class="number">1</span>)</div><div class="line">    y_tick_marks = np.arange(len(classes))</div><div class="line">    plt.xticks(x_tick_marks, [<span class="string">'precision'</span>, <span class="string">'recall'</span>, <span class="string">'f1-score'</span>], rotation=<span class="number">45</span>)</div><div class="line">    plt.yticks(y_tick_marks, classes)</div><div class="line">    plt.ylabel(<span class="string">'Classes'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Measures'</span>)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">cr = classification_report(y_true, y_pred)</div><div class="line">plot_classification_report(cr)</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/8a76cb0d-6ead-494f-b000-d3bd3ec58aad/classificationreport_large.png" alt="Classification Report"></p>
<p>看起来挺容易，对不？发现分类热力图的另外一个好处，它可以让我们看出一类错误 VS 二类错误。但有一个缺陷，它并不能垮模型进行比较，而这对评估拟合模型是相当重要的。因为这个原因，接下来将使用第二篇文章中的<em>classify</em>和<em>regress</em>代码。</p>
<p>下面的<em>get_preds</em>函数将输出一个实际标记值和预测值的二元组，这个二元组将会使得后续的跨模型的可视化比较变得容易：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_preds</span><span class="params">(attributes, targets, model)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line">    Executes classification or regression using the specified model</div><div class="line">    and returns expected and predicted values.</div><div class="line">    Useful for comparison plotting!</div><div class="line">    '''</div><div class="line">    splits = cv.train_test_split(attributes, targets, test_size=<span class="number">0.2</span>)</div><div class="line">    X_train, X_test, y_train, y_test = splits</div><div class="line"></div><div class="line">    model.fit(X_train, y_train)</div><div class="line">    y_true = y_test</div><div class="line">    y_pred = model.predict(X_test)</div><div class="line">    <span class="keyword">return</span> (y_true,y_pred)</div></pre></td></tr></table></figure>
<h5 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h5><p>另一种评估分类模型的方法是ROC（Receiver Operating Characteristic）曲线。我们能从<strong><em>Scikit-Learn</em></strong> 指标模块中import <em>roc_curve</em>，计算 true positive率和false positive 率的数值。我们也可以画出ROC曲线来权衡模型的敏感性和特异性。</p>
<p>下面的代码将画出ROC，Y轴代表true positive率，X轴代表false positive 率。同时，我们也可以增加同时比较两种不同的拟合模型，这里看到的是 <em>KNeighborsClassifier</em> 分类器远胜 <em>LinearSVC</em> 分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">roc_compare_two</span><span class="params">(y, yhats, models)</span>:</span></div><div class="line">    f, (ax1, ax2) = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, sharey=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">for</span> yhat, m, ax <span class="keyword">in</span> ((yhats[<span class="number">0</span>], models[<span class="number">0</span>], ax1), (yhats[<span class="number">1</span>], models[<span class="number">1</span>], ax2)):</div><div class="line">        false_positive_rate, true_positive_rate, thresholds = roc_curve(y,yhat)</div><div class="line">        roc_auc = auc(false_positive_rate, true_positive_rate)</div><div class="line">        ax.set_title(<span class="string">'ROC for %s'</span> % m)</div><div class="line">        ax.plot(false_positive_rate, true_positive_rate, \</div><div class="line">                c=<span class="string">'#2B94E9'</span>, label=<span class="string">'AUC = %0.2f'</span>% roc_auc)</div><div class="line">        ax.legend(loc=<span class="string">'lower right'</span>)</div><div class="line">        ax.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">'m--'</span>,c=<span class="string">'#666666'</span>)</div><div class="line">    plt.xlim([<span class="number">0</span>,<span class="number">1</span>])</div><div class="line">    plt.ylim([<span class="number">0</span>,<span class="number">1.1</span>])</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">y_true_svc, y_pred_svc = get_preds(stdfeatures, labels, LinearSVC())</div><div class="line">y_true_knn, y_pred_knn = get_preds(stdfeatures, labels, KNeighborsClassifier())</div><div class="line"></div><div class="line">actuals = np.array([y_true_svc,y_true_knn])</div><div class="line">predictions = np.array([y_pred_svc,y_pred_knn])</div><div class="line">models = [<span class="string">'LinearSVC'</span>,<span class="string">'KNeighborsClassifier'</span>]</div><div class="line"></div><div class="line">roc_compare_two(actuals, predictions, models)</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/27a127d5-5486-4175-9194-6f0f520bbe03/roc_auc_compare_large.png" alt="ROC_AUC Curve"></p>
<p>在ROC空间，ROC曲线越凸向左上方向效果越好；越靠近对角线，分类器越趋向于随机分类器。</p>
<p>同时，我们也会计算曲线下的面积（AUC），可以结合上图。如果AUC的值达到0.80，那说明分类器分类非常准确；如果AUC值在0.60～0.80之间，那分类器还算好，但是我们调调参数可能会得到更好的性能；如果AUC值小于0.60，那就惨不忍睹了，你得好好分析下咯。</p>
<h5 id="评估回归模型"><a href="#评估回归模型" class="headerlink" title="评估回归模型"></a>评估回归模型</h5><p>对于混凝土数据集试验一些不同的机器学习模型，然后评判哪种更好。在第二篇文章中，我们使用的平均方差和 R 平方值，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Mean squared error = <span class="number">116.268</span></div><div class="line">R2 score = <span class="number">0.606</span></div></pre></td></tr></table></figure>
<p>这些数值是有用的，特别是对不同的拟合模型比较平均方差和 R 平方值。但是，这是不够的，它不能告诉我们为什么一个模型远胜于另外一个；也不能告诉我们如何对模型调参数提高评分。接下来，我们将看到两种可视化的评估技术来帮助诊断模型有效性：预测错误曲线 和 残差曲线。</p>
<h5 id="预测错误曲线"><a href="#预测错误曲线" class="headerlink" title="预测错误曲线"></a>预测错误曲线</h5><p>为了知道我们的模型预测值与期望值到底有多接近，我们将拿混凝土数据集（混凝土强度）做例子，画出其期望值和模型预测值曲线。下面是不同回归模型的错误曲线：<em>Ridge</em>， <em>SVR</em> 和<em>RANSACRegressor</em>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">error_compare_three</span><span class="params">(mods,X,y)</span>:</span></div><div class="line">    f, (ax1, ax2, ax3) = plt.subplots(<span class="number">3</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">for</span> mod, ax <span class="keyword">in</span> ((mods[<span class="number">0</span>], ax1),(mods[<span class="number">1</span>], ax2),(mods[<span class="number">2</span>], ax3)):</div><div class="line">        predicted = cv.cross_val_predict(mod[<span class="number">0</span>], X, y, cv=<span class="number">12</span>)</div><div class="line">        ax.scatter(y, predicted, c=<span class="string">'#F2BE2C'</span>)</div><div class="line">        ax.set_title(<span class="string">'Prediction Error for %s'</span> % mod[<span class="number">1</span>])</div><div class="line">        ax.plot([y.min(), y.max()], [y.min(), y.max()], <span class="string">'k--'</span>, lw=<span class="number">4</span>, c=<span class="string">'#2B94E9'</span>)</div><div class="line">        ax.set_ylabel(<span class="string">'Predicted'</span>)</div><div class="line">    plt.xlabel(<span class="string">'Measured'</span>)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">models = np.array([(Ridge(),<span class="string">'Ridge'</span>), (SVR(),<span class="string">'SVR'</span>), (RANSACRegressor(),<span class="string">'RANSAC'</span>)])</div><div class="line">error_compare_three(models, features, labels)</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/65b6159a-1745-41ac-a71e-d77624566774/model_error_large.png" alt="Visualizing error in regression models"></p>
<p>从这里可以很清晰的看出预测值和期望值的关系。同时也发现线性回归模型效果好。</p>
<h5 id="残差曲线"><a href="#残差曲线" class="headerlink" title="残差曲线"></a>残差曲线</h5><p> 残差是数据集每个实例的实际标记值和预测值之间的差值。通过画出一系列实例的残差，可以帮助我们检测它们是否随机错误。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">resids_compare_three</span><span class="params">(mods,X,y)</span>:</span></div><div class="line">    f, (ax1, ax2, ax3) = plt.subplots(<span class="number">3</span>, sharex=<span class="keyword">True</span>, sharey=<span class="keyword">True</span>)</div><div class="line">    plt.title(<span class="string">'Plotting residuals using training (blue) and test (green) data'</span>)</div><div class="line">    <span class="keyword">for</span> m, ax <span class="keyword">in</span> ((mods[<span class="number">0</span>], ax1),(mods[<span class="number">1</span>], ax2),(mods[<span class="number">2</span>], ax3)):</div><div class="line">        <span class="keyword">for</span> feature <span class="keyword">in</span> list(X):</div><div class="line">            splits = cv.train_test_split(X[[feature]], y, test_size=<span class="number">0.2</span>)</div><div class="line">            X_tn, X_tt, y_tn, y_tt = splits</div><div class="line">            m[<span class="number">0</span>].fit(X_tn, y_tn)</div><div class="line">            ax.scatter(m[<span class="number">0</span>].predict(X_tn),m[<span class="number">0</span>].predict(X_tn)-y_tn,c=<span class="string">'#2B94E9'</span>,s=<span class="number">40</span>,alpha=<span class="number">0.5</span>)</div><div class="line">            ax.scatter(m[<span class="number">0</span>].predict(X_tt), m[<span class="number">0</span>].predict(X_tt)-y_tt,c=<span class="string">'#94BA65'</span>,s=<span class="number">40</span>)</div><div class="line">        ax.hlines(y=<span class="number">0</span>, xmin=<span class="number">0</span>, xmax=<span class="number">100</span>)</div><div class="line">        ax.set_title(m[<span class="number">1</span>])</div><div class="line">        ax.set_ylabel(<span class="string">'Residuals'</span>)</div><div class="line">    plt.xlim([<span class="number">20</span>,<span class="number">70</span>])        <span class="comment"># Adjust according to your dataset</span></div><div class="line">    plt.ylim([<span class="number">-50</span>,<span class="number">50</span>])  </div><div class="line">    plt.show()</div><div class="line"></div><div class="line">models = np.array([(Ridge(),<span class="string">'Ridge'</span>), (LinearRegression(),<span class="string">'Linear Regression'</span>), (SVR(),<span class="string">'SVR'</span>)])</div><div class="line">resids_compare_three(models, features, labels)</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/f4280e91-f5b1-43f8-a2a8-5a0e5b7174b2/residuals_large.png" alt="Plotting residuals in regression models"></p>
<h5 id="Bias-VS-Variance"><a href="#Bias-VS-Variance" class="headerlink" title="Bias VS Variance"></a>Bias VS Variance</h5><p>每种评估器都有是有利有弊。</p>
<p>首先 Error = Bias + Variance。Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。</p>
<h5 id="机器学习可视化调参"><a href="#机器学习可视化调参" class="headerlink" title="机器学习可视化调参"></a>机器学习可视化调参</h5><p>在文章开篇，我们提出了两个问题：我们如何知道一个机器学习模型可以工作？我们如何让这个模型工作（运行）的更好？</p>
<p>接下来，我们将回答第二个问题。如果你有注意，我们用的模型都是使用<strong><em>Scikit-Learn</em></strong> 默认的参数。对于我们的大部分拟合模型来讲，评分已经相当好了。但有时并没有那么幸运，这时我们就得自己调参数。</p>
<h6 id="可视化训练和验证模型"><a href="#可视化训练和验证模型" class="headerlink" title="# 可视化训练和验证模型"></a># 可视化训练和验证模型</h6><p> 如何选择最好的模型参数呢？一种方法是，用单一参数的不同值去验证一个模型的评估分数。让我们拿<em>SVC</em> 分类器来试验，通过调不同的gama值来画出训练值和测试纸的曲线。</p>
<p>我们的关注点是训练值和测试值都高的点。如果两者都低，那是欠拟合（underfit）；如果训练值高但是测试值低，那说明是过拟合（overfit）。</p>
<p>下面的代码画出来的曲线是拿信用卡数据集来做例子，这里用的 6折交叉验证。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_val_curve</span><span class="params">(features, labels, model)</span>:</span></div><div class="line">    p_range = np.logspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line"></div><div class="line">    train_scores, test_scores = validation_curve(</div><div class="line">        model, features, labels, param_name=<span class="string">'gamma'</span>, param_range=p_range,</div><div class="line">        cv=<span class="number">6</span>, scoring=<span class="string">'accuracy'</span>, n_jobs=<span class="number">1</span></div><div class="line">    )</div><div class="line"></div><div class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</div><div class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</div><div class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">    plt.title(<span class="string">'Validation Curve'</span>)</div><div class="line">    plt.xlabel(<span class="string">'$\gamma$'</span>)</div><div class="line">    plt.ylabel(<span class="string">'Score'</span>)</div><div class="line">    plt.semilogx(p_range, train_scores_mean, label=<span class="string">'Training score'</span>, color=<span class="string">'#E29539'</span>)</div><div class="line">    plt.semilogx(p_range, test_scores_mean, label=<span class="string">'Cross-validation score'</span>, color=<span class="string">'#94BA65'</span>)</div><div class="line">    plt.legend(loc=<span class="string">'best'</span>)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">X = scale(credit[[<span class="string">'limit'</span>,<span class="string">'sex'</span>,<span class="string">'edu'</span>,<span class="string">'married'</span>,<span class="string">'age'</span>,<span class="string">'apr_delay'</span>]])</div><div class="line">y = credit[<span class="string">'default'</span>]</div><div class="line">plot_val_curve(X, y, SVC())</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/5b3cc5b4-a6f3-4146-b951-94a2b3547bfc/validation_curve_large.png" alt="Validation curve"></p>
<h5 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h5><p>对于超参数调优，大部分人使用的grid search。Grid search是一种暴力调参方法，即遍历所有可能的参数值。</p>
<p>对于信用卡数据集使用 <em>SVC</em>模型，我们通过试验不同内核系数gama来提高预测准确性：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.grid_search <span class="keyword">import</span> GridSearchCV</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">blind_gridsearch</span><span class="params">(model, X, y)</span>:</span></div><div class="line">    C_range = np.logspace(<span class="number">-2</span>, <span class="number">10</span>, <span class="number">5</span>)</div><div class="line">    gamma_range = np.logspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line">    param_grid = dict(gamma=gamma_range, C=C_range)</div><div class="line">    grid = GridSearchCV(SVC(), param_grid=param_grid)</div><div class="line">    grid.fit(X, y)</div><div class="line"></div><div class="line">    print(</div><div class="line">        <span class="string">'The best parameters are &#123;&#125; with a score of &#123;:0.2f&#125;.'</span>.format(</div><div class="line">            grid.best_params_, grid.best_score_</div><div class="line">        )</div><div class="line">    )</div><div class="line">features = credit[[<span class="string">'limit'</span>,<span class="string">'sex'</span>,<span class="string">'edu'</span>,<span class="string">'married'</span>,<span class="string">'age'</span>,<span class="string">'apr_delay'</span>]]</div><div class="line">labels   = credit[<span class="string">'default'</span>]</div><div class="line">blind_gridsearch(SVC(), features, labels)</div></pre></td></tr></table></figure>
<p>但是，grid search需要我们理解哪些参数是合适的，参数的意义，参数是如何影响模型的以及参数的合理的搜索范围来初始化搜索。</p>
<p>这里，我们使用 <em>visual_gridsearch</em> 代替 <em>blind_gridsearch</em> 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">visual_gridsearch</span><span class="params">(model, X, y)</span>:</span></div><div class="line">    C_range = np.logspace(<span class="number">-2</span>, <span class="number">10</span>, <span class="number">5</span>)</div><div class="line">    gamma_range = np.logspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">5</span>)</div><div class="line">    param_grid = dict(gamma=gamma_range, C=C_range)</div><div class="line">    grid = GridSearchCV(SVC(), param_grid=param_grid)</div><div class="line">    grid.fit(X, y)</div><div class="line"></div><div class="line">    scores = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> grid.grid_scores_]</div><div class="line">    scores = np.array(scores).reshape(len(C_range), len(gamma_range))</div><div class="line"></div><div class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</div><div class="line">    plt.subplots_adjust(left=<span class="number">.2</span>, right=<span class="number">0.95</span>, bottom=<span class="number">0.15</span>, top=<span class="number">0.95</span>)</div><div class="line">    plt.imshow(scores, interpolation=<span class="string">'nearest'</span>, cmap=ddlheatmap)</div><div class="line">    plt.xlabel(<span class="string">'gamma'</span>)</div><div class="line">    plt.ylabel(<span class="string">'C'</span>)</div><div class="line">    plt.colorbar()</div><div class="line">    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=<span class="number">45</span>)</div><div class="line">    plt.yticks(np.arange(len(C_range)), C_range)</div><div class="line">    plt.title(</div><div class="line">        <span class="string">"The best parameters are &#123;&#125; with a score of &#123;:0.2f&#125;."</span>.format(</div><div class="line">        grid.best_params_, grid.best_score_)</div><div class="line">    )</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">visual_gridsearch(SVC(), features, labels)</div></pre></td></tr></table></figure>
<p><img src="https://silvrback.s3.amazonaws.com/uploads/e440eb11-0bbd-44be-a9fb-4264581d654b/validation_heatmap_large.png" alt="Validation accuracy as a function of gamma and C"></p>
<p> <em>visual_gridsearch</em> 的方法可以帮助我们理解不同的模型参数下的精确值。但是超参数调优的路程很长，好些人为此研究了几十年。</p>
<h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>这是可视化机器学习部分的最后一篇，可视化在机器学习的过程占用重要的角色。许多工具都提供这个功能，比如， <em>Scikit-Learn</em> ，<em>Matplotlib</em> ， <em>Pandas</em> ，<em>Bokeh</em> 和 <em>Seaborn</em>。</p>
<p><em>希望我写的对部分人有用，如果是这样，请让我知道，谢谢。</em></p>
<p>Enjoy!</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/20/streaming-process-data-access/" itemprop="url">
                  流式处理架构的“瓶颈”：数据访问（上）
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-20T11:31:42+08:00" content="2016-09-20">
              2016-09-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>写在之前：这是用微软输入法打出来的一篇文章。</em></p>
<h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a><em>背景</em></h5><p>在Linkedin，大体架构是：ApacheSamza作为流式处理框架，Apache Kafka作为持久化的订阅/发布消息中间件，Databus监控数据库的变化。</p>
<h5 id="数据访问（Data-access）的两种模式"><a href="#数据访问（Data-access）的两种模式" class="headerlink" title="数据访问（Data access）的两种模式"></a><em>数据访问（Data access）的两种模式</em></h5><p>为什么数据访问是规模化的挑战？我们处理的数据访问模式主要分两种：</p>
<p><em>Read/write data：</em><br>这里给出一个在Linkedin中使用read-write数据访问模式的场景。Linkedin许多应用需要推送信息给会员，不管通过email还是通知。为了保证更好的用户体验，尽量不多给会员发email。Linkedin开发一个基于Samza的ATC（Air Traffic Control ）应用来控制email和通知送达终端用户。ATC追踪每个会员收到的最后一封邮件的时间，以及所有新的邮件请求的时间。ATC维护着每个会员的状态信息（read/write）。</p>
<p><em>Read-only data：</em><br>同样，给出一个只读数据访问的场景。Linkedin开发一个应用监听会员点击某条广告的事件的时间。这个应用会生成一个AdQuality事件，并高亮点击某些特定广告的会员特征。AdQuality事件会用来训练广告推荐机器学习模型。应用处理AdQuality事件会去查询点击广告的会员的画像。</p>
<h5 id="数据访问的关键特征"><a href="#数据访问的关键特征" class="headerlink" title="数据访问的关键特征"></a><em>数据访问的关键特征</em></h5><p>除了以上描述的两种访问模式，以下两种数据访问的关键特征也将会极大地影响事件流式处理的架构。</p>
<p><em>数据访问是否分区？</em><br>上面的场景，Kafka的topic是以MemberId分区的。输入事件已经按会员信息（MemberId）分区，那每个事件处理节点仅仅需要访问一个不变的会员数据集。后续我们会看到，如何对已经分区过的数据访问进行缓存优化。<br>另外一种场景，假设处理每个事件都需要查询Company库获取会员的更多信息，那每个事件处理节点就得查询可能的每个公司。这是访问未分区数据的例子。</p>
<p><em>数据集的大小</em><br>后面会看到，访问一个5M数据集的解决方案跟访问一个5 TB数据集的方案完全不同。比如，你可以把5M数据集完整的存储到每个节点上，然而很显然你不可能对5 TB的数据集做同样的操作。</p>
<h5 id="数据访问的解决方案"><a href="#数据访问的解决方案" class="headerlink" title="数据访问的解决方案"></a><em>数据访问的解决方案</em></h5><p>下面是展示的是两种常用的数据访问解决方案。</p>
<p><em>远程存储</em>：这是开发应用的传统的方式。当一个应用处理一个事件，它会远程调用一个隔离的SQL或者No-SQL数据库。这种方法中，写操作总是采用远程调用，但是读数据可以通过本地缓存进行一定程度上的优化。LinkedIn有大量的应用采用这种方法。<br>另外一种模式，在远程数据库（比如，Oracle）前面前置一个远程缓存（比如，Couchbase）。远程缓存主要用来数据读操作，应用通过Databus之类的工具追踪数据库变化，并替代远程缓存。</p>
<p><em>本地存储（嵌入式）</em>：这种方法是要求事件处理结果存储的位置和事件处理的地方在同一台机器上。极端的情况是所有数据存储访问都是本地，这样效率最高。<br>Samza天生就是支持嵌入式本地数据库，它支持把RocksDB嵌入你的事件处理器。它是通过Kafka log compacted topic做备份。</p>
<p>也有其它框架，比如Microsoft ServiceFabric，它本身内建支持本地应用存储。ServiceFabric支持持久化数据到本地磁盘，并把备份持久化到其它处理器实例。ServiceFabric持久化会自动备份到Azure storage。</p>
<h5 id="事件到达点-VS-事件处理点"><a href="#事件到达点-VS-事件处理点" class="headerlink" title="事件到达点 VS 事件处理点"></a><em>事件到达点 VS 事件处理点</em></h5><p>本地存储和远程存储的讨论是相对于事件处理的位置。在选择使用的框架时，事件到达的位置可能与事件处理的位置不在同一位置。</p>
<p>像GoogleDataflow这类框架支持从未分区的输入源（GooglePub-Sub）读取数据。这种模型中，当事件到达时，处理器会先找出当前事件应该归哪个处理器处理，并转发到对应的实际处理器处理。</p>
<p>而Samza、Spark Streaming和Flink之类的流处理框架《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483679&amp;idx=1&amp;sn=5e544ae789c8773f73b9e1d552e5f991#wechat_redirect" target="_blank" rel="external">实时流处理框架选型：就应该这样“拉出来遛遛”</a>》天然就支持事件的分区（Kafka，Kinesis等），因此不会再做一步转发处理。</p>
<p>如果你的应用得瓶颈是网络带宽或者计算能力，那处理事件的节点和事件的到达在同一个节点不会出现转发，将会极大的提高性能。</p>
<p>抛开在事件处理之前进行事件转发的情况，本文讨论的在事件处理过程中考虑数据访问这些依然对所有的事件处理框架适用。</p>
<h5 id="在路上"><a href="#在路上" class="headerlink" title="在路上"></a><em>在路上</em></h5><p>海量流式处理没有一个完美的方案，大家可以根据公司的场景进行平衡取舍。</p>
<p>本文主要讲了两种数据访问的模式，以及数据访问关键特征对架构的影响，并给出了相应的解决方案。最后讨论了事件到达和事件处理位置不同带来的解决方案不同。下期更精彩，敬请关注。</p>
<p><em>PS：最近更新较少，说声抱歉了。这几周工作较忙，主要涉及到的知识点有：CDH集群、Hue、Zeppelin、Caravel for Hive、elasticsearch on hadoop/Hive、Gobblin、Neo4j等，可以后台留言交流。</em></p>
<p><em>参考</em>：<br>[1] <a href="https://engineering.linkedin.com/blog/2016/08/stream-processing-hard-problems-part-ii--data-access" target="_blank" rel="external">https://engineering.linkedin.com/blog/2016/08/stream-processing-hard-problems-part-ii--data-access</a></p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/09/20/visual-tools-for-machine-learning-part-2/" itemprop="url">
                  机器学习模型选择如此简单
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-09-20T11:26:21+08:00" content="2016-09-20">
              2016-09-20
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>写在之前：有些概念跟平时你见过的机器学习文章描述的可能不太一样，但是它会给你一种“心里一颤“的感觉</em>。</p>
<p>机器学习的讨论经常会涉及到的问题是：什么机器学习模型才是最好的？是逻辑回归模型，随机森林模型，贝叶斯方法模型，支持向量机模型？抑或是神经网络模型？每个人似乎都有自己心中最爱！但这些讨论试图把机器学习的挑战缩减为单个问题，而这对机器学习的初学者带来了特别严重的误解。</p>
<p>选择一个好的机器学习模型固然重要，但这远远不够。在缺乏领域知识，基本假设，数据选型和实际的应用的情况下，还是值得商榷的。关于机器学习模型评价这部分将留在下一篇文章阐述。</p>
<p> feature engineering (FE)，algorithm selection (AS)，and parameter tuning (PT)；</p>
<h5 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h5><p>能训练一个“合适”的模型和预测是相当依赖特征工程、参数调优和模型选择。模型选择是机器学习过程比较难的部分，复杂、迭代，经常不断的去“试错”和重复。</p>
<h6 id="模型选择实战"><a href="#模型选择实战" class="headerlink" title="模型选择实战"></a>模型选择实战</h6><p>相信大家对Scikit-Learn“<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map" target="_blank" rel="external">如何选择Estimator</a>”里的流程图非常熟悉了，不熟悉的点开链接读读。这个流程图是给初学者一个选择机器学习算法的最佳实践的参考手册。</p>
<p><img src="http://scikit-learn.org/stable/_static/ml_map.png" alt="image"></p>
<p>首先，看下我们的数据集（三个数据集参考上篇《<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483684&amp;idx=1&amp;sn=428cf35632b2408e1dc7d36dff497c53&amp;scene=21#wechat_redirect" target="_blank" rel="external">可视化图表让机器学习“biu”的一样简单：特征分析</a>》）的样本数是否够50。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">print</span> len(occupancy) <span class="comment"># 8,143</span></div><div class="line"><span class="keyword">print</span> len(credit)    <span class="comment"># 30,000</span></div><div class="line"><span class="keyword">print</span> len(concrete)  <span class="comment"># 1,030</span></div></pre></td></tr></table></figure>
<p>很显然这个条件是满足的。接着看下是否我们是否预测类别。对于房屋入住和信用卡数据集来说是判断类别；而混凝土数据集，缓凝土的抗压强度是连续数据，所以预测的是数量。因此，为前两个数据集选择分类器（classifier）；为后者选择回归模型（regressor）。</p>
<p>因为我们的两个判断类别的数据集都小于100K，接着按图选择<em>sklearn.svm.LinearSVC</em>（其会将数据集映射到高维特征空间）；如果失败，就再选择<em>sklearn.neighbors.KNeighborsClassifier</em>（其会分配样本到它的K领域）。你应该还记得房屋入住数据集单位不统一，所以这里引入<em>scale</em>进行归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> scale</div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(attributes, targets, model)</span>:</span></div><div class="line">    <span class="comment"># Split data into 'test' and 'train' for cross validation</span></div><div class="line">    splits = cv.train_test_split(attributes, targets, test_size=<span class="number">0.2</span>)</div><div class="line">    X_train, X_test, y_train, y_test = splits</div><div class="line"></div><div class="line">    model.fit(X_train, y_train)</div><div class="line">    y_true = y_test</div><div class="line">    y_pred = model.predict(X_test)</div><div class="line">    print(confusion_matrix(y_true, y_pred))</div><div class="line"></div><div class="line"><span class="comment"># Divide data frame into features and labels</span></div><div class="line">features = occupancy[[<span class="string">'temp'</span>, <span class="string">'humid'</span>, <span class="string">'light'</span>, <span class="string">'co2'</span>, <span class="string">'hratio'</span>]]</div><div class="line">labels   = occupancy[<span class="string">'occupied'</span>]</div><div class="line"></div><div class="line"><span class="comment"># Scale the features</span></div><div class="line">stdfeatures = scale(features)</div><div class="line"></div><div class="line">classify(stdfeatures, labels, LinearSVC())</div><div class="line">classify(stdfeatures, labels, KNeighborsClassifier())</div></pre></td></tr></table></figure>
<p>对于信用卡数据集使用相同的<em>classify</em>，根据上篇的特征分析经验，我们这里需要先进行数据缺失处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">features = credit[[</div><div class="line">    <span class="string">'limit'</span>, <span class="string">'sex'</span>, <span class="string">'edu'</span>, <span class="string">'married'</span>, <span class="string">'age'</span>, <span class="string">'apr_delay'</span>, <span class="string">'may_delay'</span>,</div><div class="line">    <span class="string">'jun_delay'</span>, <span class="string">'jul_delay'</span>, <span class="string">'aug_delay'</span>, <span class="string">'sep_delay'</span>, <span class="string">'apr_bill'</span>, <span class="string">'may_bill'</span>,</div><div class="line">    <span class="string">'jun_bill'</span>, <span class="string">'jul_bill'</span>, <span class="string">'aug_bill'</span>, <span class="string">'sep_bill'</span>, <span class="string">'apr_pay'</span>, <span class="string">'may_pay'</span>,</div><div class="line">    <span class="string">'jun_pay'</span>, <span class="string">'jul_pay'</span>, <span class="string">'aug_pay'</span>, <span class="string">'sep_pay'</span></div><div class="line">]]</div><div class="line">labels   = credit[<span class="string">'default'</span>]</div><div class="line"></div><div class="line">stdfeatures = scale(features)</div><div class="line"></div><div class="line">classify(stdfeatures, labels, LinearSVC())</div><div class="line">classify(stdfeatures, labels, KNeighborsClassifier())</div></pre></td></tr></table></figure>
<p>对于混凝土数据集，我们得决定是否所有的特征都重要，或者只有一部分重要。如果选择所有的特征都很重要，那根据流程图手册路线应该选择<em>sklearn.linear_model.RidgeRegression</em>或者<em>sklearn.svm.SVR</em>（有点类似LinearSVC classifier）；如果觉得只有部分特征重要，那就选择<em>sklearn.linear_model.Lasso</em>（其会在预测时舍弃部分特征）或者<em>sklearn.linear_model.ElasticNet</em>（其介入 Lasso方法和Ridge方法之间，L1和 L2惩罚的线性组合）。</p>
<p>下面来试试看咯：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, Lasso, ElasticNet</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">regress</span><span class="params">(attributes, targets, model)</span>:</span></div><div class="line">    splits = cv.train_test_split(attributes, targets, test_size=<span class="number">0.2</span>)</div><div class="line">    X_train, X_test, y_train, y_test = splits</div><div class="line"></div><div class="line">    model.fit(X_train, y_train)</div><div class="line">    y_true = y_test</div><div class="line">    y_pred = model.predict(X_test)</div><div class="line">    print(<span class="string">'Mean squared error = &#123;:0.3f&#125;'</span>.format(mse(y_true, y_pred)))</div><div class="line">    print(<span class="string">'R2 score = &#123;:0.3f&#125;'</span>.format(r2_score(y_true, y_pred)))</div><div class="line"></div><div class="line">features = concrete[[</div><div class="line">    <span class="string">'cement'</span>, <span class="string">'slag'</span>, <span class="string">'ash'</span>, <span class="string">'water'</span>, <span class="string">'splast'</span>, <span class="string">'coarse'</span>, <span class="string">'fine'</span>, <span class="string">'age'</span></div><div class="line">]]</div><div class="line">labels   = concrete[<span class="string">'strength'</span>]</div><div class="line"></div><div class="line">regress(features, labels, Ridge())</div><div class="line">regress(features, labels, Lasso())</div><div class="line">regress(features, labels, ElasticNet())</div></pre></td></tr></table></figure>
<p>正如上面代码展示的那样，Scikit-Learn API使得我们可以快速的发布我们需要的模型，这是Scikit-Learn一个强有力的魔力。</p>
<h5 id="可视化模型"><a href="#可视化模型" class="headerlink" title="可视化模型"></a>可视化模型</h5><p>Scikit-Learn流程图非常有用是因为其提供了使用路径地图，但是它不能提供各种模型的函数。因而另外两幅图成为Scikit-Learn的权威：<a href="http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html" target="_blank" rel="external">分类器比较</a>和<a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html" target="_blank" rel="external">聚类比较</a>。</p>
<p>多个小图形很容易比较出不同的数据集适合的聚类算法：</p>
<p><img src="http://scikit-learn.org/stable/_images/plot_classifier_comparison_001.png" alt="image"></p>
<p>类似的，分类器比较图很好的帮助我们对不同的数据集选择哪种合适的分类器：</p>
<p><img src="http://scikit-learn.org/stable/_images/plot_cluster_comparison_001.png" alt="image"></p>
<p>一般来说，这些图形仅仅是证明了各种模型对不同数据集的优化；但我相信大家都希望有一种可视化工具可以对同一个数据集使用不同的模型的情况进行比较。</p>
<h5 id="模型簇"><a href="#模型簇" class="headerlink" title="模型簇"></a>模型簇</h5><p>首先给出“模型”这个词的定义，它包括三方面：</p>
<ul>
<li>模型簇：比如，linear model，nearest neighbors，SVM，Bayes等模型；</li>
<li>模型形式：比如，sklearn.linear_model.Ridge()，sklearn.linear_model.Lasso()，sklearn.linear_model.ElasticNet等；</li>
<li>拟合模型：比如，Ridge().fit(X_train, y_train)。</li>
</ul>
<p>模型簇是由特征空间决定的；模型形式是通过试验和统计检验来选择的；拟合模型是由参数调优和机器计算生成的。</p>
<p>我们讨论的这些，模型形式的试验会在后续文章中讲到，这部分是我们期望能够得到回报的想象空间。模型形式是指出我们的特征是如何和模型簇相关。</p>
<p>我喜欢的模型展示工具之一是Dr. Saed Sayad的可交互的“<a href="http://www.saedsayad.com/data_mining_map.htm" target="_blank" rel="external">数据挖掘地图</a>”。它比Scikit-Learn的流程图手册综合性更高，并且结合里模型簇和模型形式的概念。除了预测方法外，Sayad地图也包含了统计方法部分。</p>
<p>这里给出一个普适的流程图，它旨在结合Sayad地图和 Scikit-Learn流程图。颜色和等级代表模型形式和模型簇：</p>
<p><img src="http://img1.ph.126.net/xsHUK3F6SKjQtXUHzDliYA==/6631651606375660672.png" alt="image"></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>通过在同一个数据集上比较和对比不同模型的性能，我们能从模型簇中直观的选取模型形式。</p>
<p>下期将会讲解拟合模型和调参的可视化工具。</p>
<p><em>PS：这周深入的“研究”了下Flume，日志收集利器，但是对Hadoop版本的支持太低，填了不少坑……</em></p>
<p><em>希望我写的对部分人有用，如果是这样，请让我知道，谢谢。</em></p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/spark-two-series-part-2/" itemprop="url">
                  【Spark 2.0系列】：Catalog和自定义Optimizer
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T23:37:48+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Catalog和自定义Optimizer"><a href="#Catalog和自定义Optimizer" class="headerlink" title="Catalog和自定义Optimizer"></a>Catalog和自定义Optimizer</h4><p>Spark 2.0系列第一篇见<a href="http://mp.weixin.qq.com/s?__biz=MzI0MDIxMDM0MQ==&amp;mid=2247483729&amp;idx=1&amp;sn=22c40fd932ba7ffc3684a45e1861dd27#wechat_redirect" target="_blank" rel="external">Spark 2.0系列】：Spark Session API和Dataset API</a>，本文将讲解Spark 2.0 的Catalog 和Custom Optimizer。</p>
<p>首先，先了解下RDD 和Dataset 在开发中使用对比。</p>
<h5 id="RDD-和Dataset-使用对比"><a href="#RDD-和Dataset-使用对比" class="headerlink" title="RDD 和Dataset 使用对比"></a>RDD 和Dataset 使用对比</h5><p>Dataset API 是RDD 和DataFrame API 的统一，但大部分Dataset API 与RDD API使用方法看起来是相似的（其实实现方法是不同的）。所以RDD代码很容易转换成Dataset API。下面直接上代码：</p>
<h6 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h6><ul>
<li>RDD</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> rdd = sparkContext.textFile(<span class="string">"src/main/resources/data.txt"</span>)</div><div class="line">  </div><div class="line"><span class="keyword">val</span> wordsRDD = rdd.flatMap(value =&gt; value.split(<span class="string">"\\s+"</span>))</div><div class="line"><span class="keyword">val</span> wordsPair = wordsRDD.map(word =&gt; (word,<span class="number">1</span>))</div><div class="line"><span class="keyword">val</span> wordCount = wordsPair.reduceByKey(_+_)</div></pre></td></tr></table></figure>
<ul>
<li>Dataset</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> ds = sparkSession.read.text(<span class="string">"src/main/resources/data.txt"</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> sparkSession.implicits._</div><div class="line"><span class="keyword">val</span> wordsDs = ds.flatMap(value =&gt; value.split(<span class="string">"\\s+"</span>))</div><div class="line"><span class="keyword">val</span> wordsPairDs = wordsDs.groupByKey(value =&gt; value)</div><div class="line"><span class="keyword">val</span> wordCountDs = wordsPairDs.count()</div></pre></td></tr></table></figure>
<h6 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h6><table>
<thead>
<tr>
<th></th>
<th>RDD</th>
<th>Dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td>Caching</td>
<td>rdd.cache()</td>
<td>ds.cache()</td>
</tr>
<tr>
<td>Filter</td>
<td>val filteredRDD = wordsRDD.filter(value =&gt; value ==”hello”)</td>
<td>val filteredDS = wordsDs.filter(value =&gt; value ==”hello”)</td>
</tr>
<tr>
<td>Map Partition</td>
<td>val mapPartitionsRDD = rdd.mapPartitions(iterator =&gt; List(iterator.count(value =&gt; true)).iterator)</td>
<td>val mapPartitionsDs = ds.mapPartitions(iterator =&gt; List(iterator.count(value =&gt; true)).iterator)</td>
</tr>
<tr>
<td>reduceByKey</td>
<td>val reduceCountByRDD = wordsPair.reduceByKey(<em>+</em>)</td>
<td>val reduceCountByDs = wordsPairDs.mapGroups((key,values) =&gt;(key,values.length))</td>
</tr>
</tbody>
</table>
<h6 id="Dataset-和RDD-相互转换"><a href="#Dataset-和RDD-相互转换" class="headerlink" title="Dataset 和RDD 相互转换"></a>Dataset 和RDD 相互转换</h6><ul>
<li>RDD</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> dsToRDD = ds.rdd</div></pre></td></tr></table></figure>
<ul>
<li>Dataset</li>
</ul>
<p>RDD 转换成Dataframe稍麻烦，需要指定schema。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> rddStringToRowRDD = rdd.map(value =&gt; <span class="type">Row</span>(value))</div><div class="line"><span class="keyword">val</span> dfschema = <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">"value"</span>,<span class="type">StringType</span>)))</div><div class="line"><span class="keyword">val</span> rddToDF = sparkSession.createDataFrame(rddStringToRowRDD,dfschema)</div><div class="line"><span class="keyword">val</span> rDDToDataSet = rddToDF.as[<span class="type">String</span>]</div></pre></td></tr></table></figure>
<h5 id="Catalog-API"><a href="#Catalog-API" class="headerlink" title="Catalog API"></a>Catalog API</h5><p>DataSet 和Dataframe API 支持结构化数据分析，而结构化数据重要的是管理metadata。这里的metadata包括temporary metadata（临时表）；registered udfs；permanent metadata（Hive metadata或HCatalog）。</p>
<p>早期Spark版本并未提供标准的API访问metadata，开发者需要使用类似<em>show tables</em>的查询来查询metadata；而Spark 2.0 在Spark SQL中提供标准API 调用catalog来访问metadata。</p>
<h6 id="访问Catalog"><a href="#访问Catalog" class="headerlink" title="访问Catalog"></a>访问Catalog</h6><p>建立SparkSession，然后调用Catalog：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> catalog = sparkSession.catalog</div></pre></td></tr></table></figure>
<h6 id="查询数据库"><a href="#查询数据库" class="headerlink" title="查询数据库"></a>查询数据库</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.listDatabases().select(<span class="string">"name"</span>).show()</div></pre></td></tr></table></figure>
<p><em>listDatabases</em>可查询所有数据库。在Hive中，Catalog可以访问Hive metadata中的数据库。<em>listDatabases</em>返回一个dataset，所以你可以使用适用于dataset的所有操作去处理metadata。</p>
<h6 id="用createTempView-注册Dataframe"><a href="#用createTempView-注册Dataframe" class="headerlink" title="用createTempView 注册Dataframe"></a>用createTempView 注册Dataframe</h6><p>早期版本Spark用<em>registerTempTable</em>注册dataframe，而Spark 2.0 用<em>createTempView</em>替代。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">df.createTempView(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>一旦注册视图，即可使用<em>listTables</em>访问所有表。</p>
<h6 id="查询表"><a href="#查询表" class="headerlink" title="查询表"></a>查询表</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.listTables().select(<span class="string">"name"</span>).show()</div></pre></td></tr></table></figure>
<h6 id="检查表缓存"><a href="#检查表缓存" class="headerlink" title="检查表缓存"></a>检查表缓存</h6><p>通过Catalog可检查表是否缓存。访问频繁的表缓存起来是非常有用的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.isCached(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>默认表是不缓存的，所以你会得到false。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df.cache()</div><div class="line">catalog.isCached(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<p>现在将会打印true。</p>
<h6 id="删除视图"><a href="#删除视图" class="headerlink" title="删除视图"></a>删除视图</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">catalog.dropTempView(<span class="string">"sales"</span>)</div></pre></td></tr></table></figure>
<h6 id="查询注册函数"><a href="#查询注册函数" class="headerlink" title="查询注册函数"></a>查询注册函数</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">catalog.listFunctions().</div><div class="line">select(<span class="string">"name"</span>,<span class="string">"description"</span>,<span class="string">"className"</span>,<span class="string">"isTemporary"</span>).show(<span class="number">100</span>)</div></pre></td></tr></table></figure>
<p>Catalog不仅能查询表，也可以访问UDF。上面代码会显示Spark Session中所有的注册函数（包括内建函数）。</p>
<h5 id="自定义-Optimizer"><a href="#自定义-Optimizer" class="headerlink" title="自定义 Optimizer"></a>自定义 Optimizer</h5><h6 id="Catalyst-optimizer"><a href="#Catalyst-optimizer" class="headerlink" title="Catalyst optimizer"></a>Catalyst optimizer</h6><p>Spark SQL使用Catalyst优化所有的查询，优化之后的查询比直接操作RDD速度要快。Catalyst是基于rule的，每个rule都有一个特定optimization，比如，<em>ConstantFolding</em> rule用来移除常数表达式，具体可直接看Spark SQL源代码。</p>
<p>在早期版本Spark中，如果想自定义optimization，需要开发者修改Spark源代码。操作起来麻烦，而且要求开发者能读懂源码。在Spark 2.0中，已提供API自定义optimization。</p>
<h6 id="访问Optimized-plan"><a href="#访问Optimized-plan" class="headerlink" title="访问Optimized plan"></a>访问Optimized plan</h6><p>在开始编写自定义optimization之前，先来看看如何访问optimized plan：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> df = sparkSession.read.option(<span class="string">"header"</span>,<span class="string">"true"</span>).csv(<span class="string">"src/main/resources/data.csv"</span>)</div><div class="line"><span class="keyword">val</span> multipliedDF = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</div><div class="line">println(multipliedDF.queryExecution.optimizedPlan.numberedTreeString)</div></pre></td></tr></table></figure>
<p>上面的代码是加载一个csv文件，并对某一行所有值乘以1。<em>queryExecution</em> 可访问查询相关的所有执行信息。 <em>queryExecution</em> 的<em>optimizedPlan</em>对象可以访问dataframe的optimized plan。</p>
<p>Spark中的执行计划以tree表示，所以用<em>numberedTreeString</em>打印optimized plan。打印结果如下：</p>
<blockquote>
<p>00 Project [(cast(amountPaid#3 as double) <em> 1.0) AS (amountPaid </em> 1)#5]<br>01 +- Relation[transactionId#0,customerId#1,itemId#2,amountPaid#3] csv</p>
</blockquote>
<p>所有执行计划是由底向上读取：</p>
<ul>
<li>01 Relation - 从csv 文件建立一个dataframe</li>
<li>00 Project - 投影操作</li>
</ul>
<h6 id="编写自定义optimizer-rule"><a href="#编写自定义optimizer-rule" class="headerlink" title="编写自定义optimizer rule"></a>编写自定义optimizer rule</h6><p>从上面的执行计划可以清晰的看到：对一列的每个值乘以1 这里并没有优化。我们知道，乘以1 这个操作应该返回的是值本身，所以可以利用这个特点来增加只能点的optimizer。代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">MultiplyOptimizationRule</span> <span class="keyword">extends</span> <span class="title">Rule</span>[<span class="type">LogicalPlan</span>] </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = plan transformAllExpressions &#123;</div><div class="line">      <span class="keyword">case</span> <span class="type">Multiply</span>(left,right) <span class="keyword">if</span> right.isInstanceOf[<span class="type">Literal</span>] &amp;&amp;</div><div class="line">        right.asInstanceOf[<span class="type">Literal</span>].value.asInstanceOf[<span class="type">Double</span>] == <span class="number">1.0</span> =&gt;</div><div class="line">        println(<span class="string">"optimization of one applied"</span>)</div><div class="line">        left</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>这里MultiplyOptimizationRule扩展自Rule类，采用Scala的模式匹配编写。检测右操作数是否是 1，如果是1 则直接返回左节点。</p>
<p>把MultiplyOptimizationRule加入进optimizer：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sparkSession.experimental.extraOptimizations = <span class="type">Seq</span>(<span class="type">MultiplyOptimizationRule</span>)</div></pre></td></tr></table></figure>
<p>你可以使用<em>extraOptimizations</em>将定义好的Rule加入 catalyst。</p>
<p>下面实际使用看看效果：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">val</span> multipliedDFWithOptimization = df.selectExpr(<span class="string">"amountPaid * 1"</span>)</div><div class="line">println(<span class="string">"after optimization"</span>)</div><div class="line">println(multipliedDFWithOptimization.queryExecution.</div><div class="line">optimizedPlan.numberedTreeString)</div></pre></td></tr></table></figure>
<p>我们看到打印结果：</p>
<blockquote>
<p>00 Project [cast(amountPaid#3 as double) AS (amountPaid * 1)#7]<br>01 +- Relation[transactionId#0,customerId#1,itemId#2,amountPaid#3] csv</p>
</blockquote>
<p>说明自定义Optimizer已生效。</p>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/spark_and_redis_connector/" itemprop="url">
                  Spark借助Redis提升45倍处理效率！
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:57:36+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>摘要</em>:时常采用内存数据结构会使得程序更加高效，比如，Spark借助Redis可以提速45倍。</p>
<p>Spark代表着下一代大数据处理技术，并且，借着开源算法和计算节点集群分布式处理，Spark和Hadoop在执行的方式和速度已经远远的超过传统单节点的技术架构。但Spark利用内存进行数据处理，这让Spark的处理速度超过基于磁盘的Hadoop 100x 倍。</p>
<p>但Spark和内存数据库<a href="http://www.redislabs.com" target="_blank" rel="external">Redis</a>结合后可显著的提高Spark运行任务的性能，这源于Redis优秀的数据结构和执行过程，从而减小数据处理的复杂性和开销。Spark通过一个Redis连接器可以访问Redis的数据和API，加速Spark处理数据。</p>
<p>Spark和Redis结合使用到底有多大的性能提升呢？结合这两者来处理时序数据时可以提高46倍以上——而不是提高百分之四十五。</p>
<p>为什么这些数据处理速度的提升是很重要的呢？现在，越来越多的公司期望在交易完成的同时完成对应的数据分析。公司的决策也需要自动化，而这些需要数据分析能够实时的进行。Spark是一个用的较多的数据处理框架，但它不能做到百分之百实时，要想做到实时处理Spark还有很大一步工作需要做。<br><img src="http://img2.ph.126.net/QalVm_AuA5Vx7x2lxXEWbw==/6598067023763790131.png" alt="此处输入图片的描述"><br>图1</p>
<h3 id="Spark-RDD"><a href="#Spark-RDD" class="headerlink" title="Spark RDD"></a>Spark RDD</h3><p>Spark采用弹性分布式数据集（RDD），可将数据存在易变的内存中或持久化到磁盘上。 RDD具有不可变化性，分布式存储在Spark集群的各节点，RDD经过tansform操作后创建出一个新的RDD。RDD是Spark中数据集的一种重要抽象，具有良好的容错性、高效的迭代处理。</p>
<h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><p>Redis天生为高性能设计，通过良好的数据存储结构能达到亚毫秒级的延迟。Redis的数据存储结构不仅仅提高内存的利用和减小应用的复杂性，也降低了网络负载、带宽消耗和处理时间。Redis数据结构包括字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets）， bitmaps， hyperloglogs 和 地理空间（geospatial）索引半径查询。</p>
<p>下面来展示Redis的数据结构如何来简化应用的处理时间和复杂度。这里用有序集合来举例，一个以评分（score）大小排序的元素集合。<br><img src="http://img2.ph.126.net/MatkU-NaQLLkh2i_OdRhtw==/6598097810089184567.png" alt="此处输入图片的描述"><br>图2</p>
<p>Redis能存储多种数据类型，并自动的以评分（score）排序。常见的例子有，按价格排序的商品，以阅读数排序的文章名，股票价格时序数据，带时间戳的传感器读数。<br>有序集合依赖Redis优秀的内建操作可以实现范围查询、求交集，可以非常快地（O(log(N))）完成添加，删除和更新元素的操作。Redis内建函数不仅减少代码开发，在内存中执行也减小了网络延时和带宽消耗，可达到亚毫秒级的吞吐延迟。特别地，对时序数据集合来讲，有序集合数据结构比使用内存键值对或使用磁盘的数据库，能给数据分析带来数量级上的性能提升。</p>
<h3 id="Spark-Redis-connector"><a href="#Spark-Redis-connector" class="headerlink" title="Spark-Redis connector"></a>Spark-Redis connector</h3><p>为了提高Spark数据分析的能力，Redis团队开发了一个<a href="https://github.com/RedisLabs/spark-redis" target="_blank" rel="external">Spark-Redis connector</a>，它使得Spark可以直接使用Redis作为数据源，顺理成章的Spark也能使用Redis的各数据结构，进而显著的提升Spark分析数据的速度。<br><img src="http://img2.ph.126.net/EjDbOsyQrDo6nRi2gkM66w==/6598082416926397895.png" alt="此处输入图片的描述"><br>图3</p>
<p>为了展示Spark结合Redis所产生的效果，Redis团队拿时序数据集合做基准测试，测试了Spark在不同情况下执行时间范围查询：Spark使用堆外内存；Spark使用Tachyon作为堆外缓存；Spark使用HDFS存储；Spark结合Redis使用。</p>
<p>Redis团队改进了Cloudera的Spark分析时序数据的包，采用<a href="https://github.com/RedisLabs/spark-timeseries/blob/redis/src/main/scala/com/redislabs/provider/redis/rdd/RedisRDD.scala#L26" target="_blank" rel="external">Redis有序集合数据结构加速时序数据分析</a>，并且实现Spark访问Redis各类数据结构的接口。此Spark-Redis时序开发包主要做了两件事：</p>
<ol>
<li>它让Redis节点与Spark集群的节点自动匹配，确保每个Spark节点都使用本地Redis节点，这样可以明显的优化延迟时间；</li>
<li>集成Spark DataFrame和Spark读取数据源，使得Spark SQL查询可自动转化，并能借助Redis能有效的恢复数据。</li>
</ol>
<p>换句话说，使用Spark-Redis时序开发包意味着用户无需担心Spark和Redis两者如何使用。用户使用Spark SQL进行数据分析可以获得极大的查询性能提升。</p>
<h3 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h3><p>基准测试的时序数据集是跨度32年的1024个股票交易市场按天随机生成的数据。每个股票交易所都有有序数据集，以日期和元素属性（开盘价、最高价、最低价、收盘价等）排序，在Redis中以有序数据结构存储，采用Spark进行数据分析，描述如图4<br><img src="http://img2.ph.126.net/heSh9IpGiauqBoZqf9egyg==/6598076919368260259.png" alt="此处输入图片的描述"><br>图4</p>
<p>在上述列子中，就有序集合AAPL来看，有序数据集合以天为评分（score，以蓝色表示），每天相关的值为一行（Member，以灰色表示）。在Redis中，只要执行一个ZRANGEBYSCORE操作就可以获取一个指定时间范围内的所有股票数据，并且Redis执行此查询要比其他Key/Value数据库快100倍。<br>从图x可以看到，横向比较各种情况的基准测试，Spark结合Redis执行时间片的查询速度比Spark使用HDFS快135倍、比Spark使用堆内内存或Spark使用Tachyon作为堆外内存要快45倍。<br><img src="http://img1.ph.126.net/UieEUHKoGh9ex9aJiMHPZQ==/6598192368089172532.png" alt="此处输入图片的描述"><br>图5</p>
<h3 id="Spark-Redis其它应用"><a href="#Spark-Redis其它应用" class="headerlink" title="Spark-Redis其它应用"></a>Spark-Redis其它应用</h3><p>按照“<a href="https://redislabs.com/solutions/spark-and-redis" target="_blank" rel="external">Getting Started with Spark and Redis</a>”指南，你可以一步步安装Spark集群和使用Spark-Redis包。它提供一个简单的wordcount的例子展示如何使用Spark结合Redis。待你熟练使用后可以自己进一步挖掘、优化其他的Redis数据结构。<br>Redis的有序集合数据结构很适合时序数据集合，而Redis其他数据结构（比如，列表（lists）， 集合（sets）和 地理空间（geospatial）索引半径查询）也能进一步丰富Spark的数据分析。当使用Spark抽取地理空间信息来获取新产品的人群偏好和邻近中心的位置，可结合Redis的地理空间（geospatial）索引半径查询来优化。</p>
<p>Spark支持一系列的数据分析，包括SQL、机器学习、图计算和流式数据。Spark本身的内存数据处理能力有一定的限制，而借着Redis可以让Spark更快的做数据分析。其实Spark的DataFrame和Datasets已经在做类似的优化，先把数据进行结构化放在内存里进行计算，并且Datasets可以省掉序列化和反序列化的消耗。结合Spark和Redis，借助Redis的共享分布式内存数据存储机制，可以处理数百万个记录乃至上亿的记录<br>时序数据的分析仅仅是一个开始，更多的性能优化可以参见：<a href="https://github.com/RedisLabs/spark-timeseries" target="_blank" rel="external">Spark-Redis</a>。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考;"></a><em>参考</em>;</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/use_spark_sql_or_dataframe_for_query_graphgrame/" itemprop="url">
                  基于Spark DataFrame的图数据库GraphFrame：用Spark SQL查询Graph
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:52:22+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="GraphFrame发布"><a href="#GraphFrame发布" class="headerlink" title="GraphFrame发布"></a><strong>GraphFrame发布</strong></h3><p>GraphFrame基于Spark SQL的DataFrame，继承了DataFrame扩展性和高性能。并且可以提供支持Scala、Java和Python等语言的统一API。</p>
<h3 id="什么是GraphFrame"><a href="#什么是GraphFrame" class="headerlink" title="什么是GraphFrame"></a><strong>什么是GraphFrame</strong></h3><p>GraphFrame是类似于Spark的GraphX库，支持图处理。但GraphFrame建立在Spark DataFrame之上，具有以下重要的优势：<br>支持Scala ，Java 和Python API：GraphFrame提供统一的三种编程语言APIs，而GraphX的所有算法支持Python和Java<br>方便、简单的图查询：GraphFrame允许用户使用Spark SQL和DataFrame的API查询<br>支持导出和导入图：GraphFrame支持DataFrame数据源，使得可以读取和写入多种格式的图，比如，Parquet、JSON和CSV格式。</p>
<h3 id="社交网络的列子"><a href="#社交网络的列子" class="headerlink" title="社交网络的列子"></a><strong>社交网络的列子</strong></h3><p>社交网络中的人是以关系来互相连接的，我们能把这个网络看成一幅图，其中人看成顶点，人与人之间的关系看作是边，如图1所示：<br><img src="http://img0.ph.126.net/7NaiyaPFpW3oxa26wNb0SQ==/4856287773289568868.jpg" alt="此处输入图片的描述"><br>图1<br>在社交网络上，每个人可能由年龄和名字，每个人之间的关系也有不同类型。如表1和表2<br>表1<br><img src="http://img0.ph.126.net/VHcML9Lw9s0HThseBKZFIQ==/6631272274867367835.png" alt="此处输入图片的描述"></p>
<p>表2<br><img src="http://img2.ph.126.net/QX3apWm32f7AHBbYV8-kXA==/6631388823099920156.png" alt="此处输入图片的描述"></p>
<h3 id="图查询示列"><a href="#图查询示列" class="headerlink" title="图查询示列"></a><strong>图查询示列</strong></h3><p>由于GraphFrame的顶点和边存储为DataFrame，可以用DataFrame或SQL来很简单的查询图。<br>比如，查询有多少年龄大于35的人？<br>g.vertices.filtr(“age &gt; 35”)<br>比如，有多少人至少被2个人关注？<br>g.inDegrees.filter(“inDegree &gt;=2”)</p>
<p>GraphFrames支持所有GraphX的算法，包括PageRank、Shortest Paths、Connected components、Strongly Connected components、Triangle count 和Label Propagation Algorithm（LPA）</p>
<p>GraphFrame和GraphX之间可以无损的来回转换。<br>val gx: Graph[Row, Row] = g.toGraphX()<br>val g2: GraphFrame = GraphFrame.fromGraphX(gx)<br>更相信的GraphFrame API文档见<a href="http://graphframes.github.io/api/scala/index.html#org.graphframes.GraphFrames" target="_blank" rel="external">这里</a>。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考:"></a><em>参考</em>:</h3><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2016/08/21/tiwtter_discover_and_consume_data/" itemprop="url">
                  Twitter数据平台的架构演化：分析数据的数据发现和消费
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-08-21T19:42:48+08:00" content="2016-08-21">
              2016-08-21
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em>导读</em>：本文详细讲述Twitter数据平台的架构演化：分析数据的数据发现和消费。</p>
<h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p> Twitter数据平台维护数据系统来支持和管理各种业务的数据生产和消费，包括，公用报表指标（比如，月活跃或者天活跃），个性化推荐，A/B测试，广告营销等。 Twitter数据平台运维着一些全球最大的Hadoop集群，其中有几个集群超过1万个节点，存储着数百PB级数据集，每天有超过10万个日常job作业处理数十PB级数据量。<a href="http://github.com/twitter/scalding" target="_blank" rel="external">Scalding</a>用来在HDFS上进行数据清洗（ETL：Extract，Transform和Load），数据科学家和数据分析师使用<a href="https://github.com/prestodb/presto" target="_blank" rel="external">Presto</a>进行交互式查询。MySQL或者Vertica用来作普通的数据集聚合，然后Tableau仪表板展示。<a href="https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale" target="_blank" rel="external">Manhattan</a>是Twitter的分布式数据库，其为实时服务服务。</p>
<p>Twitter数据平台团队从刚开始的单个数据分析组，其仅仅拥有核心的数据集，到成百上千的员工（团队）产生和消费这些数据集。这意味着：数据源发现，数据源的完成链（例如，这些数据源是如何产生和消费的）的获取，不考虑数据源的格式、位置和工具的数据集消费和它们整个生命周期内的一致性管理，将成为一个比较现实的问题。</p>
<p>为了满足这些需求，数据平台团队开发出数据访问层（Data Access Layer (DAL)）：</p>
<ul>
<li>数据发现：如何发现最重要的数据集？谁拥有这些数据集？数据集的语义和其它相关的元数据是什么？</li>
<li>数据审计：数据集的创建这或者消费者是哪位？数据集是如何创建这些数据的？数据集的依赖和服务等级协议（SLAs）是什么？数据集的报警规则是什么？数据集和它们的依赖是否一致？数据集的生命周期是如何管理的？</li>
<li>数据抽象：数据的逻辑描述是什么？数据的物理描述是什么？数据存储在哪里？数据副本在哪里？数据格式是什么？</li>
<li>数据消费：各种客户端（比如，Scalding，Presto，Hive等等）是如何交互地使用数据平台的各数据集？</li>
</ul>
<p>本文中将讨论DAL更高层次的设计和使用，DAL是如何符合整个大数据平台生态，以及分析一些实践和经验教训。</p>
<h4 id="DAL架构设计"><a href="#DAL架构设计" class="headerlink" title="DAL架构设计"></a>DAL架构设计</h4><p>为了让数据抽象，DAL有一个逻辑数据集和物理数据集的概念。逻辑数据集代表着数据集要独立于存储类型、存储位置、存储格式和存储副本之外。一个逻辑数据集可以物化到多个存储位置，甚至可以存储到不同的存储系统上，比如，HDFS或者Vertica。物理数据集是和物理存储位置（比如，HDFS namenode，像Vertica或MySQL这样的数据库等）关联的，所有的分片（物理数据块）在物理存储上。根据它们的类型，分片可以是分区或者快照。消费的数据集的元数据（Metadata）存储在物理数据集层。<br>这种抽象的好处：<br>a）：跨多种物理实现，分组聚合相同的逻辑数据集，更易数据集发现；<br>b）：提供消费数据集所需要的所有信息，包括数据集存储格式，数据集存储位置，以及调用的客户端（比如，Scalding或者Presto）。DAL数据集附加元数据，使得数据发现和数据消费更容易（如下所示）。因为所有的数据访问都通过DAL层，我们使用DAL层获取所有数据集的生产和消费的完整链。（其实跟阿里内部的数据地图差不多的意义）。</p>
<p>下面的架构图显示DAL层是如何配合数据平台的架构：<br><img src="http://img1.ph.126.net/FEKRG1rW3QV61GWGt6PdKg==/6631557048373689499.jpg" alt="此处输入图片的描述"></p>
<p>在技术栈的底层，核心基础设施包括Hadoop集群和数据库（比如，Vertica，MySQL和Manhattan）。核心数据服务层包括数据访问层（DAL），checkpoit作业状态和依赖的应用状态管理服务，以及job作业延迟报警服务。建立在核心数据服务层之上的是数据生命周期管理，包括数据复制服务和数据删除服务，数据复制服务会管理跨Hadoop集群的数据复制；数据删除服务会根据数据过期策略来删除数据。数据处理工具包括前面提到的Scalding和Presto，也包括自建的ETL工具来实现不同后端（比如，HDFS，Vertica或者MySQL）间的数据转换。</p>
<p>数据展示的UI（外界称为EagleEye）通过核心数据服务层聚合元数据（Metadata），也作为Twitter数据入口的控制。EagleEye用来发现数据集和应用，以及展示它们之间依赖的关系图。</p>
<h4 id="如何发现和消费数据集？"><a href="#如何发现和消费数据集？" class="headerlink" title="如何发现和消费数据集？"></a>如何发现和消费数据集？</h4><p>像前面涉及到的，DAL数据集带有额外的元数据，可以轻松做到数据发现和消费。Twitter数据平台团队使用下面的数据资源管理来发现和消费数据集。</p>
<h5 id="发现一个数据集"><a href="#发现一个数据集" class="headerlink" title="发现一个数据集"></a>发现一个数据集</h5><p>数据平台提供的数据资源管理中 “Discover Data Sources”模块能发现使用过的数据集，或者搜索感兴趣的数据集。数据资源管理通过DAL层搜索这个数据集。<br><img src="http://img2.ph.126.net/rDzMYLctfac6cUUkbZjHQA==/6631478983048121372.jpg" alt="此处输入图片的描述"></p>
<h5 id="数据集的预览信息"><a href="#数据集的预览信息" class="headerlink" title="数据集的预览信息"></a>数据集的预览信息</h5><p>如果数据资源管理找到了我们想查询的数据集，它将展示给使用者预览信息。如下图，数据集在HDFS上被找到，数据资源管理中可以看到数据拥有者的描述，以及通过一定的启发式计算数据集的整体健康状态。我们也能预览的元数据字段有数据集的拥有者，数据集的访问频率，代表数据schema的thrift类，HDFS上的物理位置。<br><img src="http://img2.ph.126.net/JzYMozxQTLRtmOK7x8j26A==/6631502072792305004.jpg" alt="此处输入图片的描述"></p>
<p>我们也可以检验数据集的schema，包括用户对特殊字段添加的评论。类似的，schema也可以让其它系统（Vertica或者MySQL）发现。<br><img src="http://img0.ph.126.net/tPUwFyvntZgM4T4hmJr4GA==/6631754960466694542.jpg" alt="此处输入图片的描述"></p>
<p>接下来给个例子，下面给出的代码截图是使用Scalding的例子。注意到，对读者来说，数据存储格式和位置都经过抽象。当通过Scalding运行下面的代码，时间范围提供给DAL，DAL提供数据分片的位置和格式。DAL的Scalding客户端接收刚才的信息，以Hadoop的合适的split数目来构造合适的Cascading Tap。</p>
<p><img src="http://img0.ph.126.net/q0EO2PDu337DcBl_QLCtuQ==/6631602128350431009.jpg" alt="此处输入图片的描述"></p>
<h5 id="数据集的完整链和依赖"><a href="#数据集的完整链和依赖" class="headerlink" title="数据集的完整链和依赖"></a>数据集的完整链和依赖</h5><p>数据集资源管理也可以查看生产和消费数据集的作业和作业的完整链。从图中可以看出，有一个job作业产生数据集（图中红框），同时有好几个job作业在消费这个数据集。红框中的数据的生成依赖HDFS上的好几个数据集。并且，如果其中的某个job作业生成数据集延迟了，将会发出告警。<br><img src="http://img0.ph.126.net/gKCb7V9eheRIf62pZGTdHQ==/6631780249234131459.png" alt="此处输入图片的描述"></p>
<h4 id="实践-amp-经验教训"><a href="#实践-amp-经验教训" class="headerlink" title="实践 &amp; 经验教训"></a>实践 &amp; 经验教训</h4><p>在这样Twitter体量的公司，想简化数据集跨所有数据格式和存储系统进行消费是很难的。也有一些像<a href="https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore" target="_blank" rel="external">Hive Metastore</a>这样开源的工具可以解决数据抽象，但其只有一部分功能。其它功能，比如数据集的审计和依赖链，管理数据集过期和复制和数据更易消费，也是很重要。</p>
<p>在实现DAL时做出了设计上的选择：把DAL设计成一个抽象和消费层，而不是仅仅聚焦在数据发现和审计。这么做的目的是为了让DAL成为数据集真实的来源，这将帮助我们透明的转换数据格式（比如，从lzo压缩的Thrift转换到Parquet格式），帮助我们使用相同的元数据从各种工具中产生和消费数据（比如，Scalding和Presto），帮助我们进行job作业角色的迁移（因为job作业的所有者和团队角色的不断演化，发生的相当频繁），让数据集的过期管理和复制管理在同一个地方完成。</p>
<p>在DAL刚开始实现阶段，Twitter团队把DAL作为一种library，并且DAL可以直接喝后端数据库会话。这是相当脆弱的，有这么几个原因：安全很弱，因为证书不得不分发到各个客户端；每个客户端都直接连接到数据库是相当困难的；由于客户端的重新发布对所有用户来说，更新是非常缓慢的。数据平台团队移除了这个模块，构建了服务层。</p>
<p>数据平台团队开发DAL涉及到成千上万的job作业需要重新部署依赖（例如，从HDFS到DAL），这个过程中却有正在线上运行的产品。需要严密和严谨的工作而不中断这些job作业。如果仅仅在意数据依赖链和审计，那这个实现将是相当的简单和安全，因为作者可以通过异步或者离线处理。迁移是困难的，耗时的，做好的做法是增量式迁移。但作者知道元数据服务是每个数据平台都需要的，所以，强烈推荐首先要做的是构建一个数据平台基础。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"><br>~</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://img0.ph.126.net/3vPAbMoh_6fH3-g_I0zo-w==/6631748363397501906.jpg"
               alt="侠天" />
          <p class="site-author-name" itemprop="name">侠天</p>
          <p class="site-description motion-element" itemprop="description">侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">21</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1333564335" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.infoq.com/cn/author/%E4%BE%A0%E5%A4%A9" target="_blank" title="InfoQ">
                  
                    <i class="fa fa-fw fa-infoq"></i>
                  
                  InfoQ
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">侠天</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
