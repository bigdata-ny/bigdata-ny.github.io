<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="神机喵算" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="3.4 电影影评分类二元分类，或者称为二值分类，可能是应用最广泛的机器学习问题。通过学习本例，你将掌握如何基于文本内容将影评分为正、负二类。
3.4.1 IMDB数据集本文将从互联网电影数据库（IMDB）获取50,000个流行电影影评作为数据集。这里将其分割为25,000个影评的训练集和25,000个影评的测试集。其中每个数据集都包含50%的好评和50%的差评。
为什么要将数据集分割成训练集和测试">
<meta property="og:type" content="article">
<meta property="og:title" content="《Deep Learning with Python》第三章 3.4 走进神经网络之电影影评分类">
<meta property="og:url" content="http://yoursite.com/2018/03/17/deep-learning-with-python-chapter-3-3.4/index.html">
<meta property="og:site_name" content="神机喵算">
<meta property="og:description" content="3.4 电影影评分类二元分类，或者称为二值分类，可能是应用最广泛的机器学习问题。通过学习本例，你将掌握如何基于文本内容将影评分为正、负二类。
3.4.1 IMDB数据集本文将从互联网电影数据库（IMDB）获取50,000个流行电影影评作为数据集。这里将其分割为25,000个影评的训练集和25,000个影评的测试集。其中每个数据集都包含50%的好评和50%的差评。
为什么要将数据集分割成训练集和测试">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/9803709-8c1b32ee4e274044..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/9803709-59e3a51f28a20168..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/9803709-c1ef5755c15b6a15..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/9803709-7dc921898c9a6b17..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/9803709-45638c54a2426c6a..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg">
<meta property="og:updated_time" content="2018-03-17T04:06:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Deep Learning with Python》第三章 3.4 走进神经网络之电影影评分类">
<meta name="twitter:description" content="3.4 电影影评分类二元分类，或者称为二值分类，可能是应用最广泛的机器学习问题。通过学习本例，你将掌握如何基于文本内容将影评分为正、负二类。
3.4.1 IMDB数据集本文将从互联网电影数据库（IMDB）获取50,000个流行电影影评作为数据集。这里将其分割为25,000个影评的训练集和25,000个影评的测试集。其中每个数据集都包含50%的好评和50%的差评。
为什么要将数据集分割成训练集和测试">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/9803709-8c1b32ee4e274044..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://yoursite.com/2018/03/17/deep-learning-with-python-chapter-3-3.4/"/>

  <title> 《Deep Learning with Python》第三章 3.4 走进神经网络之电影影评分类 | 神机喵算 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">神机喵算</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                《Deep Learning with Python》第三章 3.4 走进神经网络之电影影评分类
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2018-03-17T12:06:00+08:00" content="2018-03-17">
              2018-03-17
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h5 id="3-4-电影影评分类"><a href="#3-4-电影影评分类" class="headerlink" title="3.4 电影影评分类"></a>3.4 电影影评分类</h5><p>二元分类，或者称为二值分类，可能是应用最广泛的机器学习问题。通过学习本例，你将掌握如何基于文本内容将影评分为正、负二类。</p>
<h6 id="3-4-1-IMDB数据集"><a href="#3-4-1-IMDB数据集" class="headerlink" title="3.4.1 IMDB数据集"></a>3.4.1 IMDB数据集</h6><p>本文将从互联网电影数据库（IMDB）获取50,000个流行电影影评作为数据集。这里将其分割为25,000个影评的训练集和25,000个影评的测试集。其中每个数据集都包含50%的好评和50%的差评。</p>
<p>为什么要将数据集分割成训练集和测试集呢？因为测试机器学习模型所使用的数据集不能和训练该模型的数据集是同一个。在训练集上表现良好的模型，并不意味着一定会在“未曾见过”的测试集上也有相同的表现。也就是说，你更关注的是训练的模型在新数据集上的性能（因为训练集数据的标签是已知的，很显然这些是不需要去预测的）。例如，可能你的模型可以将训练样本和对应的目标在内存中进行一一映射，但是这个模型对从“未见过的”数据无法进行预测。下一章会更详细的讨论该观点。</p>
<p>Keras已经包括IMDB数据集，并进行了数据预处理：影评（单词序列）转换成整数序列，这里每个整数代表对应单词在字典的索引值。</p>
<p>下面的代码将加载IMDB数据集，当你首次运行该代码，将会在服务器上下载大约80M的数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.1 Loading the IMDB dataset</span></div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdv</div><div class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(</div><div class="line">num_words=<span class="number">10000</span>)</div></pre></td></tr></table></figure>
<p>设置参数num_words=10000，保留训练集中词频为top 10000的单词，低频单词丢弃。变量train_data和test_data是影评列表（list），每条影评看成是单词序列，用单词索引进行编码。train_labels和test_labels是0和1的列表，其中0代表差评（negative），1代表好评（positive）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>train_data[<span class="number">0</span>]</div><div class="line">[<span class="number">1</span>, <span class="number">14</span>, <span class="number">22</span>, <span class="number">16</span>, ... <span class="number">178</span>, <span class="number">32</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>train_labels[<span class="number">0</span>]</div><div class="line"><span class="number">1</span></div></pre></td></tr></table></figure>
<p>前面限制影评中的单词词频为top 10000，所以单词的索引不会超过10000：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>max([max(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> train_data])</div><div class="line"><span class="number">9999</span></div></pre></td></tr></table></figure>
<p>下面来个好玩的，如何将编码后的影评进行解码得到单词呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''</span></div><div class="line">word_index is a dictionary mapping</div><div class="line">words to an integer index.</div><div class="line">'''</div><div class="line">word_index = imdb.get_word_index()</div><div class="line">reverse_word_index = dict(</div><div class="line"><span class="string">'''</span></div><div class="line">Reverses it, mapping integer indices to words</div><div class="line">'''</div><div class="line">    [(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</div><div class="line">decoded_review = <span class="string">' '</span>.join(</div><div class="line"><span class="string">'''</span></div><div class="line">Decodes the review. Note that the indices</div><div class="line">are offset by 3 because 0, 1, and 2 are</div><div class="line">reserved indices for “padding,” “start of</div><div class="line">sequence,” and “unknown.”</div><div class="line">'''</div><div class="line">    [reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])</div></pre></td></tr></table></figure>
<h6 id="3-4-2-准备数据"><a href="#3-4-2-准备数据" class="headerlink" title="3.4.2 准备数据"></a>3.4.2 准备数据</h6><p>神经网络不能输入整数列表，所以需要将整数列表转换成张量。有两种方式可以实现：</p>
<ul>
<li>填充列表：先将列表填充成相同长度的，再转成形状为（样本数，单词索引长度）的整数张量。接着用神经网络的第一层layer（Embedding layer）处理整数张量。</li>
<li>one-hot编码：one-hot编码是将单词索引转成0、1的向量。比如，将序列[3, 5]转成10,000维向量，其中索引3和5的值为1，其它索引对应的值为0。然后使用神经网络的Dense layer作为第一层layer处理浮点型向量数据。</li>
</ul>
<p>下面采用后一种方法向量化数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></div><div class="line">    <span class="comment">#Creates an all-zero matrix of shape (len(sequences), dimension)</span></div><div class="line">    results = np.zeros((len(sequences), dimension))</div><div class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</div><div class="line">         <span class="string">'''</span></div><div class="line">         Sets specific indices of results[i] to 1s</div><div class="line">         '''</div><div class="line">          results[i, sequence] = <span class="number">1.</span></div><div class="line">    <span class="keyword">return</span> results</div><div class="line"><span class="string">'''</span></div><div class="line">Vectorized training data and test data</div><div class="line">'''</div><div class="line">x_train = vectorize_sequences(train_data)</div><div class="line">x_test = vectorize_sequences(test_data)</div></pre></td></tr></table></figure>
<p>下面看下向量化后的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x_train[<span class="number">0</span>]</div><div class="line">array([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>, ...,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]</div></pre></td></tr></table></figure>
<p>同理，向量化对应的label：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</div><div class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</div></pre></td></tr></table></figure>
<p>数据准备好了，就等着传入神经网络模型。</p>
<h6 id="3-4-3-构建神经网络模型"><a href="#3-4-3-构建神经网络模型" class="headerlink" title="3.4.3 构建神经网络模型"></a>3.4.3 构建神经网络模型</h6><p>输入数据为向量，label为标量（1和0），相当简单。一系列带有relu激活函数的全联接层（Dense layer）的神经网络就可以很好的解决影评分类：Dense(16, activation=’relu’)。</p>
<p>每个Dense layer设置隐藏单元（<em>hidden unit</em>）数为16。<em>hidden unit</em>是layer的一维表征空间。由第二章得知，每个带有relu激励函数的Dense layer可以实现下面链式的张量操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">output = relu(dot(W, input) + b)</div></pre></td></tr></table></figure>
<p>16个<em>hidden unit</em>意味着权重矩阵的形状为（输入维度，16）:输入数据与权重矩阵W点积的结果是投影到16维表征空间（，接着加上偏置向量b，然后应用relu激活操作）。给人的直觉是表征空间的维数即是中间学习表示的自由度。<em>hidden unit</em>越多（高维表征空间）允许神经网络学习更复杂的表示，但同时也让神经网络计算成本增加，可能导致不可预期的模式（模式会提高训练集上的性能，降低测试集上的表现，也就是常说的“过拟合”现象）。</p>
<p>逐层排列的Dense layer架构有两个关键点：</p>
<ul>
<li>选择多少层Dense layer</li>
<li>每个Dense layer选择多少个<em>hidden unit</em></li>
</ul>
<p>在第四章中的常规性原则将会指导你对上述问题做出选择。此时，你就暂时相信下面的架构选择哦：</p>
<ul>
<li>两个具有16个<em>hidden unit</em>的中间层</li>
<li>第三层layer将输出当前影评的情感预测值（标量）</li>
</ul>
<p>中间层使用relu作为激活函数，最后一层layer使用sigmoid激活函数，输出0到1之间的概率值。激活函数relu（rectified linear unit（修正线性单元），ReLU），对于所有负值都置为0，而正值不变，见图3.4；而激活函数sigmoid将变量值映射为[0, 1]区间，可以看作是概率值，见图3.5。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/9803709-8c1b32ee4e274044..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>图3.4 Relu激活函数</p>
<p><img src="http://upload-images.jianshu.io/upload_images/9803709-59e3a51f28a20168..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>图3.5 Sigmoid激活函数</p>
<p><img src="http://upload-images.jianshu.io/upload_images/9803709-c1ef5755c15b6a15..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>图3.6 三层layer神经网络</p>
<p>图3.6显示了神经网络的大体架构。下面是Keras的实现，和前面MNIST数字识别的例子类似：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.3 The model definition</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</div><div class="line">model = models.Sequential()</div><div class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</div><div class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>)) model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</div></pre></td></tr></table></figure>
<blockquote>
<p>什么是激活函数呢？为什么要使用激活函数？</p>
<p>没有像relu这样的激活函数（俗称非线性单元）的话，那Dense layer只剩下两个线性操作：点积和加法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt; output = dot(W, input) + b</div><div class="line">&gt;</div></pre></td></tr></table></figure>
</blockquote>
<p>&gt;</p>
<blockquote>
<p>这样layer只能学习到输入数据的线性变换（仿射变换）：layer的假设空间就成了输入数据的所有可能的线性变换到16维表征空间的集合。这样的假设空间并不能学习到多层layer的表征，因为一系列的线性layer等效于一个线性操作：layer数的增加并不会扩展假设空间。</p>
<p>为了从深度学习中得到更丰富的假设空间，你需要加入非线性部分，或者激活函数。relu是深度学习中最常用的激活函数之一，但是也有其它可选：prelu激活函数、elu激活函数等等。</p>
</blockquote>
<p>接着，选择损失函数和优化器。因为本例是二值分类问题，神经网络模型输出是概率值（网络的最后一层layer带有sigmoid激活函数，输出一维数据），所以最好的损失函数是binary_crossentropy损失函数。但这不是唯一的选择，你也可以使用mean_squared_error损失函数。一般输出为概率值的模型优先选择交叉熵损失函数（<em>crossentropy</em>）。交叉熵是信息论中的指标，用来度量概率分布之间的距离。本例是用来度量实际分布与预测值的差距。</p>
<p>这里为模型选择binary_crossentropy损失函数和rmsprop优化器。注意监控模型训练过程中的准确度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.4 Compiling the model</span></div><div class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</div><div class="line">loss=<span class="string">'binary_crossentropy'</span>,</div><div class="line">metrics=[<span class="string">'accuracy'</span>])</div></pre></td></tr></table></figure>
<p>上面传入的优化器<em>optimizer</em>、损失函数<em>loss</em>和指标<em>metrics</em>三个参数都是字符串型，这是因为’rmsprop’、’binary_crossentropy’和’accuracy’都是Keras内置实现的。如果想配置自定义的优化器或者损失函数或者指标函数，你可以用参数<em>optimizer</em>传入优化器类，见代码3.5；用参数<em>loss</em>和<em>metrics</em>传入函数对象，见代码3.6：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.5 Configuring the optimiser</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimisers</div><div class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</div><div class="line">loss=<span class="string">'binary_crossentropy'</span>,</div><div class="line">metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="comment">#Listing 3.6 Using custom losses and metrics</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> losses</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> metrics</div><div class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</div><div class="line">loss=losses.binary_crossentropy,</div><div class="line">metrics=[metrics.binary_accuracy])</div></pre></td></tr></table></figure>
<h6 id="3-4-4-验证模型"><a href="#3-4-4-验证模型" class="headerlink" title="3.4.4 验证模型"></a>3.4.4 验证模型</h6><p>为了监控模型训练过程中模型在新数据上的准确度，需要从原始的训练数据集中分出10,000个样本作为验证集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.7 Setting aside a validation set</span></div><div class="line">x_val = x_train[:<span class="number">10000</span>]</div><div class="line">partial_x_train = x_train[<span class="number">10000</span>:]</div><div class="line"></div><div class="line">y_val = y_train[:<span class="number">10000</span>]</div><div class="line">partial_y_train = y_train[<span class="number">10000</span>:]</div></pre></td></tr></table></figure>
<p>现在开始模型训练，迭代训练的epoch（在所有训练集数据上跑完一次称为一个epoch）为20个，mini-batch大小为512。训练过程中监控验证集数据上的损失函数和准确度，设置参数validation_data。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.8 Training your model</span></div><div class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</div><div class="line">loss=<span class="string">'binary_crossentropy'</span>,</div><div class="line">metrics=[<span class="string">'acc'</span>])</div><div class="line">history = model.fit(partial_x_train,</div><div class="line">partial_y_train,</div><div class="line">epochs=<span class="number">20</span>,</div><div class="line">batch_size=<span class="number">512</span>,</div><div class="line">validation_data=(x_val, y_val))</div></pre></td></tr></table></figure>
<p>在CPU上训练模型时，每个epoch耗时不到2秒，整个训练过程大概持续20秒。每个epoch结束时，会有个短暂的停顿，这时模型会计算验证集数据上的损失值和准确度。</p>
<p>注意，调用model.fit()会返回一个History对象，该对象有个history成员，它是一个包含训练过程的每个数据的字典。下面来看下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>history_dict = history.history</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>history_dict.keys()</div><div class="line">[<span class="string">u'acc'</span>, <span class="string">u'loss'</span>, <span class="string">u'val_acc'</span>, <span class="string">u'val_loss'</span>]</div></pre></td></tr></table></figure>
<p>history字典有四项：模型训练和验证中每个指标一项。接下来的两段代码，使用Matplotlib在同一幅图中绘制训练集损失和验证集损失，见图3.7；同时将训练集准确度和验证集准确度绘制在同一幅图中，见图3.8。注意，因为神经网络的初始化是随机的，可能会导致你的结果与本例稍有差别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.9 Plotting the training and validation loss</span></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> pet</div><div class="line"></div><div class="line">history_dict = history.history</div><div class="line">loss_values = history_dict[<span class="string">'loss'</span>]</div><div class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</div><div class="line"></div><div class="line">epochs = range(<span class="number">1</span>, len(acc) + <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="string">'''</span></div><div class="line">“bo” is for “blue dot.”</div><div class="line">“b” is for “solid blue line.”</div><div class="line">'''</div><div class="line">plt.plot(epochs, loss_values, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>) plt.plot(epochs, val_loss_values, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</div><div class="line">plt.title(<span class="string">'Training and validation loss'</span>)</div><div class="line">plt.xlabel(<span class="string">'Epochs'</span>)</div><div class="line">plt.ylabel(<span class="string">'Loss'</span>)</div><div class="line">plt.legend()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/9803709-7dc921898c9a6b17..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>图3.7 迭代训练中训练集和验证集的损失趋势</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.10 Plotting the training and validation accuracy</span></div><div class="line"><span class="comment">#Clears the figure</span></div><div class="line">plt.clf()</div><div class="line">acc_values = history_dict[<span class="string">'acc'</span>]</div><div class="line">val_acc_values = history_dict[<span class="string">'val_acc'</span>]</div><div class="line"></div><div class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</div><div class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</div><div class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</div><div class="line">plt.xlabel(<span class="string">'Epochs'</span>)</div><div class="line">plt.ylabel(<span class="string">'Loss'</span>)</div><div class="line">plt.legend()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://upload-images.jianshu.io/upload_images/9803709-45638c54a2426c6a..png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p>
<p>图3.8 迭代训练中的训练集和验证集的准确度趋势</p>
<p>正如你所见，随着迭代训练的epoch，训练损失值不断减小，而准确度不断提升。这也是执行梯度下降优化器期待的结果：不断迭代训练减小损失。但是验证集数据上的损失值和准确度表现的并不是如此：验证集在第四个epoch后效果达到最好。这也是前面提醒过的：模型在训练集上表现良好并不代表在新数据集上也有同样的表现。准确地来讲，这是过拟合（<em>overfiting</em>）：在迭代训练第2个epoch后，模型在训练集上出现了过度优化，最终的学习表征像是为训练集特制的，对新数据丧失了泛化能力。</p>
<p>本例中，为了防止过拟合出现，需要在迭代训练3个epoch后停止训练。一般来讲，我们可以使用多种技术解决过拟合，这些会在第四章中详细介绍。</p>
<p>下面从头迭代训练4个epoch生成新的神经网络，并在测试集上评估效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Listing 3.11 Retraining a model from scratch</span></div><div class="line">model = models.Sequential()</div><div class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,))) model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>)) model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</div><div class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</div><div class="line">              loss=<span class="string">'binary_crossentropy'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</div><div class="line">results = model.evaluate(x_test, y_test)</div></pre></td></tr></table></figure>
<p>最好的评估结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>results</div><div class="line">[<span class="number">0.2929924130630493</span>, <span class="number">0.88327999999999995</span>]</div></pre></td></tr></table></figure>
<p>这个相当直白的方法获得了88%的准确度。使用最新的方法，你将会得到接近95%的准确度。</p>
<h6 id="3-4-5-模型预测"><a href="#3-4-5-模型预测" class="headerlink" title="3.4.5 模型预测"></a>3.4.5 模型预测</h6><p>训练完神经网络模型，使用predict方法进行影评情感预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>model.predict(x_test)</div><div class="line">array([[ <span class="number">0.98006207</span>]</div><div class="line">       [ <span class="number">0.99758697</span>]</div><div class="line">       [ <span class="number">0.99975556</span>]</div><div class="line">       ...,</div><div class="line">       [ <span class="number">0.82167041</span>]</div><div class="line">       [ <span class="number">0.02885115</span>]</div><div class="line">       [ <span class="number">0.65371346</span>]], dtype=float32)</div></pre></td></tr></table></figure>
<p>正如你所看到的结果，神经网络模型对一些样本数据的预测结果自信（概率为0.99或者更高，或者0.01或者更小），但是对另外一些的预测结果不是太自信（概率为0.6，0.4的情况）。</p>
<h6 id="3-4-6-延伸实验"><a href="#3-4-6-延伸实验" class="headerlink" title="3.4.6 延伸实验"></a>3.4.6 延伸实验</h6><p>下面的一些实验使得神经网络的架构选择更合理些（虽然还是有待提升的空间）：</p>
<ul>
<li>本例使用的两个隐藏层。可以尝试选择一个或者三个隐藏层，看下会怎样影响验证集和测试集的准确度；</li>
<li>选择更多的<em>hidden unit</em>或者更少的<em>hidden unit</em>：32个unit，64个unit等等；</li>
<li>使用mse损失函数代替binary_crossentropy损失函数；</li>
<li>使用tanh激活函数（在神经网络早期常用的激活函数）代替relu激活函数。</li>
</ul>
<h6 id="3-4-7-总结"><a href="#3-4-7-总结" class="headerlink" title="3.4.7 总结"></a>3.4.7 总结</h6><p>从本实例学到的知识点：</p>
<ul>
<li>原始数据集预处理为张量传入神经网络。单词序列编码为二值向量或者其它形式；</li>
<li>一系列带有relu激活函数的Dense layer能解决广泛的问题，包括情感分类，后续会常用到的；</li>
<li>二值分类问题（输出两个类别）中，最后的一个Dense layer带有一个sigmoid激活函数和一个单元：网络输出是0到1之间的标量，代表概率值；</li>
<li>二分类问题中有sigmoid标量输出的，损失函数选择binary_crossentropy损失函数；</li>
<li>rmsprop优化器对于大部分深度学习模型来说是足够好的选择；</li>
<li>随着在训练集上表现越来越好，神经网络模型开始过拟合，在新数据上表现越来越差。关注验证集上的监控指标</li>
</ul>
<p>未完待续。。。</p>
<p>Enjoy!</p>
<blockquote>
<p>翻译本书系列的初衷是，觉得其中把深度学习讲解的通俗易懂。不光有实例，也包含作者多年实践对深度学习概念、原理的深度理解。最后说不重要的一点，François Chollet是Keras作者。<br>声明本资料仅供个人学习交流、研究，禁止用于其他目的。如果喜欢，请购买英文原版。</p>
</blockquote>
<hr>
<p>侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
<p>若发现以上文章有任何不妥，请联系我。</p>
<p><img src="http://img1.ph.126.net/FQI2AsgiKe9OkxHv6LZ2JQ==/6631621919559857881.jpg" alt="image"></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/17/deep-learning-with-python-chapter-3-3.2/" rel="next" title="《Deep Learning with Python》第三章 3.2 走进神经网络之Keras简单入门">
                <i class="fa fa-chevron-left"></i> 《Deep Learning with Python》第三章 3.2 走进神经网络之Keras简单入门
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://img0.ph.126.net/3vPAbMoh_6fH3-g_I0zo-w==/6631748363397501906.jpg"
               alt="侠天" />
          <p class="site-author-name" itemprop="name">侠天</p>
          <p class="site-description motion-element" itemprop="description">侠天，专注于大数据、机器学习和数学相关的内容，并有个人公众号：bigdata_ny分享相关技术文章。</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">32</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/1333564335" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://www.infoq.com/cn/author/%E4%BE%A0%E5%A4%A9" target="_blank" title="InfoQ">
                  
                    <i class="fa fa-fw fa-infoq"></i>
                  
                  InfoQ
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-4-电影影评分类"><span class="nav-number">1.</span> <span class="nav-text">3.4 电影影评分类</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-1-IMDB数据集"><span class="nav-number">1.1.</span> <span class="nav-text">3.4.1 IMDB数据集</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-2-准备数据"><span class="nav-number">1.2.</span> <span class="nav-text">3.4.2 准备数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-3-构建神经网络模型"><span class="nav-number">1.3.</span> <span class="nav-text">3.4.3 构建神经网络模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-4-验证模型"><span class="nav-number">1.4.</span> <span class="nav-text">3.4.4 验证模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-5-模型预测"><span class="nav-number">1.5.</span> <span class="nav-text">3.4.5 模型预测</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-6-延伸实验"><span class="nav-number">1.6.</span> <span class="nav-text">3.4.6 延伸实验</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#3-4-7-总结"><span class="nav-number">1.7.</span> <span class="nav-text">3.4.7 总结</span></a></li></ol></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">侠天</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
